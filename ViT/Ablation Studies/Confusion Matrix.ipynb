{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4438b40-ad23-4755-85ef-e72d4ee6194b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f32385dfe50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from einops import rearrange\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "from sklearn import neighbors\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca6bc21-c97f-4fd4-8d55-9af440e498a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ViT Implementation ðŸ”¥\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv =  nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.act = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.maxpool(self.act((self.bn(self.conv(x)))))\n",
    "\n",
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "\n",
    "    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert the image into patches and then project them into a vector space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        # Calculate the number of patches from the image size and patch size\n",
    "        # self.num_patches = (self.image_size // self.patch_size) ** 3\n",
    "        # Create a projection layer to convert the image into patches\n",
    "        # The layer projects each patch into a vector of size hidden_size\n",
    "        self.conv_1 = ConvBlock(self.num_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_2 = ConvBlock(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_3 = ConvBlock(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_4 = ConvBlock(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_5 = ConvBlock(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.num_patches = 512\n",
    "        #self.projection = nn.Conv3d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, num_channels, image_depth, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "        x = self.conv_4(x)\n",
    "        x = self.conv_5(x)\n",
    "        #x = self.projection(x)\n",
    "        x = rearrange(x, 'b c d w h -> b c (d w h)')\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine the patch embeddings with the class token and position embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        # Create a learnable [CLS] token\n",
    "        # Similar to BERT, the [CLS] token is added to the beginning of the input sequence\n",
    "        # and is used to classify the entire sequence\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
    "        # Create position embeddings for the [CLS] token and the patch embeddings\n",
    "        # Add 1 to the sequence length for the [CLS] token\n",
    "        self.position_embeddings = \\\n",
    "            nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        # Expand the [CLS] token to the batch size\n",
    "        # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # Concatenate the [CLS] token to the beginning of the input sequence\n",
    "        # This results in a sequence length of (num_patches + 1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single attention head.\n",
    "    This module is used in the MultiHeadAttention module.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "        # Create the query, key, and value projection layers\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project the input into query, key, and value\n",
    "        # The same input is used to generate the query, key, and value,\n",
    "        # so it's usually called self-attention.\n",
    "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, attention_head_size)\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        # Calculate the attention scores\n",
    "        # softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        # Calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module.\n",
    "    This module is used in the TransformerEncoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a list of attention heads\n",
    "        self.heads = nn.ModuleList([])\n",
    "        for _ in range(self.num_attention_heads):\n",
    "            head = AttentionHead(\n",
    "                self.hidden_size,\n",
    "                self.attention_head_size,\n",
    "                config[\"attention_probs_dropout_prob\"],\n",
    "                self.qkv_bias\n",
    "            )\n",
    "            self.heads.append(head)\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the attention output for each attention head\n",
    "        attention_outputs = [head(x) for head in self.heads]\n",
    "        # Concatenate the attention outputs from each attention head\n",
    "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
    "        # Project the concatenated attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
    "            return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class FasterMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module with some optimizations.\n",
    "    All the heads are processed simultaneously with merged query, key, and value projections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a linear layer to project the query, key, and value\n",
    "        self.qkv_projection = nn.Linear(self.hidden_size, self.all_head_size * 3, bias=self.qkv_bias)\n",
    "        self.attn_dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Project the query, key, and value\n",
    "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, all_head_size * 3)\n",
    "        qkv = self.qkv_projection(x)\n",
    "        # Split the projected query, key, and value into query, key, and value\n",
    "        # (batch_size, sequence_length, all_head_size * 3) -> (batch_size, sequence_length, all_head_size)\n",
    "        query, key, value = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Resize the query, key, and value to (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
    "        batch_size, sequence_length, _ = query.size()\n",
    "        query = query.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        key = key.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        value = value.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        # Calculate the attention scores\n",
    "        # softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        # Calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        # Resize the attention output\n",
    "        # from (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
    "        # To (batch_size, sequence_length, all_head_size)\n",
    "        attention_output = attention_output.transpose(1, 2) \\\n",
    "                                           .contiguous() \\\n",
    "                                           .view(batch_size, sequence_length, self.all_head_size)\n",
    "        # Project the attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-layer perceptron module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
    "        self.activation = NewGELUActivation()\n",
    "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.use_faster_attention = config.get(\"use_faster_attention\", False)\n",
    "        if self.use_faster_attention:\n",
    "            self.attention = FasterMultiHeadAttention(config)\n",
    "        else:\n",
    "            self.attention = MultiHeadAttention(config)\n",
    "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Self-attention\n",
    "        attention_output, attention_probs = \\\n",
    "            self.attention(self.layernorm_1(x), output_attentions=output_attentions)\n",
    "        # Skip connection\n",
    "        x = x + attention_output\n",
    "        # Feed-forward network\n",
    "        mlp_output = self.mlp(self.layernorm_2(x))\n",
    "        # Skip connection\n",
    "        x = x + mlp_output\n",
    "        # Return the transformer block's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, attention_probs)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The transformer encoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Create a list of transformer blocks\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config[\"num_hidden_layers\"]):\n",
    "            block = Block(config)\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the transformer block's output for each block\n",
    "        all_attentions = []\n",
    "        for block in self.blocks:\n",
    "            x, attention_probs = block(x, output_attentions=output_attentions)\n",
    "            if output_attentions:\n",
    "                all_attentions.append(attention_probs)\n",
    "        # Return the encoder's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, all_attentions)\n",
    "\n",
    "\n",
    "class ViTForClassfication(nn.Module):\n",
    "    \"\"\"\n",
    "    The ViT model for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        # Create the embedding module\n",
    "        self.embedding = Embeddings(config)\n",
    "        # Create the transformer encoder module\n",
    "        self.encoder = Encoder(config)\n",
    "        # Create a linear layer to project the encoder's output to the number of classes\n",
    "        self.attention_pool = nn.Linear(self.hidden_size, 1)\n",
    "        self.classifier = nn.Linear(2*self.hidden_size, self.num_classes)\n",
    "        # Initialize the weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the embedding output\n",
    "        embedding_output = self.embedding(x)\n",
    "        # Calculate the encoder's output\n",
    "        encoder_output, all_attentions = self.encoder(embedding_output, output_attentions=output_attentions)\n",
    "        # Calculate the logits, take the [CLS] token's output as features for classification\n",
    "        cls_logits, activation_logits = encoder_output[:, 0, :], encoder_output[:, 1:, :]\n",
    "        activation_logits = torch.matmul(nn.functional.softmax(self.attention_pool(activation_logits), dim=1).transpose(-1, -2), activation_logits).squeeze(-2)\n",
    "        logits = torch.cat((cls_logits, activation_logits), dim=1)\n",
    "        logits = self.classifier(logits)\n",
    "        # Return the logits and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (logits, None)\n",
    "        else:\n",
    "            return (logits, all_attentions)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, Embeddings):\n",
    "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.position_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.position_embeddings.dtype)\n",
    "\n",
    "            module.cls_token.data = nn.init.trunc_normal_(\n",
    "                module.cls_token.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.cls_token.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30befe1-a0e1-45b6-9df2-4cff3aba873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#@title Prepare Data ðŸ“Š\n",
    "# Import libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FolderDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    "        self.image_paths = glob.glob(f'{self.folder}/*/*.pt')\n",
    "        self.labels = {\n",
    "            'CN' : 0,\n",
    "            'MCI' : 1,\n",
    "            'AD' : 2\n",
    "        }\n",
    "        self.transform = False #tio.transforms.Compose(\n",
    "            #[tio.transforms.RandomAffine(degrees=5)\n",
    "            #tio.transforms.RandomBiasField()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __label_dist__(self):\n",
    "        cn,mci, ad = 0, 0, 0\n",
    "        for path in self.image_paths:\n",
    "            if self.__label_extract__(path) == 0:\n",
    "                cn += 1\n",
    "            elif self.__label_extract__(path) == 1:\n",
    "                mci += 1\n",
    "            elif self.__label_extract__(path) == 2:\n",
    "                ad += 1\n",
    "        \n",
    "        return {'CN': cn, 'MCI': mci, 'AD': ad}\n",
    "    \n",
    "    def __label_extract__(self, path):\n",
    "        if 'CN' in path:\n",
    "            return 0\n",
    "        elif 'MCI' in path:\n",
    "            return 1\n",
    "        elif 'AD' in path:\n",
    "            return 2\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tensor, label = torch.load(self.image_paths[idx]), self.__label_extract__(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            tensor = self.transform(tensor)\n",
    "        \n",
    "        return tensor, label\n",
    "    \n",
    "def prepare_data(batch_size=4, num_workers=2, train_sample_size=None, test_sample_size=None):\n",
    "    train_dataset = FolderDataset(folder='/home/admin1/Arindam/Alzheimer/ViT/data/3D (part II)/Train')\n",
    "    val_dataset = FolderDataset(folder='/home/admin1/Arindam/Alzheimer/ViT/data/3D (part II)/Val')\n",
    "    test_dataset = FolderDataset(folder='/home/admin1/Arindam/Alzheimer/ViT/data/3D (part II)/Test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    classes = ('CN', 'MCI', 'AD')\n",
    "    class_dist = {\n",
    "        'Train': train_dataset.__label_dist__(),\n",
    "        'Val': val_dataset.__label_dist__(),\n",
    "        'Test': test_dataset.__label_dist__()\n",
    "    }\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader, class_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dafbedd-a63b-494f-9f9e-01ba6c5c2471",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size' : 3,\n",
    "    'image_size' : 192,\n",
    "    'patch_size' : 6,\n",
    "    \"hidden_size\": 216,\n",
    "    \"num_hidden_layers\": None,\n",
    "    \"num_attention_heads\": 8,\n",
    "    \"intermediate_size\": 3 * 216, # 3 * hidden_size\n",
    "    \"hidden_dropout_prob\": 0.25,\n",
    "    \"attention_probs_dropout_prob\": 0.25,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"num_classes\": 3, # num_classes\n",
    "    \"num_channels\": 1,\n",
    "    \"qkv_bias\": True,\n",
    "    \"use_faster_attention\": True,\n",
    "    'save_model_every' : 0,\n",
    "    'exp_name' : 'HCCT Models Evaluation'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d05af8-e687-400c-9349-bdd36c4a4d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in train, val and test set are, 1526, 326, 330\n",
      "\t\tCN\tMCI\tAD\n",
      "Train\t: \t523\t686\t317\n",
      "Val\t: \t112\t147\t67\n",
      "Test\t: \t113\t148\t69\n",
      "\n",
      "Shape of images and labels of a signle batch is torch.Size([3, 1, 192, 192, 192]) and torch.Size([3]) respectively.\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader, class_dist = prepare_data()\n",
    "\n",
    "print(f\"Total number of images in train, val and test set are, {len(train_loader.dataset)}, {len(valid_loader.dataset)}, {len(test_loader.dataset)}\")\n",
    "\n",
    "assert len(train_loader.dataset)==1526\n",
    "assert len(valid_loader.dataset)==326\n",
    "assert len(test_loader.dataset)==330\n",
    "\n",
    "print(f\"\\t\\tCN\\tMCI\\tAD\")\n",
    "for key in class_dist.keys():\n",
    "    print(f\"{key}\\t: \\t{class_dist[key]['CN']}\\t{class_dist[key]['MCI']}\\t{class_dist[key]['AD']}\")\n",
    "# Check a sample batch size\n",
    "idx =0\n",
    "for data in train_loader:\n",
    "    images, labels = data\n",
    "    print(f\"\\nShape of images and labels of a signle batch is {images.shape} and {labels.shape} respectively.\")\n",
    "    break\n",
    "\n",
    "\n",
    "# Get parameters for each layer of the model in a tabular format\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b9a0b3d-af1e-4003-8aee-3487d017815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    The simple evaluator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, loss_fn, device):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, testloader):\n",
    "        original_labels, predicted_labels = [], []\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in testloader:\n",
    "                # Move the batch to the device\n",
    "                batch = [t.to(self.device) for t in batch]\n",
    "                images, labels = batch\n",
    "\n",
    "                # Get predictions\n",
    "                logits = self.model(images)[0]\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = self.loss_fn(logits, nn.functional.one_hot(labels, num_classes=3).type(torch.FloatTensor).cuda())\n",
    "                total_loss += loss.item() * len(images)\n",
    "\n",
    "                # Calculate the accuracy\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "\n",
    "                # Append to the lists\n",
    "                original_labels = original_labels + labels.tolist()\n",
    "                predicted_labels = predicted_labels + predictions.tolist()\n",
    "        \n",
    "        accuracy = correct / len(testloader.dataset)\n",
    "        avg_loss = total_loss / len(testloader.dataset)\n",
    "        return avg_loss, accuracy, original_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9269d615-3636-4d14-8a1c-c18ac3e4ac8f",
   "metadata": {},
   "source": [
    "## **Hybrid-CCT Fine-Tune (Transformer Encoder Layer No - 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad3d767d-5ffb-42c9-853a-0d017c5c63e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------------+\n",
      "|                       Modules                       | Parameters |\n",
      "+-----------------------------------------------------+------------+\n",
      "|                 embedding.cls_token                 |    216     |\n",
      "|            embedding.position_embeddings            |   110808   |\n",
      "|    embedding.patch_embeddings.conv_1.conv.weight    |    864     |\n",
      "|     embedding.patch_embeddings.conv_1.conv.bias     |     32     |\n",
      "|     embedding.patch_embeddings.conv_1.bn.weight     |     32     |\n",
      "|      embedding.patch_embeddings.conv_1.bn.bias      |     32     |\n",
      "|    embedding.patch_embeddings.conv_2.conv.weight    |   55296    |\n",
      "|     embedding.patch_embeddings.conv_2.conv.bias     |     64     |\n",
      "|     embedding.patch_embeddings.conv_2.bn.weight     |     64     |\n",
      "|      embedding.patch_embeddings.conv_2.bn.bias      |     64     |\n",
      "|    embedding.patch_embeddings.conv_3.conv.weight    |   221184   |\n",
      "|     embedding.patch_embeddings.conv_3.conv.bias     |    128     |\n",
      "|     embedding.patch_embeddings.conv_3.bn.weight     |    128     |\n",
      "|      embedding.patch_embeddings.conv_3.bn.bias      |    128     |\n",
      "|    embedding.patch_embeddings.conv_4.conv.weight    |   884736   |\n",
      "|     embedding.patch_embeddings.conv_4.conv.bias     |    256     |\n",
      "|     embedding.patch_embeddings.conv_4.bn.weight     |    256     |\n",
      "|      embedding.patch_embeddings.conv_4.bn.bias      |    256     |\n",
      "|    embedding.patch_embeddings.conv_5.conv.weight    |  3538944   |\n",
      "|     embedding.patch_embeddings.conv_5.conv.bias     |    512     |\n",
      "|     embedding.patch_embeddings.conv_5.bn.weight     |    512     |\n",
      "|      embedding.patch_embeddings.conv_5.bn.bias      |    512     |\n",
      "|   encoder.blocks.0.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.0.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.0.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.0.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.0.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.0.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.0.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.0.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.1.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.1.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.1.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.1.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.1.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.1.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.1.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.1.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.2.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.2.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.2.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.2.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.2.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.2.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.2.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.2.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_2.bias          |    216     |\n",
      "|                attention_pool.weight                |    216     |\n",
      "|                 attention_pool.bias                 |     1      |\n",
      "|                  classifier.weight                  |    1296    |\n",
      "|                   classifier.bias                   |     3      |\n",
      "+-----------------------------------------------------+------------+\n",
      "Total Trainable Params: 6223996\n",
      "Total parameters: 6223996, Trainable Parameters 6223996\n",
      "Total parameters: 6.223996M, Trainable Parameters 6.223996M\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Create the model, loss function and trainer\n",
    "config['num_hidden_layers'] = 3\n",
    "config['model_name'] = 'model_best_finetuned.pt'\n",
    "config['weight_base_dir'] = '/home/admin1/Arindam/Alzheimer/ViT/experiments/Hybrid-Finetune'\n",
    "\n",
    "model = ViTForClassfication(config)\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(os.path.join(config['weight_base_dir'], config['model_name'])))\n",
    "\n",
    "count_parameters(model)\n",
    "# Get number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}, Trainable Parameters {total_trainable_params}\\nTotal parameters: {total_params/1000000}M, Trainable Parameters {total_trainable_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c5ca10-9ff6-4931-b9f4-80bc661d91cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Hybrid CCT Fine-Tune, Model name: model_best_finetuned.pt \n",
      "Test Loss: 0.13755577205595873, Test Accuracy: 0.9606060606060606\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "tester = Evaluator(model, loss_fn, device)\n",
    "test_loss, test_accuracy, original_labels, predicted_labels = tester.evaluate(test_loader)\n",
    "print(f\"Test Name: Hybrid CCT Fine-Tune, Model name: {config['model_name']} \\nTest Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399ee350-cb72-4a21-9e61-9dc33a475b43",
   "metadata": {},
   "source": [
    "### Create Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1307cb93-2581-481a-9c0a-2efd42815913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1814658/1066014562.py:14: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + labels, fontsize=15)\n",
      "/tmp/ipykernel_1814658/1066014562.py:15: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + labels, fontsize=15)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x750 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "labels = ['CN', 'MCI', 'AD']\n",
    "conf_matrix = confusion_matrix(original_labels, predicted_labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "\n",
    "if labels:\n",
    "    ax.set_xticklabels([''] + labels, fontsize=15)\n",
    "    ax.set_yticklabels([''] + labels, fontsize=15)\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()\n",
    "fig.savefig(f\"{config['weight_base_dir']}/confusion_matrix.png\", dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "833f110c-857e-4d0c-9a20-4f3145992235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f32385dfe50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
