{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4438b40-ad23-4755-85ef-e72d4ee6194b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f32385dfe50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from einops import rearrange\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "from sklearn import neighbors\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca6bc21-c97f-4fd4-8d55-9af440e498a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ViT Implementation ðŸ”¥\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv =  nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.act = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.maxpool(self.act((self.bn(self.conv(x)))))\n",
    "\n",
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "\n",
    "    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert the image into patches and then project them into a vector space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        # Calculate the number of patches from the image size and patch size\n",
    "        # self.num_patches = (self.image_size // self.patch_size) ** 3\n",
    "        # Create a projection layer to convert the image into patches\n",
    "        # The layer projects each patch into a vector of size hidden_size\n",
    "        self.conv_1 = ConvBlock(self.num_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_2 = ConvBlock(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_3 = ConvBlock(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_4 = ConvBlock(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_5 = ConvBlock(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.num_patches = 512\n",
    "        #self.projection = nn.Conv3d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, num_channels, image_depth, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "        x = self.conv_4(x)\n",
    "        x = self.conv_5(x)\n",
    "        #x = self.projection(x)\n",
    "        x = rearrange(x, 'b c d w h -> b c (d w h)')\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine the patch embeddings with the class token and position embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        # Create a learnable [CLS] token\n",
    "        # Similar to BERT, the [CLS] token is added to the beginning of the input sequence\n",
    "        # and is used to classify the entire sequence\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
    "        # Create position embeddings for the [CLS] token and the patch embeddings\n",
    "        # Add 1 to the sequence length for the [CLS] token\n",
    "        self.position_embeddings = \\\n",
    "            nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        # Expand the [CLS] token to the batch size\n",
    "        # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # Concatenate the [CLS] token to the beginning of the input sequence\n",
    "        # This results in a sequence length of (num_patches + 1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single attention head.\n",
    "    This module is used in the MultiHeadAttention module.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "        # Create the query, key, and value projection layers\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project the input into query, key, and value\n",
    "        # The same input is used to generate the query, key, and value,\n",
    "        # so it's usually called self-attention.\n",
    "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, attention_head_size)\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        # Calculate the attention scores\n",
    "        # softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        # Calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module.\n",
    "    This module is used in the TransformerEncoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a list of attention heads\n",
    "        self.heads = nn.ModuleList([])\n",
    "        for _ in range(self.num_attention_heads):\n",
    "            head = AttentionHead(\n",
    "                self.hidden_size,\n",
    "                self.attention_head_size,\n",
    "                config[\"attention_probs_dropout_prob\"],\n",
    "                self.qkv_bias\n",
    "            )\n",
    "            self.heads.append(head)\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the attention output for each attention head\n",
    "        attention_outputs = [head(x) for head in self.heads]\n",
    "        # Concatenate the attention outputs from each attention head\n",
    "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
    "        # Project the concatenated attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
    "            return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class FasterMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module with some optimizations.\n",
    "    All the heads are processed simultaneously with merged query, key, and value projections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a linear layer to project the query, key, and value\n",
    "        self.qkv_projection = nn.Linear(self.hidden_size, self.all_head_size * 3, bias=self.qkv_bias)\n",
    "        self.attn_dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Project the query, key, and value\n",
    "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, all_head_size * 3)\n",
    "        qkv = self.qkv_projection(x)\n",
    "        # Split the projected query, key, and value into query, key, and value\n",
    "        # (batch_size, sequence_length, all_head_size * 3) -> (batch_size, sequence_length, all_head_size)\n",
    "        query, key, value = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Resize the query, key, and value to (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
    "        batch_size, sequence_length, _ = query.size()\n",
    "        query = query.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        key = key.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        value = value.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        # Calculate the attention scores\n",
    "        # softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        # Calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        # Resize the attention output\n",
    "        # from (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
    "        # To (batch_size, sequence_length, all_head_size)\n",
    "        attention_output = attention_output.transpose(1, 2) \\\n",
    "                                           .contiguous() \\\n",
    "                                           .view(batch_size, sequence_length, self.all_head_size)\n",
    "        # Project the attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-layer perceptron module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
    "        self.activation = NewGELUActivation()\n",
    "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.use_faster_attention = config.get(\"use_faster_attention\", False)\n",
    "        if self.use_faster_attention:\n",
    "            self.attention = FasterMultiHeadAttention(config)\n",
    "        else:\n",
    "            self.attention = MultiHeadAttention(config)\n",
    "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Self-attention\n",
    "        attention_output, attention_probs = \\\n",
    "            self.attention(self.layernorm_1(x), output_attentions=output_attentions)\n",
    "        # Skip connection\n",
    "        x = x + attention_output\n",
    "        # Feed-forward network\n",
    "        mlp_output = self.mlp(self.layernorm_2(x))\n",
    "        # Skip connection\n",
    "        x = x + mlp_output\n",
    "        # Return the transformer block's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, attention_probs)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The transformer encoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Create a list of transformer blocks\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config[\"num_hidden_layers\"]):\n",
    "            block = Block(config)\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the transformer block's output for each block\n",
    "        all_attentions = []\n",
    "        for block in self.blocks:\n",
    "            x, attention_probs = block(x, output_attentions=output_attentions)\n",
    "            if output_attentions:\n",
    "                all_attentions.append(attention_probs)\n",
    "        # Return the encoder's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, all_attentions)\n",
    "\n",
    "\n",
    "class ViTForClassfication(nn.Module):\n",
    "    \"\"\"\n",
    "    The ViT model for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        # Create the embedding module\n",
    "        self.embedding = Embeddings(config)\n",
    "        # Create the transformer encoder module\n",
    "        self.encoder = Encoder(config)\n",
    "        # Create a linear layer to project the encoder's output to the number of classes\n",
    "        self.attention_pool = nn.Linear(self.hidden_size, 1)\n",
    "        self.classifier = nn.Linear(2*self.hidden_size, self.num_classes)\n",
    "        # Initialize the weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the embedding output\n",
    "        embedding_output = self.embedding(x)\n",
    "        # Calculate the encoder's output\n",
    "        encoder_output, all_attentions = self.encoder(embedding_output, output_attentions=output_attentions)\n",
    "        # Calculate the logits, take the [CLS] token's output as features for classification\n",
    "        cls_logits, activation_logits = encoder_output[:, 0, :], encoder_output[:, 1:, :]\n",
    "        activation_logits = torch.matmul(nn.functional.softmax(self.attention_pool(activation_logits), dim=1).transpose(-1, -2), activation_logits).squeeze(-2)\n",
    "        logits = torch.cat((cls_logits, activation_logits), dim=1)\n",
    "        logits = self.classifier(logits)\n",
    "        # Return the logits and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (logits, None)\n",
    "        else:\n",
    "            return (logits, all_attentions)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, Embeddings):\n",
    "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.position_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.position_embeddings.dtype)\n",
    "\n",
    "            module.cls_token.data = nn.init.trunc_normal_(\n",
    "                module.cls_token.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.cls_token.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30befe1-a0e1-45b6-9df2-4cff3aba873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#@title Prepare Data ðŸ“Š\n",
    "# Import libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchio as tio\n",
    "\n",
    "class FolderDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    "        self.image_paths = glob.glob(f'{self.folder}/*/*.pt')\n",
    "        self.labels = {\n",
    "            'CN' : 0,\n",
    "            'MCI' : 1,\n",
    "            'AD' : 2\n",
    "        }\n",
    "        self.transform = False #tio.transforms.Compose(\n",
    "            #[tio.transforms.RandomAffine(degrees=5)\n",
    "            #tio.transforms.RandomBiasField()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __label_dist__(self):\n",
    "        cn,mci, ad = 0, 0, 0\n",
    "        for path in self.image_paths:\n",
    "            if self.__label_extract__(path) == 0:\n",
    "                cn += 1\n",
    "            elif self.__label_extract__(path) == 1:\n",
    "                mci += 1\n",
    "            elif self.__label_extract__(path) == 2:\n",
    "                ad += 1\n",
    "        \n",
    "        return {'CN': cn, 'MCI': mci, 'AD': ad}\n",
    "    \n",
    "    def __label_extract__(self, path):\n",
    "        if 'CN' in path:\n",
    "            return 0\n",
    "        elif 'MCI' in path:\n",
    "            return 1\n",
    "        elif 'AD' in path:\n",
    "            return 2\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tensor, label = torch.load(self.image_paths[idx]), self.__label_extract__(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            tensor = self.transform(tensor)\n",
    "        \n",
    "        return tensor, label\n",
    "    \n",
    "def prepare_data(batch_size=4, num_workers=2, train_sample_size=None, test_sample_size=None):\n",
    "    train_dataset = FolderDataset(folder='/home/admin1/Arindam/Alzheimer/ViT/data/3D (part II)/Train')\n",
    "    val_dataset = FolderDataset(folder='/home/admin1/Arindam/Alzheimer/ViT/data/3D (part II)/Val')\n",
    "    test_dataset = FolderDataset(folder='/home/admin1/Arindam/Alzheimer/ViT/data/3D (part II)/Test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    classes = ('CN', 'MCI', 'AD')\n",
    "    class_dist = {\n",
    "        'Train': train_dataset.__label_dist__(),\n",
    "        'Val': val_dataset.__label_dist__(),\n",
    "        'Test': test_dataset.__label_dist__()\n",
    "    }\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader, class_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dafbedd-a63b-494f-9f9e-01ba6c5c2471",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size' : 3,\n",
    "    'image_size' : 192,\n",
    "    'patch_size' : 6,\n",
    "    \"hidden_size\": 216,\n",
    "    \"num_hidden_layers\": None,\n",
    "    \"num_attention_heads\": 8,\n",
    "    \"intermediate_size\": 3 * 216, # 3 * hidden_size\n",
    "    \"hidden_dropout_prob\": 0.25,\n",
    "    \"attention_probs_dropout_prob\": 0.25,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"num_classes\": 3, # num_classes\n",
    "    \"num_channels\": 1,\n",
    "    \"qkv_bias\": True,\n",
    "    \"use_faster_attention\": True,\n",
    "    'save_model_every' : 0,\n",
    "    'exp_name' : 'HCCT Models Evaluation'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d05af8-e687-400c-9349-bdd36c4a4d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in train, val and test set are, 1526, 326, 330\n",
      "\t\tCN\tMCI\tAD\n",
      "Train\t: \t523\t686\t317\n",
      "Val\t: \t112\t147\t67\n",
      "Test\t: \t113\t148\t69\n",
      "\n",
      "Shape of images and labels of a signle batch is torch.Size([3, 1, 192, 192, 192]) and torch.Size([3]) respectively.\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader, class_dist = prepare_data()\n",
    "\n",
    "print(f\"Total number of images in train, val and test set are, {len(train_loader.dataset)}, {len(valid_loader.dataset)}, {len(test_loader.dataset)}\")\n",
    "\n",
    "assert len(train_loader.dataset)==1526\n",
    "assert len(valid_loader.dataset)==326\n",
    "assert len(test_loader.dataset)==330\n",
    "\n",
    "print(f\"\\t\\tCN\\tMCI\\tAD\")\n",
    "for key in class_dist.keys():\n",
    "    print(f\"{key}\\t: \\t{class_dist[key]['CN']}\\t{class_dist[key]['MCI']}\\t{class_dist[key]['AD']}\")\n",
    "# Check a sample batch size\n",
    "idx =0\n",
    "for data in train_loader:\n",
    "    images, labels = data\n",
    "    print(f\"\\nShape of images and labels of a signle batch is {images.shape} and {labels.shape} respectively.\")\n",
    "    break\n",
    "\n",
    "\n",
    "# Get parameters for each layer of the model in a tabular format\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b9a0b3d-af1e-4003-8aee-3487d017815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    The simple evaluator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, loss_fn, device):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, testloader):\n",
    "        original_labels, predicted_labels = [], []\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in testloader:\n",
    "                # Move the batch to the device\n",
    "                batch = [t.to(self.device) for t in batch]\n",
    "                images, labels = batch\n",
    "\n",
    "                # Get predictions\n",
    "                logits = self.model(images)[0]\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = self.loss_fn(logits, nn.functional.one_hot(labels, num_classes=3).type(torch.FloatTensor).cuda())\n",
    "                total_loss += loss.item() * len(images)\n",
    "\n",
    "                # Calculate the accuracy\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "\n",
    "                # Append to the lists\n",
    "                original_labels = original_labels + labels.tolist()\n",
    "                predicted_labels = predicted_labels + predictions.tolist()\n",
    "        \n",
    "        accuracy = correct / len(testloader.dataset)\n",
    "        avg_loss = total_loss / len(testloader.dataset)\n",
    "        return avg_loss, accuracy, original_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9269d615-3636-4d14-8a1c-c18ac3e4ac8f",
   "metadata": {},
   "source": [
    "## **Hybrid-CCT Fine-Tune (Transformer Encoder Layer No - 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad3d767d-5ffb-42c9-853a-0d017c5c63e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------------+\n",
      "|                       Modules                       | Parameters |\n",
      "+-----------------------------------------------------+------------+\n",
      "|                 embedding.cls_token                 |    216     |\n",
      "|            embedding.position_embeddings            |   110808   |\n",
      "|    embedding.patch_embeddings.conv_1.conv.weight    |    864     |\n",
      "|     embedding.patch_embeddings.conv_1.conv.bias     |     32     |\n",
      "|     embedding.patch_embeddings.conv_1.bn.weight     |     32     |\n",
      "|      embedding.patch_embeddings.conv_1.bn.bias      |     32     |\n",
      "|    embedding.patch_embeddings.conv_2.conv.weight    |   55296    |\n",
      "|     embedding.patch_embeddings.conv_2.conv.bias     |     64     |\n",
      "|     embedding.patch_embeddings.conv_2.bn.weight     |     64     |\n",
      "|      embedding.patch_embeddings.conv_2.bn.bias      |     64     |\n",
      "|    embedding.patch_embeddings.conv_3.conv.weight    |   221184   |\n",
      "|     embedding.patch_embeddings.conv_3.conv.bias     |    128     |\n",
      "|     embedding.patch_embeddings.conv_3.bn.weight     |    128     |\n",
      "|      embedding.patch_embeddings.conv_3.bn.bias      |    128     |\n",
      "|    embedding.patch_embeddings.conv_4.conv.weight    |   884736   |\n",
      "|     embedding.patch_embeddings.conv_4.conv.bias     |    256     |\n",
      "|     embedding.patch_embeddings.conv_4.bn.weight     |    256     |\n",
      "|      embedding.patch_embeddings.conv_4.bn.bias      |    256     |\n",
      "|    embedding.patch_embeddings.conv_5.conv.weight    |  3538944   |\n",
      "|     embedding.patch_embeddings.conv_5.conv.bias     |    512     |\n",
      "|     embedding.patch_embeddings.conv_5.bn.weight     |    512     |\n",
      "|      embedding.patch_embeddings.conv_5.bn.bias      |    512     |\n",
      "|   encoder.blocks.0.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.0.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.0.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.0.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.0.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.0.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.0.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.0.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.1.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.1.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.1.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.1.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.1.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.1.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.1.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.1.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.2.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.2.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.2.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.2.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.2.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.2.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.2.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.2.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_2.bias          |    216     |\n",
      "|                attention_pool.weight                |    216     |\n",
      "|                 attention_pool.bias                 |     1      |\n",
      "|                  classifier.weight                  |    1296    |\n",
      "|                   classifier.bias                   |     3      |\n",
      "+-----------------------------------------------------+------------+\n",
      "Total Trainable Params: 6223996\n",
      "Total parameters: 6223996, Trainable Parameters 6223996\n",
      "Total parameters: 6.223996M, Trainable Parameters 6.223996M\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Create the model, loss function and trainer\n",
    "config['num_hidden_layers'] = 3\n",
    "config['model_name'] = 'model_best_finetuned.pt'\n",
    "config['weight_base_dir'] = '/home/admin1/Arindam/Alzheimer/ViT/experiments/Hybrid-Finetune'\n",
    "\n",
    "model = ViTForClassfication(config)\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(os.path.join(config['weight_base_dir'], config['model_name'])))\n",
    "\n",
    "count_parameters(model)\n",
    "# Get number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}, Trainable Parameters {total_trainable_params}\\nTotal parameters: {total_params/1000000}M, Trainable Parameters {total_trainable_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c5ca10-9ff6-4931-b9f4-80bc661d91cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Hybrid CCT Fine-Tune, Model name: model_best_finetuned.pt \n",
      "Test Loss: 0.13755577205595873, Test Accuracy: 0.9606060606060606\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "tester = Evaluator(model, loss_fn, device)\n",
    "test_loss, test_accuracy, original_labels, predicted_labels = tester.evaluate(test_loader)\n",
    "print(f\"Test Name: Hybrid CCT Fine-Tune, Model name: {config['model_name']} \\nTest Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399ee350-cb72-4a21-9e61-9dc33a475b43",
   "metadata": {},
   "source": [
    "### Create Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1307cb93-2581-481a-9c0a-2efd42815913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1814658/101600357.py:14: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + labels)\n",
      "/tmp/ipykernel_1814658/101600357.py:15: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + labels)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x750 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "labels = ['CN', 'MCI', 'AD']\n",
    "conf_matrix = confusion_matrix(original_labels, predicted_labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "\n",
    "if labels:\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()\n",
    "fig.savefig(f\"{config['weight_base_dir']}/confusion_matrix.png\", dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "833f110c-857e-4d0c-9a20-4f3145992235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f32385dfe50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
