{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f37a1c-a6ab-40c4-9325-e8298a6e363e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0c40fb3e30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from einops import rearrange\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "from sklearn import neighbors\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6510fd77-e1b9-456d-9229-6bd4881502ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ViT Implementation üî•\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv =  nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.act = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.maxpool(self.act((self.bn(self.conv(x)))))\n",
    "\n",
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "\n",
    "    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert the image into patches and then project them into a vector space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        # Calculate the number of patches from the image size and patch size\n",
    "        # self.num_patches = (self.image_size // self.patch_size) ** 3\n",
    "        # Create a projection layer to convert the image into patches\n",
    "        # The layer projects each patch into a vector of size hidden_size\n",
    "        self.conv_1 = ConvBlock(self.num_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_2 = ConvBlock(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_3 = ConvBlock(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_4 = ConvBlock(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_5 = ConvBlock(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.num_patches = 512\n",
    "        #self.projection = nn.Conv3d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, num_channels, image_depth, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "        x = self.conv_4(x)\n",
    "        x = self.conv_5(x)\n",
    "        #x = self.projection(x)\n",
    "        x = rearrange(x, 'b c d w h -> b c (d w h)')\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine the patch embeddings with the class token and position embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        # Create a learnable [CLS] token\n",
    "        # Similar to BERT, the [CLS] token is added to the beginning of the input sequence\n",
    "        # and is used to classify the entire sequence\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
    "        # Create position embeddings for the [CLS] token and the patch embeddings\n",
    "        # Add 1 to the sequence length for the [CLS] token\n",
    "        self.position_embeddings = \\\n",
    "            nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        # Expand the [CLS] token to the batch size\n",
    "        # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # Concatenate the [CLS] token to the beginning of the input sequence\n",
    "        # This results in a sequence length of (num_patches + 1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single attention head.\n",
    "    This module is used in the MultiHeadAttention module.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "        # Create the query, key, and value projection layers\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project the input into query, key, and value\n",
    "        # The same input is used to generate the query, key, and value,\n",
    "        # so it's usually called self-attention.\n",
    "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, attention_head_size)\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        # Calculate the attention scores\n",
    "        # softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        # Calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module.\n",
    "    This module is used in the TransformerEncoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a list of attention heads\n",
    "        self.heads = nn.ModuleList([])\n",
    "        for _ in range(self.num_attention_heads):\n",
    "            head = AttentionHead(\n",
    "                self.hidden_size,\n",
    "                self.attention_head_size,\n",
    "                config[\"attention_probs_dropout_prob\"],\n",
    "                self.qkv_bias\n",
    "            )\n",
    "            self.heads.append(head)\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the attention output for each attention head\n",
    "        attention_outputs = [head(x) for head in self.heads]\n",
    "        # Concatenate the attention outputs from each attention head\n",
    "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
    "        # Project the concatenated attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
    "            return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class FasterMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module with some optimizations.\n",
    "    All the heads are processed simultaneously with merged query, key, and value projections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a linear layer to project the query, key, and value\n",
    "        self.qkv_projection = nn.Linear(self.hidden_size, self.all_head_size * 3, bias=self.qkv_bias)\n",
    "        self.attn_dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Project the query, key, and value\n",
    "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, all_head_size * 3)\n",
    "        qkv = self.qkv_projection(x)\n",
    "        # Split the projected query, key, and value into query, key, and value\n",
    "        # (batch_size, sequence_length, all_head_size * 3) -> (batch_size, sequence_length, all_head_size)\n",
    "        query, key, value = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Resize the query, key, and value to (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
    "        batch_size, sequence_length, _ = query.size()\n",
    "        query = query.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        key = key.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        value = value.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        # Calculate the attention scores\n",
    "        # softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        # Calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        # Resize the attention output\n",
    "        # from (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
    "        # To (batch_size, sequence_length, all_head_size)\n",
    "        attention_output = attention_output.transpose(1, 2) \\\n",
    "                                           .contiguous() \\\n",
    "                                           .view(batch_size, sequence_length, self.all_head_size)\n",
    "        # Project the attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-layer perceptron module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
    "        self.activation = NewGELUActivation()\n",
    "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.use_faster_attention = config.get(\"use_faster_attention\", False)\n",
    "        if self.use_faster_attention:\n",
    "            self.attention = FasterMultiHeadAttention(config)\n",
    "        else:\n",
    "            self.attention = MultiHeadAttention(config)\n",
    "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Self-attention\n",
    "        attention_output, attention_probs = \\\n",
    "            self.attention(self.layernorm_1(x), output_attentions=output_attentions)\n",
    "        # Skip connection\n",
    "        x = x + attention_output\n",
    "        # Feed-forward network\n",
    "        mlp_output = self.mlp(self.layernorm_2(x))\n",
    "        # Skip connection\n",
    "        x = x + mlp_output\n",
    "        # Return the transformer block's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, attention_probs)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The transformer encoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Create a list of transformer blocks\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config[\"num_hidden_layers\"]):\n",
    "            block = Block(config)\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the transformer block's output for each block\n",
    "        all_attentions = []\n",
    "        for block in self.blocks:\n",
    "            x, attention_probs = block(x, output_attentions=output_attentions)\n",
    "            if output_attentions:\n",
    "                all_attentions.append(attention_probs)\n",
    "        # Return the encoder's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, all_attentions)\n",
    "\n",
    "\n",
    "class ViTForClassfication(nn.Module):\n",
    "    \"\"\"\n",
    "    The ViT model for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        # Create the embedding module\n",
    "        self.embedding = Embeddings(config)\n",
    "        # Create the transformer encoder module\n",
    "        self.encoder = Encoder(config)\n",
    "        # Create a linear layer to project the encoder's output to the number of classes\n",
    "        self.attention_pool = nn.Linear(self.hidden_size, 1)\n",
    "        self.classifier = nn.Linear(2*self.hidden_size, self.num_classes)\n",
    "        # Initialize the weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the embedding output\n",
    "        embedding_output = self.embedding(x)\n",
    "        # Calculate the encoder's output\n",
    "        encoder_output, all_attentions = self.encoder(embedding_output, output_attentions=output_attentions)\n",
    "        # Calculate the logits, take the [CLS] token's output as features for classification\n",
    "        cls_logits, activation_logits = encoder_output[:, 0, :], encoder_output[:, 1:, :]\n",
    "        activation_logits = torch.matmul(nn.functional.softmax(self.attention_pool(activation_logits), dim=1).transpose(-1, -2), activation_logits).squeeze(-2)\n",
    "        logits = torch.cat((cls_logits, activation_logits), dim=1)\n",
    "        logits = self.classifier(logits)\n",
    "        # Return the logits and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (logits, None)\n",
    "        else:\n",
    "            return (logits, all_attentions)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, Embeddings):\n",
    "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.position_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.position_embeddings.dtype)\n",
    "\n",
    "            module.cls_token.data = nn.init.trunc_normal_(\n",
    "                module.cls_token.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.cls_token.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd95605-0a1a-4194-a264-18ed210919b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#@title Prepare Data üìä\n",
    "# Import libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchio as tio\n",
    "\n",
    "class FolderDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    "        self.image_paths = glob.glob(f'{self.folder}/*/*.pt')\n",
    "        self.labels = {\n",
    "            'CN' : 0,\n",
    "            'MCI' : 1,\n",
    "            'AD' : 2\n",
    "        }\n",
    "        self.transform = False #tio.transforms.Compose(\n",
    "            #[tio.transforms.RandomAffine(degrees=5)\n",
    "            #tio.transforms.RandomBiasField()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __label_dist__(self):\n",
    "        cn,mci, ad = 0, 0, 0\n",
    "        for path in self.image_paths:\n",
    "            if self.__label_extract__(path) == 0:\n",
    "                cn += 1\n",
    "            elif self.__label_extract__(path) == 1:\n",
    "                mci += 1\n",
    "            elif self.__label_extract__(path) == 2:\n",
    "                ad += 1\n",
    "        \n",
    "        return {'CN': cn, 'MCI': mci, 'AD': ad}\n",
    "    \n",
    "    def __label_extract__(self, path):\n",
    "        if 'CN' in path:\n",
    "            return 0\n",
    "        elif 'MCI' in path:\n",
    "            return 1\n",
    "        elif 'AD' in path:\n",
    "            return 2\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tensor, label = torch.load(self.image_paths[idx]), self.__label_extract__(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            tensor = self.transform(tensor)\n",
    "        \n",
    "        return tensor, label\n",
    "    \n",
    "def prepare_data(batch_size=4, num_workers=2, train_sample_size=None, test_sample_size=None):\n",
    "    train_dataset = FolderDataset(folder='/home/admin1/Arindam/Alzheimer/ViT/data/3D (part II)/Train')\n",
    "    val_dataset = FolderDataset(folder='/home/admin1/Arindam/Alzheimer/ViT/data/3D (part II)/Val')\n",
    "    test_dataset = FolderDataset(folder='/home/admin1/Arindam/Alzheimer/ViT/data/3D (part II)/Test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    classes = ('CN', 'MCI', 'AD')\n",
    "    class_dist = {\n",
    "        'Train': train_dataset.__label_dist__(),\n",
    "        'Val': val_dataset.__label_dist__(),\n",
    "        'Test': test_dataset.__label_dist__()\n",
    "    }\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader, class_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4de036-d40c-4605-b86b-6275e23e22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size' : 3,\n",
    "    'image_size' : 192,\n",
    "    'patch_size' : 6,\n",
    "    \"hidden_size\": 216,\n",
    "    \"num_hidden_layers\": 4,\n",
    "    \"num_attention_heads\": 8,\n",
    "    \"intermediate_size\": 3 * 216, # 3 * hidden_size\n",
    "    \"hidden_dropout_prob\": 0.25,\n",
    "    \"attention_probs_dropout_prob\": 0.25,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"num_classes\": 3, # num_classes\n",
    "    \"num_channels\": 1,\n",
    "    \"qkv_bias\": True,\n",
    "    \"use_faster_attention\": True,\n",
    "    'lr' : 8e-5,\n",
    "    'save_model_every' : 0,\n",
    "    'exp_name' : '3D ViT Ablation',\n",
    "    'model_name' : 'Hybrid-Ablation-TEL4',\n",
    "    'epochs' : 50,\n",
    "    'weight_base_dir' : \"/home/admin1/Arindam/Alzheimer/ViT/Ablation Studies/experiments/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f090f416-afb5-4508-9d92-956b05d6efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Utils üõ†Ô∏è\n",
    "import json, os, math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def save_experiment(experiment_name, config, model, train_losses, val_losses, train_acces, val_acces, train_logs, base_dir=\"experiments\"):\n",
    "    outdir = os.path.join(base_dir, experiment_name)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Save the config\n",
    "    configfile = os.path.join(outdir, 'config_finetuned.json')\n",
    "    with open(configfile, 'w') as f:\n",
    "        json.dump(config, f, sort_keys=True, indent=4)\n",
    "\n",
    "    # Save the metrics\n",
    "    jsonfile = os.path.join(outdir, 'metrics_finetuned.json')\n",
    "    with open(jsonfile, 'w') as f:\n",
    "        data = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'train_acces': train_acces,\n",
    "            'val_acces': val_acces\n",
    "        }\n",
    "        json.dump(data, f, sort_keys=True, indent=4)\n",
    "\n",
    "    # Save the model\n",
    "    save_checkpoint(experiment_name, model, f\"final__finetuned_{config['model_name']}\", base_dir=base_dir)\n",
    "    \n",
    "    # Save the logs\n",
    "    logfile = os.path.join(outdir, 'logs_finetuned.txt')\n",
    "    with open(logfile, 'w') as f:\n",
    "        TEXT = \"\"\n",
    "        for logs in train_logs:\n",
    "            TEXT += f\"{logs}\\n\"\n",
    "        f.write(TEXT)\n",
    "\n",
    "\n",
    "def save_checkpoint(experiment_name, model, epoch, base_dir=\"experiments\"):\n",
    "    outdir = os.path.join(base_dir, experiment_name)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    cpfile = os.path.join(outdir, f'model_{epoch}.pt')\n",
    "    torch.save(model.state_dict(), cpfile)\n",
    "\n",
    "\n",
    "def load_experiment(experiment_name, checkpoint_name=f\"model_final__finetuned_{config['model_name']}.pt\", base_dir=\"experiments\"):\n",
    "    outdir = os.path.join(base_dir, experiment_name)\n",
    "    # Load the config\n",
    "    configfile = os.path.join(outdir, 'config_finetuned.json')\n",
    "    with open(configfile, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    # Load the metrics\n",
    "    jsonfile = os.path.join(outdir, 'metrics_finetuned.json')\n",
    "    with open(jsonfile, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    train_losses = data['train_losses']\n",
    "    val_losses = data['val_losses']\n",
    "    train_acces = data['train_acces']\n",
    "    val_acces = data['val_acces']\n",
    "    # Load the model\n",
    "    model = ViTForClassfication(config)\n",
    "    cpfile = os.path.join(outdir, checkpoint_name)\n",
    "    model.load_state_dict(torch.load(cpfile))\n",
    "    return config, model, train_losses, val_losses, train_acces,val_acces\n",
    "\n",
    "\n",
    "\n",
    "def visualize_images(dataset):\n",
    "    classes = ('CN', 'MCI', 'AD')\n",
    "\n",
    "    # Pick 4 samples randomly\n",
    "    indices = torch.randperm(len(dataset))[:4]\n",
    "    images = [np.asarray(dataset[i][0]) for i in indices]\n",
    "    labels = [dataset[i][1] for i in indices]\n",
    "    # Visualize the images using matplotlib\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    for i in range(4):\n",
    "        img = np.squeeze(images[i])\n",
    "        img_1 = img[img.shape[0]//2, :, :]\n",
    "        ax = fig.add_subplot(4, 3, 3*i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(np.rot90(img_1), cmap='gray')\n",
    "        ax.set_title(f\"{classes[labels[i]]} (sagital)\")\n",
    "        \n",
    "        img_2 = img[:, img.shape[1]//2, :]\n",
    "        ax = fig.add_subplot(4, 3, 3*i+2, xticks=[], yticks=[])\n",
    "        ax.imshow(np.rot90(img_2), cmap='gray')\n",
    "        ax.set_title(f\"{classes[labels[i]]} (coronal)\")\n",
    "        \n",
    "        img_3 = img[:, :, img.shape[2]//2]\n",
    "        ax = fig.add_subplot(4, 3, 3*i+3, xticks=[], yticks=[])\n",
    "        ax.imshow(np.rot90(img_3), cmap='gray')\n",
    "        ax.set_title(f\"{classes[labels[i]]} (axial)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df559cd8-df24-4eeb-8797-1127eb6cf6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    The simple trainer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, loss_fn, exp_name, device):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.exp_name = exp_name\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, trainloader, valloader, epochs, scheduler, save_model_every_n_epochs=0):\n",
    "        \"\"\"\n",
    "        Train the model for the specified number of epochs.\n",
    "        \"\"\"\n",
    "        # Keep track of the losses and accuracies\n",
    "        train_losses, val_losses, train_acces, val_acces, train_logs = [], [], [], [], []\n",
    "        # Keep track of best model\n",
    "        best_train_loss, best_val_loss = 1.0, 1.0\n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        for i in range(epochs):\n",
    "            train_loss, train_acc = self.train_epoch(trainloader)\n",
    "            val_loss, val_acc = self.evaluate(valloader)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_acces.append(train_acc)\n",
    "            val_acces.append(val_acc)\n",
    "            total_time = time.time() - start_time\n",
    "            text = f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Time: {total_time/60:.1f}min ({total_time/(60*(i+1)):.1f} min/epoch)\"\n",
    "            print(text)\n",
    "            train_logs.append(text)\n",
    "            if train_loss < best_train_loss and val_loss < best_val_loss:\n",
    "                best_train_loss, best_val_loss = train_loss, val_loss\n",
    "                text = f\"\\tSave best checkpoint at epoch {i+1}\"\n",
    "                print(text)\n",
    "                save_checkpoint(self.exp_name, self.model, f'best__finetuned_{config[\"model_name\"]}')\n",
    "                train_logs.append(text)\n",
    "            if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != epochs:\n",
    "                print('\\tSave checkpoint at epoch', i+1)\n",
    "                save_checkpoint(self.exp_name, self.model, i+1)\n",
    "            #wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc, \"val_loss\": val_loss, \"val_acc\": val_acc}, commit=True)\n",
    "            scheduler.step()\n",
    "        # Save the experiment\n",
    "        save_experiment(self.exp_name, config, self.model, train_losses, val_losses, train_acces, val_acces, train_logs)\n",
    "\n",
    "    def train_epoch(self, trainloader):\n",
    "        \"\"\"\n",
    "        Train the model for one epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for batch in trainloader:\n",
    "            # Move the batch to the device\n",
    "            batch = [t.to(self.device) for t in batch]\n",
    "            images, labels = batch\n",
    "            # Zero the gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            # Calculate the loss\n",
    "            logits = self.model(images)[0]\n",
    "            loss = self.loss_fn(logits, nn.functional.one_hot(labels, num_classes=3).type(torch.FloatTensor).cuda())\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "            # Update the model's parameters\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item() * len(images)\n",
    "            \n",
    "            # Calculate the accuracy\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            total_correct += torch.sum(predictions == labels).item()\n",
    "\n",
    "        return total_loss / len(trainloader.dataset), total_correct/len(trainloader.dataset)\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, testloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in testloader:\n",
    "                # Move the batch to the device\n",
    "                batch = [t.to(self.device) for t in batch]\n",
    "                images, labels = batch\n",
    "\n",
    "                # Get predictions\n",
    "                logits = self.model(images)[0]\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = self.loss_fn(logits, nn.functional.one_hot(labels, num_classes=3).type(torch.FloatTensor).cuda())\n",
    "                total_loss += loss.item() * len(images)\n",
    "\n",
    "                # Calculate the accuracy\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "        \n",
    "        accuracy = correct / len(testloader.dataset)\n",
    "        avg_loss = total_loss / len(testloader.dataset)\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    \n",
    "# Get parameters for each layer of the model in a tabular format\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0244d4b-44ef-4e64-a9e9-c3fb7d7b61e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in train, val and test set are, 1526, 326, 330\n",
      "\t\tCN\tMCI\tAD\n",
      "Train\t: \t523\t686\t317\n",
      "Val\t: \t112\t147\t67\n",
      "Test\t: \t113\t148\t69\n",
      "\n",
      "Shape of images and labels of a signle batch is torch.Size([3, 1, 192, 192, 192]) and torch.Size([3]) respectively.\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader, class_dist = prepare_data()\n",
    "\n",
    "print(f\"Total number of images in train, val and test set are, {len(train_loader.dataset)}, {len(valid_loader.dataset)}, {len(test_loader.dataset)}\")\n",
    "\n",
    "assert len(train_loader.dataset)==1526\n",
    "assert len(valid_loader.dataset)==326\n",
    "assert len(test_loader.dataset)==330\n",
    "\n",
    "print(f\"\\t\\tCN\\tMCI\\tAD\")\n",
    "for key in class_dist.keys():\n",
    "    print(f\"{key}\\t: \\t{class_dist[key]['CN']}\\t{class_dist[key]['MCI']}\\t{class_dist[key]['AD']}\")\n",
    "# Check a sample batch size\n",
    "idx =0\n",
    "for data in train_loader:\n",
    "    images, labels = data\n",
    "    print(f\"\\nShape of images and labels of a signle batch is {images.shape} and {labels.shape} respectively.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a73c8fa-9f11-4705-99f5-f6e3edc2050d",
   "metadata": {},
   "source": [
    "## **Hybrid-ViT (Transformer Encoder Layer No - 4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64508a58-1c57-4153-b580-9b0553a7a196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTForClassfication(\n",
      "  (embedding): Embeddings(\n",
      "    (patch_embeddings): PatchEmbeddings(\n",
      "      (conv_1): ConvBlock(\n",
      "        (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_2): ConvBlock(\n",
      "        (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_3): ConvBlock(\n",
      "        (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_4): ConvBlock(\n",
      "        (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_5): ConvBlock(\n",
      "        (conv): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (blocks): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attention): FasterMultiHeadAttention(\n",
      "          (qkv_projection): Linear(in_features=216, out_features=648, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.25, inplace=False)\n",
      "          (output_projection): Linear(in_features=216, out_features=216, bias=True)\n",
      "          (output_dropout): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "        (layernorm_1): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (dense_1): Linear(in_features=216, out_features=648, bias=True)\n",
      "          (activation): NewGELUActivation()\n",
      "          (dense_2): Linear(in_features=648, out_features=216, bias=True)\n",
      "          (dropout): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "        (layernorm_2): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (attention_pool): Linear(in_features=216, out_features=1, bias=True)\n",
      "  (classifier): Linear(in_features=432, out_features=3, bias=True)\n",
      ")\n",
      "+-----------------------------------------------+------------+\n",
      "|                    Modules                    | Parameters |\n",
      "+-----------------------------------------------+------------+\n",
      "|              embedding.cls_token              |    216     |\n",
      "|         embedding.position_embeddings         |   110808   |\n",
      "| embedding.patch_embeddings.conv_1.conv.weight |    864     |\n",
      "|  embedding.patch_embeddings.conv_1.conv.bias  |     32     |\n",
      "|  embedding.patch_embeddings.conv_1.bn.weight  |     32     |\n",
      "|   embedding.patch_embeddings.conv_1.bn.bias   |     32     |\n",
      "| embedding.patch_embeddings.conv_2.conv.weight |   55296    |\n",
      "|  embedding.patch_embeddings.conv_2.conv.bias  |     64     |\n",
      "|  embedding.patch_embeddings.conv_2.bn.weight  |     64     |\n",
      "|   embedding.patch_embeddings.conv_2.bn.bias   |     64     |\n",
      "| embedding.patch_embeddings.conv_3.conv.weight |   221184   |\n",
      "|  embedding.patch_embeddings.conv_3.conv.bias  |    128     |\n",
      "|  embedding.patch_embeddings.conv_3.bn.weight  |    128     |\n",
      "|   embedding.patch_embeddings.conv_3.bn.bias   |    128     |\n",
      "| embedding.patch_embeddings.conv_4.conv.weight |   884736   |\n",
      "|  embedding.patch_embeddings.conv_4.conv.bias  |    256     |\n",
      "|  embedding.patch_embeddings.conv_4.bn.weight  |    256     |\n",
      "|   embedding.patch_embeddings.conv_4.bn.bias   |    256     |\n",
      "| embedding.patch_embeddings.conv_5.conv.weight |  3538944   |\n",
      "|  embedding.patch_embeddings.conv_5.conv.bias  |    512     |\n",
      "|  embedding.patch_embeddings.conv_5.bn.weight  |    512     |\n",
      "|   embedding.patch_embeddings.conv_5.bn.bias   |    512     |\n",
      "|             attention_pool.weight             |    216     |\n",
      "|              attention_pool.bias              |     1      |\n",
      "|               classifier.weight               |    1296    |\n",
      "|                classifier.bias                |     3      |\n",
      "+-----------------------------------------------+------------+\n",
      "Total Trainable Params: 4816540\n",
      "Total parameters: 6693148, Trainable Parameters 4816540\n",
      "Total parameters: 6.693148M, Trainable Parameters 4.81654M\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Create the model, optimizer, loss function and trainer\n",
    "model = ViTForClassfication(config)\n",
    "print(model)\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(f\"{config['weight_base_dir']}{config['model_name']}/model_best_{config['model_name']}.pt\"))\n",
    "# Freeze Transformer layer\n",
    "model.encoder.requires_grad_(False)\n",
    "\n",
    "count_parameters(model)\n",
    "# Get number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}, Trainable Parameters {total_trainable_params}\\nTotal parameters: {total_params/1000000}M, Trainable Parameters {total_trainable_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93422f0a-da3a-4102-a2aa-5ec0ce2423a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "save_model_every_n_epochs = config['save_model_every']\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-3)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20, 30, 40, 50], gamma=0.7)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1e71028-1238-48f5-b351-c518fb6b05bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 0.0482, Train Acc: 0.9843, Val loss: 0.3049, Val Acc: 0.9479, Time: 2.4min (2.4 min/epoch)\n",
      "\tSave best checkpoint at epoch 1\n",
      "Epoch: 2, Train loss: 0.0222, Train Acc: 0.9954, Val loss: 0.3686, Val Acc: 0.9479, Time: 4.7min (2.4 min/epoch)\n",
      "Epoch: 3, Train loss: 0.0663, Train Acc: 0.9777, Val loss: 0.4429, Val Acc: 0.9202, Time: 7.0min (2.3 min/epoch)\n",
      "Epoch: 4, Train loss: 0.0284, Train Acc: 0.9882, Val loss: 0.6219, Val Acc: 0.8620, Time: 9.4min (2.3 min/epoch)\n",
      "Epoch: 5, Train loss: 0.0150, Train Acc: 0.9980, Val loss: 0.2567, Val Acc: 0.9540, Time: 11.7min (2.3 min/epoch)\n",
      "\tSave best checkpoint at epoch 5\n",
      "Epoch: 6, Train loss: 0.0007, Train Acc: 1.0000, Val loss: 0.2407, Val Acc: 0.9540, Time: 14.0min (2.3 min/epoch)\n",
      "\tSave best checkpoint at epoch 6\n",
      "Epoch: 7, Train loss: 0.0004, Train Acc: 1.0000, Val loss: 0.2556, Val Acc: 0.9571, Time: 16.3min (2.3 min/epoch)\n",
      "Epoch: 8, Train loss: 0.0002, Train Acc: 1.0000, Val loss: 0.2357, Val Acc: 0.9571, Time: 18.6min (2.3 min/epoch)\n",
      "\tSave best checkpoint at epoch 8\n",
      "Epoch: 9, Train loss: 0.0002, Train Acc: 1.0000, Val loss: 0.2776, Val Acc: 0.9540, Time: 20.9min (2.3 min/epoch)\n",
      "Epoch: 10, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.2604, Val Acc: 0.9540, Time: 23.2min (2.3 min/epoch)\n",
      "Epoch: 11, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.2703, Val Acc: 0.9571, Time: 25.5min (2.3 min/epoch)\n",
      "Epoch: 12, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.2913, Val Acc: 0.9571, Time: 27.8min (2.3 min/epoch)\n",
      "Epoch: 13, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.2835, Val Acc: 0.9571, Time: 30.1min (2.3 min/epoch)\n",
      "Epoch: 14, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.3046, Val Acc: 0.9632, Time: 32.4min (2.3 min/epoch)\n",
      "Epoch: 15, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2844, Val Acc: 0.9601, Time: 34.7min (2.3 min/epoch)\n",
      "Epoch: 16, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2840, Val Acc: 0.9601, Time: 37.0min (2.3 min/epoch)\n",
      "Epoch: 17, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.3037, Val Acc: 0.9571, Time: 39.3min (2.3 min/epoch)\n",
      "Epoch: 18, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.3313, Val Acc: 0.9571, Time: 41.6min (2.3 min/epoch)\n",
      "Epoch: 19, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2998, Val Acc: 0.9632, Time: 43.9min (2.3 min/epoch)\n",
      "Epoch: 20, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.3158, Val Acc: 0.9632, Time: 46.2min (2.3 min/epoch)\n",
      "Epoch: 21, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.3341, Val Acc: 0.9571, Time: 48.5min (2.3 min/epoch)\n",
      "Epoch: 22, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.3345, Val Acc: 0.9663, Time: 50.8min (2.3 min/epoch)\n",
      "Epoch: 23, Train loss: 0.0207, Train Acc: 0.9934, Val loss: 0.5163, Val Acc: 0.9080, Time: 53.1min (2.3 min/epoch)\n",
      "Epoch: 24, Train loss: 0.0402, Train Acc: 0.9869, Val loss: 0.8241, Val Acc: 0.8957, Time: 55.4min (2.3 min/epoch)\n",
      "Epoch: 25, Train loss: 0.0148, Train Acc: 0.9934, Val loss: 0.5825, Val Acc: 0.9110, Time: 57.7min (2.3 min/epoch)\n",
      "Epoch: 26, Train loss: 0.0034, Train Acc: 0.9993, Val loss: 0.4403, Val Acc: 0.9294, Time: 60.0min (2.3 min/epoch)\n",
      "Epoch: 27, Train loss: 0.0037, Train Acc: 0.9987, Val loss: 0.3539, Val Acc: 0.9479, Time: 62.3min (2.3 min/epoch)\n",
      "Epoch: 28, Train loss: 0.0003, Train Acc: 1.0000, Val loss: 0.3630, Val Acc: 0.9509, Time: 64.6min (2.3 min/epoch)\n",
      "Epoch: 29, Train loss: 0.0002, Train Acc: 1.0000, Val loss: 0.3923, Val Acc: 0.9540, Time: 67.0min (2.3 min/epoch)\n",
      "Epoch: 30, Train loss: 0.0223, Train Acc: 0.9941, Val loss: 0.3608, Val Acc: 0.9325, Time: 69.3min (2.3 min/epoch)\n",
      "Epoch: 31, Train loss: 0.0021, Train Acc: 1.0000, Val loss: 0.3879, Val Acc: 0.9479, Time: 71.6min (2.3 min/epoch)\n",
      "Epoch: 32, Train loss: 0.0038, Train Acc: 0.9974, Val loss: 0.3829, Val Acc: 0.9479, Time: 73.8min (2.3 min/epoch)\n",
      "Epoch: 33, Train loss: 0.0062, Train Acc: 0.9974, Val loss: 0.5013, Val Acc: 0.9294, Time: 76.1min (2.3 min/epoch)\n",
      "Epoch: 34, Train loss: 0.0099, Train Acc: 0.9967, Val loss: 0.3015, Val Acc: 0.9601, Time: 78.4min (2.3 min/epoch)\n",
      "Epoch: 35, Train loss: 0.0011, Train Acc: 1.0000, Val loss: 0.2861, Val Acc: 0.9632, Time: 80.7min (2.3 min/epoch)\n",
      "Epoch: 36, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.2645, Val Acc: 0.9693, Time: 83.0min (2.3 min/epoch)\n",
      "Epoch: 37, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.3015, Val Acc: 0.9601, Time: 85.3min (2.3 min/epoch)\n",
      "Epoch: 38, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.2860, Val Acc: 0.9663, Time: 87.6min (2.3 min/epoch)\n",
      "Epoch: 39, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.2855, Val Acc: 0.9601, Time: 90.0min (2.3 min/epoch)\n",
      "Epoch: 40, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2846, Val Acc: 0.9663, Time: 92.3min (2.3 min/epoch)\n",
      "Epoch: 41, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2847, Val Acc: 0.9632, Time: 94.6min (2.3 min/epoch)\n",
      "Epoch: 42, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2844, Val Acc: 0.9663, Time: 96.9min (2.3 min/epoch)\n",
      "Epoch: 43, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2715, Val Acc: 0.9632, Time: 99.2min (2.3 min/epoch)\n",
      "Epoch: 44, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2776, Val Acc: 0.9663, Time: 101.5min (2.3 min/epoch)\n",
      "Epoch: 45, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2867, Val Acc: 0.9693, Time: 103.8min (2.3 min/epoch)\n",
      "Epoch: 46, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2982, Val Acc: 0.9663, Time: 106.0min (2.3 min/epoch)\n",
      "Epoch: 47, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2960, Val Acc: 0.9693, Time: 108.3min (2.3 min/epoch)\n",
      "Epoch: 48, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.3231, Val Acc: 0.9632, Time: 110.6min (2.3 min/epoch)\n",
      "Epoch: 49, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2926, Val Acc: 0.9663, Time: 112.9min (2.3 min/epoch)\n",
      "Epoch: 50, Train loss: 0.0046, Train Acc: 0.9974, Val loss: 0.4850, Val Acc: 0.9356, Time: 115.2min (2.3 min/epoch)\n",
      "\n",
      "\n",
      "Test Loss: 0.631662103248478 and Test Accuracy: 0.9363636363636364\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, optimizer, loss_fn, config['model_name'], device=device)\n",
    "trainer.train(train_loader, valid_loader, config['epochs'], scheduler, save_model_every_n_epochs=save_model_every_n_epochs)\n",
    "test_loss, test_acc = trainer.evaluate(test_loader)\n",
    "print(f\"\\n\\nTest Loss: {test_loss} and Test Accuracy: {test_acc}\")\n",
    "#wandb.log({\"Test Loss\": test_loss, \"Test Accuracy\": test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97833397-bf5e-46d3-a5f8-dec2e29d41d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 25 19:14:50 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0 Off |                  Off |\n",
      "| 30%   50C    P2             180W / 450W |  19974MiB / 24564MiB |     72%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2411      G   /usr/lib/xorg/Xorg                          206MiB |\n",
      "|    0   N/A  N/A    551796      G   /usr/lib/xorg/Xorg                          405MiB |\n",
      "|    0   N/A  N/A    552008      G   xfwm4                                         7MiB |\n",
      "|    0   N/A  N/A    812763      G   ...irefox/3416/usr/lib/firefox/firefox      344MiB |\n",
      "|    0   N/A  N/A   1249667      G   ...seed-version=20231208-102139.191000       31MiB |\n",
      "|    0   N/A  N/A   1265111      C   /usr/bin/python3                          18790MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f2233-bcc8-4bbb-bd9e-14194b5dd774",
   "metadata": {},
   "source": [
    "## **Hybrid-ViT (Transformer Encoder Layer No - 5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57f60ba8-1708-4266-b318-224cd00f8f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 25 19:14:50 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0 Off |                  Off |\n",
      "| 30%   50C    P2             156W / 450W |   4478MiB / 24564MiB |     25%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2411      G   /usr/lib/xorg/Xorg                          206MiB |\n",
      "|    0   N/A  N/A    551796      G   /usr/lib/xorg/Xorg                          405MiB |\n",
      "|    0   N/A  N/A    552008      G   xfwm4                                         7MiB |\n",
      "|    0   N/A  N/A    812763      G   ...irefox/3416/usr/lib/firefox/firefox      344MiB |\n",
      "|    0   N/A  N/A   1249667      G   ...seed-version=20231208-102139.191000       31MiB |\n",
      "|    0   N/A  N/A   1265111      C   /usr/bin/python3                           3294MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd0848d2-8f7e-40fb-aac7-c1ecc80c9d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTForClassfication(\n",
      "  (embedding): Embeddings(\n",
      "    (patch_embeddings): PatchEmbeddings(\n",
      "      (conv_1): ConvBlock(\n",
      "        (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_2): ConvBlock(\n",
      "        (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_3): ConvBlock(\n",
      "        (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_4): ConvBlock(\n",
      "        (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_5): ConvBlock(\n",
      "        (conv): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (blocks): ModuleList(\n",
      "      (0-4): 5 x Block(\n",
      "        (attention): FasterMultiHeadAttention(\n",
      "          (qkv_projection): Linear(in_features=216, out_features=648, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.25, inplace=False)\n",
      "          (output_projection): Linear(in_features=216, out_features=216, bias=True)\n",
      "          (output_dropout): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "        (layernorm_1): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (dense_1): Linear(in_features=216, out_features=648, bias=True)\n",
      "          (activation): NewGELUActivation()\n",
      "          (dense_2): Linear(in_features=648, out_features=216, bias=True)\n",
      "          (dropout): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "        (layernorm_2): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (attention_pool): Linear(in_features=216, out_features=1, bias=True)\n",
      "  (classifier): Linear(in_features=432, out_features=3, bias=True)\n",
      ")\n",
      "+-----------------------------------------------+------------+\n",
      "|                    Modules                    | Parameters |\n",
      "+-----------------------------------------------+------------+\n",
      "|              embedding.cls_token              |    216     |\n",
      "|         embedding.position_embeddings         |   110808   |\n",
      "| embedding.patch_embeddings.conv_1.conv.weight |    864     |\n",
      "|  embedding.patch_embeddings.conv_1.conv.bias  |     32     |\n",
      "|  embedding.patch_embeddings.conv_1.bn.weight  |     32     |\n",
      "|   embedding.patch_embeddings.conv_1.bn.bias   |     32     |\n",
      "| embedding.patch_embeddings.conv_2.conv.weight |   55296    |\n",
      "|  embedding.patch_embeddings.conv_2.conv.bias  |     64     |\n",
      "|  embedding.patch_embeddings.conv_2.bn.weight  |     64     |\n",
      "|   embedding.patch_embeddings.conv_2.bn.bias   |     64     |\n",
      "| embedding.patch_embeddings.conv_3.conv.weight |   221184   |\n",
      "|  embedding.patch_embeddings.conv_3.conv.bias  |    128     |\n",
      "|  embedding.patch_embeddings.conv_3.bn.weight  |    128     |\n",
      "|   embedding.patch_embeddings.conv_3.bn.bias   |    128     |\n",
      "| embedding.patch_embeddings.conv_4.conv.weight |   884736   |\n",
      "|  embedding.patch_embeddings.conv_4.conv.bias  |    256     |\n",
      "|  embedding.patch_embeddings.conv_4.bn.weight  |    256     |\n",
      "|   embedding.patch_embeddings.conv_4.bn.bias   |    256     |\n",
      "| embedding.patch_embeddings.conv_5.conv.weight |  3538944   |\n",
      "|  embedding.patch_embeddings.conv_5.conv.bias  |    512     |\n",
      "|  embedding.patch_embeddings.conv_5.bn.weight  |    512     |\n",
      "|   embedding.patch_embeddings.conv_5.bn.bias   |    512     |\n",
      "|             attention_pool.weight             |    216     |\n",
      "|              attention_pool.bias              |     1      |\n",
      "|               classifier.weight               |    1296    |\n",
      "|                classifier.bias                |     3      |\n",
      "+-----------------------------------------------+------------+\n",
      "Total Trainable Params: 4816540\n",
      "Total parameters: 7162300, Trainable Parameters 4816540\n",
      "Total parameters: 7.1623M, Trainable Parameters 4.81654M\n"
     ]
    }
   ],
   "source": [
    "# Set Layer No = 5\n",
    "config[\"num_hidden_layers\"] = 5\n",
    "config['model_name'] = 'Hybrid-Ablation-TEL5'\n",
    "\n",
    "# Create the model, optimizer, loss function and trainer\n",
    "model = ViTForClassfication(config)\n",
    "print(model)\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(f\"{config['weight_base_dir']}{config['model_name']}/model_best_{config['model_name']}.pt\"))\n",
    "# Freeze Transformer layer\n",
    "model.encoder.requires_grad_(False)\n",
    "\n",
    "count_parameters(model)\n",
    "# Get number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}, Trainable Parameters {total_trainable_params}\\nTotal parameters: {total_params/1000000}M, Trainable Parameters {total_trainable_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "915f5d34-dde8-40fa-81b5-c6c84f58f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "save_model_every_n_epochs = config['save_model_every']\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-3)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20, 30, 40, 50], gamma=0.7)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbdd45d3-7b33-40c2-b225-dd5d64acba09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 0.0269, Train Acc: 0.9902, Val loss: 0.3988, Val Acc: 0.9387, Time: 2.3min (2.3 min/epoch)\n",
      "\tSave best checkpoint at epoch 1\n",
      "Epoch: 2, Train loss: 0.0368, Train Acc: 0.9889, Val loss: 0.3890, Val Acc: 0.9294, Time: 4.6min (2.3 min/epoch)\n",
      "Epoch: 3, Train loss: 0.0383, Train Acc: 0.9856, Val loss: 0.5612, Val Acc: 0.9049, Time: 6.9min (2.3 min/epoch)\n",
      "Epoch: 4, Train loss: 0.0158, Train Acc: 0.9948, Val loss: 0.3148, Val Acc: 0.9509, Time: 9.3min (2.3 min/epoch)\n",
      "\tSave best checkpoint at epoch 4\n",
      "Epoch: 5, Train loss: 0.0107, Train Acc: 0.9967, Val loss: 0.3740, Val Acc: 0.9264, Time: 11.6min (2.3 min/epoch)\n",
      "Epoch: 6, Train loss: 0.0269, Train Acc: 0.9928, Val loss: 0.2299, Val Acc: 0.9509, Time: 13.9min (2.3 min/epoch)\n",
      "Epoch: 7, Train loss: 0.0008, Train Acc: 1.0000, Val loss: 0.2249, Val Acc: 0.9601, Time: 16.2min (2.3 min/epoch)\n",
      "\tSave best checkpoint at epoch 7\n",
      "Epoch: 8, Train loss: 0.0684, Train Acc: 0.9751, Val loss: 1.1087, Val Acc: 0.8405, Time: 18.6min (2.3 min/epoch)\n",
      "Epoch: 9, Train loss: 0.0428, Train Acc: 0.9843, Val loss: 0.3258, Val Acc: 0.9356, Time: 20.8min (2.3 min/epoch)\n",
      "Epoch: 10, Train loss: 0.0054, Train Acc: 0.9993, Val loss: 0.2651, Val Acc: 0.9356, Time: 23.2min (2.3 min/epoch)\n",
      "Epoch: 11, Train loss: 0.0014, Train Acc: 1.0000, Val loss: 0.3143, Val Acc: 0.9479, Time: 25.5min (2.3 min/epoch)\n",
      "Epoch: 12, Train loss: 0.0080, Train Acc: 0.9974, Val loss: 0.1958, Val Acc: 0.9663, Time: 27.8min (2.3 min/epoch)\n",
      "Epoch: 13, Train loss: 0.0336, Train Acc: 0.9895, Val loss: 0.2665, Val Acc: 0.9509, Time: 30.1min (2.3 min/epoch)\n",
      "Epoch: 14, Train loss: 0.0099, Train Acc: 0.9961, Val loss: 0.3265, Val Acc: 0.9356, Time: 32.4min (2.3 min/epoch)\n",
      "Epoch: 15, Train loss: 0.0011, Train Acc: 1.0000, Val loss: 0.2604, Val Acc: 0.9509, Time: 34.8min (2.3 min/epoch)\n",
      "Epoch: 16, Train loss: 0.0003, Train Acc: 1.0000, Val loss: 0.2659, Val Acc: 0.9509, Time: 37.1min (2.3 min/epoch)\n",
      "Epoch: 17, Train loss: 0.0002, Train Acc: 1.0000, Val loss: 0.2850, Val Acc: 0.9509, Time: 39.4min (2.3 min/epoch)\n",
      "Epoch: 18, Train loss: 0.0002, Train Acc: 1.0000, Val loss: 0.2581, Val Acc: 0.9540, Time: 41.7min (2.3 min/epoch)\n",
      "Epoch: 19, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.2539, Val Acc: 0.9540, Time: 44.0min (2.3 min/epoch)\n",
      "Epoch: 20, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.2849, Val Acc: 0.9509, Time: 46.3min (2.3 min/epoch)\n",
      "Epoch: 21, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.2714, Val Acc: 0.9540, Time: 48.6min (2.3 min/epoch)\n",
      "Epoch: 22, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.2724, Val Acc: 0.9540, Time: 50.9min (2.3 min/epoch)\n",
      "Epoch: 23, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2759, Val Acc: 0.9540, Time: 53.3min (2.3 min/epoch)\n",
      "Epoch: 24, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2850, Val Acc: 0.9571, Time: 55.6min (2.3 min/epoch)\n",
      "Epoch: 25, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2963, Val Acc: 0.9571, Time: 57.9min (2.3 min/epoch)\n",
      "Epoch: 26, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2878, Val Acc: 0.9540, Time: 60.2min (2.3 min/epoch)\n",
      "Epoch: 27, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2878, Val Acc: 0.9601, Time: 62.5min (2.3 min/epoch)\n",
      "Epoch: 28, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2989, Val Acc: 0.9571, Time: 64.8min (2.3 min/epoch)\n",
      "Epoch: 29, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2916, Val Acc: 0.9601, Time: 67.2min (2.3 min/epoch)\n",
      "Epoch: 30, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2983, Val Acc: 0.9601, Time: 69.5min (2.3 min/epoch)\n",
      "Epoch: 31, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2802, Val Acc: 0.9571, Time: 71.8min (2.3 min/epoch)\n",
      "Epoch: 32, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2781, Val Acc: 0.9571, Time: 74.1min (2.3 min/epoch)\n",
      "Epoch: 33, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2952, Val Acc: 0.9601, Time: 76.5min (2.3 min/epoch)\n",
      "Epoch: 34, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2884, Val Acc: 0.9571, Time: 78.8min (2.3 min/epoch)\n",
      "Epoch: 35, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2941, Val Acc: 0.9571, Time: 81.1min (2.3 min/epoch)\n",
      "Epoch: 36, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2947, Val Acc: 0.9571, Time: 83.5min (2.3 min/epoch)\n",
      "Epoch: 37, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2859, Val Acc: 0.9571, Time: 85.8min (2.3 min/epoch)\n",
      "Epoch: 38, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2879, Val Acc: 0.9540, Time: 88.1min (2.3 min/epoch)\n",
      "Epoch: 39, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2961, Val Acc: 0.9571, Time: 90.5min (2.3 min/epoch)\n",
      "Epoch: 40, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2950, Val Acc: 0.9571, Time: 92.8min (2.3 min/epoch)\n",
      "Epoch: 41, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2786, Val Acc: 0.9571, Time: 95.1min (2.3 min/epoch)\n",
      "Epoch: 42, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2940, Val Acc: 0.9571, Time: 97.4min (2.3 min/epoch)\n",
      "Epoch: 43, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2756, Val Acc: 0.9571, Time: 99.8min (2.3 min/epoch)\n",
      "Epoch: 44, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2677, Val Acc: 0.9601, Time: 102.1min (2.3 min/epoch)\n",
      "Epoch: 45, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.3148, Val Acc: 0.9540, Time: 104.4min (2.3 min/epoch)\n",
      "Epoch: 46, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.3061, Val Acc: 0.9601, Time: 106.7min (2.3 min/epoch)\n",
      "Epoch: 47, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2988, Val Acc: 0.9571, Time: 109.1min (2.3 min/epoch)\n",
      "Epoch: 48, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.3020, Val Acc: 0.9571, Time: 111.4min (2.3 min/epoch)\n",
      "Epoch: 49, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2894, Val Acc: 0.9540, Time: 113.7min (2.3 min/epoch)\n",
      "Epoch: 50, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2837, Val Acc: 0.9601, Time: 116.0min (2.3 min/epoch)\n",
      "\n",
      "\n",
      "Test Loss: 0.2810181163897251 and Test Accuracy: 0.9636363636363636\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, optimizer, loss_fn, config['model_name'], device=device)\n",
    "trainer.train(train_loader, valid_loader, config['epochs'], scheduler, save_model_every_n_epochs=save_model_every_n_epochs)\n",
    "test_loss, test_acc = trainer.evaluate(test_loader)\n",
    "print(f\"\\n\\nTest Loss: {test_loss} and Test Accuracy: {test_acc}\")\n",
    "#wandb.log({\"Test Loss\": test_loss, \"Test Accuracy\": test_acc})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b5b80-60d1-47ce-a570-65b420e41a46",
   "metadata": {},
   "source": [
    "## **Hybrid-ViT (Transformer Encoder Layer No - 6)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1e71054-3f8c-4cb7-9b30-088e7e026a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 25 21:11:02 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0 Off |                  Off |\n",
      "| 30%   50C    P2             186W / 450W |   4480MiB / 24564MiB |     81%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2411      G   /usr/lib/xorg/Xorg                          206MiB |\n",
      "|    0   N/A  N/A    551796      G   /usr/lib/xorg/Xorg                          405MiB |\n",
      "|    0   N/A  N/A    552008      G   xfwm4                                         7MiB |\n",
      "|    0   N/A  N/A    812763      G   ...irefox/3416/usr/lib/firefox/firefox      340MiB |\n",
      "|    0   N/A  N/A   1249667      G   ...seed-version=20231208-102139.191000       32MiB |\n",
      "|    0   N/A  N/A   1265111      C   /usr/bin/python3                           3300MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ffa88b2-c55e-4d07-b35f-e78ba38a1d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTForClassfication(\n",
      "  (embedding): Embeddings(\n",
      "    (patch_embeddings): PatchEmbeddings(\n",
      "      (conv_1): ConvBlock(\n",
      "        (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_2): ConvBlock(\n",
      "        (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_3): ConvBlock(\n",
      "        (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_4): ConvBlock(\n",
      "        (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_5): ConvBlock(\n",
      "        (conv): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "        (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (blocks): ModuleList(\n",
      "      (0-5): 6 x Block(\n",
      "        (attention): FasterMultiHeadAttention(\n",
      "          (qkv_projection): Linear(in_features=216, out_features=648, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.25, inplace=False)\n",
      "          (output_projection): Linear(in_features=216, out_features=216, bias=True)\n",
      "          (output_dropout): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "        (layernorm_1): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (dense_1): Linear(in_features=216, out_features=648, bias=True)\n",
      "          (activation): NewGELUActivation()\n",
      "          (dense_2): Linear(in_features=648, out_features=216, bias=True)\n",
      "          (dropout): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "        (layernorm_2): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (attention_pool): Linear(in_features=216, out_features=1, bias=True)\n",
      "  (classifier): Linear(in_features=432, out_features=3, bias=True)\n",
      ")\n",
      "+-----------------------------------------------+------------+\n",
      "|                    Modules                    | Parameters |\n",
      "+-----------------------------------------------+------------+\n",
      "|              embedding.cls_token              |    216     |\n",
      "|         embedding.position_embeddings         |   110808   |\n",
      "| embedding.patch_embeddings.conv_1.conv.weight |    864     |\n",
      "|  embedding.patch_embeddings.conv_1.conv.bias  |     32     |\n",
      "|  embedding.patch_embeddings.conv_1.bn.weight  |     32     |\n",
      "|   embedding.patch_embeddings.conv_1.bn.bias   |     32     |\n",
      "| embedding.patch_embeddings.conv_2.conv.weight |   55296    |\n",
      "|  embedding.patch_embeddings.conv_2.conv.bias  |     64     |\n",
      "|  embedding.patch_embeddings.conv_2.bn.weight  |     64     |\n",
      "|   embedding.patch_embeddings.conv_2.bn.bias   |     64     |\n",
      "| embedding.patch_embeddings.conv_3.conv.weight |   221184   |\n",
      "|  embedding.patch_embeddings.conv_3.conv.bias  |    128     |\n",
      "|  embedding.patch_embeddings.conv_3.bn.weight  |    128     |\n",
      "|   embedding.patch_embeddings.conv_3.bn.bias   |    128     |\n",
      "| embedding.patch_embeddings.conv_4.conv.weight |   884736   |\n",
      "|  embedding.patch_embeddings.conv_4.conv.bias  |    256     |\n",
      "|  embedding.patch_embeddings.conv_4.bn.weight  |    256     |\n",
      "|   embedding.patch_embeddings.conv_4.bn.bias   |    256     |\n",
      "| embedding.patch_embeddings.conv_5.conv.weight |  3538944   |\n",
      "|  embedding.patch_embeddings.conv_5.conv.bias  |    512     |\n",
      "|  embedding.patch_embeddings.conv_5.bn.weight  |    512     |\n",
      "|   embedding.patch_embeddings.conv_5.bn.bias   |    512     |\n",
      "|             attention_pool.weight             |    216     |\n",
      "|              attention_pool.bias              |     1      |\n",
      "|               classifier.weight               |    1296    |\n",
      "|                classifier.bias                |     3      |\n",
      "+-----------------------------------------------+------------+\n",
      "Total Trainable Params: 4816540\n",
      "Total parameters: 7631452, Trainable Parameters 4816540\n",
      "Total parameters: 7.631452M, Trainable Parameters 4.81654M\n"
     ]
    }
   ],
   "source": [
    "# Set Layer No = 6\n",
    "config[\"num_hidden_layers\"] = 6\n",
    "config['model_name'] = 'Hybrid-Ablation-TEL6'\n",
    "\n",
    "# Create the model, optimizer, loss function and trainer\n",
    "model = ViTForClassfication(config)\n",
    "print(model)\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(f\"{config['weight_base_dir']}{config['model_name']}/model_best_{config['model_name']}.pt\"))\n",
    "# Freeze Transformer layer\n",
    "model.encoder.requires_grad_(False)\n",
    "\n",
    "count_parameters(model)\n",
    "# Get number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}, Trainable Parameters {total_trainable_params}\\nTotal parameters: {total_params/1000000}M, Trainable Parameters {total_trainable_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cdefa78-19a7-4f69-973f-e78f646d5541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "save_model_every_n_epochs = config['save_model_every']\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-3)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20, 30, 40, 50], gamma=0.7)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6531923-4416-4ae1-99e7-729f172db2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 0.0596, Train Acc: 0.9817, Val loss: 0.2113, Val Acc: 0.9417, Time: 2.3min (2.3 min/epoch)\n",
      "\tSave best checkpoint at epoch 1\n",
      "Epoch: 2, Train loss: 0.0139, Train Acc: 0.9934, Val loss: 0.2615, Val Acc: 0.9509, Time: 4.6min (2.3 min/epoch)\n",
      "Epoch: 3, Train loss: 0.0162, Train Acc: 0.9948, Val loss: 0.3480, Val Acc: 0.9356, Time: 7.0min (2.3 min/epoch)\n",
      "Epoch: 4, Train loss: 0.0274, Train Acc: 0.9934, Val loss: 0.4481, Val Acc: 0.9049, Time: 9.3min (2.3 min/epoch)\n",
      "Epoch: 5, Train loss: 0.0494, Train Acc: 0.9862, Val loss: 0.5578, Val Acc: 0.8865, Time: 11.6min (2.3 min/epoch)\n",
      "Epoch: 6, Train loss: 0.0027, Train Acc: 1.0000, Val loss: 0.2444, Val Acc: 0.9479, Time: 13.9min (2.3 min/epoch)\n",
      "Epoch: 7, Train loss: 0.0187, Train Acc: 0.9941, Val loss: 0.6851, Val Acc: 0.8681, Time: 16.2min (2.3 min/epoch)\n",
      "Epoch: 8, Train loss: 0.0726, Train Acc: 0.9738, Val loss: 0.4104, Val Acc: 0.9080, Time: 18.6min (2.3 min/epoch)\n",
      "Epoch: 9, Train loss: 0.0115, Train Acc: 0.9967, Val loss: 0.1363, Val Acc: 0.9724, Time: 20.9min (2.3 min/epoch)\n",
      "\tSave best checkpoint at epoch 9\n",
      "Epoch: 10, Train loss: 0.0368, Train Acc: 0.9895, Val loss: 0.4689, Val Acc: 0.9018, Time: 23.2min (2.3 min/epoch)\n",
      "Epoch: 11, Train loss: 0.0021, Train Acc: 1.0000, Val loss: 0.2131, Val Acc: 0.9540, Time: 25.6min (2.3 min/epoch)\n",
      "Epoch: 12, Train loss: 0.0024, Train Acc: 0.9993, Val loss: 0.2427, Val Acc: 0.9479, Time: 27.9min (2.3 min/epoch)\n",
      "Epoch: 13, Train loss: 0.0004, Train Acc: 1.0000, Val loss: 0.1947, Val Acc: 0.9632, Time: 30.2min (2.3 min/epoch)\n",
      "Epoch: 14, Train loss: 0.0002, Train Acc: 1.0000, Val loss: 0.2056, Val Acc: 0.9601, Time: 32.5min (2.3 min/epoch)\n",
      "Epoch: 15, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.2030, Val Acc: 0.9632, Time: 34.9min (2.3 min/epoch)\n",
      "Epoch: 16, Train loss: 0.0002, Train Acc: 1.0000, Val loss: 0.2037, Val Acc: 0.9601, Time: 37.2min (2.3 min/epoch)\n",
      "Epoch: 17, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.1943, Val Acc: 0.9601, Time: 39.5min (2.3 min/epoch)\n",
      "Epoch: 18, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.1966, Val Acc: 0.9632, Time: 41.8min (2.3 min/epoch)\n",
      "Epoch: 19, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.2215, Val Acc: 0.9632, Time: 44.1min (2.3 min/epoch)\n",
      "Epoch: 20, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2082, Val Acc: 0.9632, Time: 46.5min (2.3 min/epoch)\n",
      "Epoch: 21, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2038, Val Acc: 0.9632, Time: 48.8min (2.3 min/epoch)\n",
      "Epoch: 22, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1937, Val Acc: 0.9663, Time: 51.1min (2.3 min/epoch)\n",
      "Epoch: 23, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1961, Val Acc: 0.9632, Time: 53.4min (2.3 min/epoch)\n",
      "Epoch: 24, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.2260, Val Acc: 0.9663, Time: 55.7min (2.3 min/epoch)\n",
      "Epoch: 25, Train loss: 0.0187, Train Acc: 0.9928, Val loss: 0.5169, Val Acc: 0.9110, Time: 58.1min (2.3 min/epoch)\n",
      "Epoch: 26, Train loss: 0.0055, Train Acc: 0.9993, Val loss: 0.2421, Val Acc: 0.9509, Time: 60.4min (2.3 min/epoch)\n",
      "Epoch: 27, Train loss: 0.0008, Train Acc: 1.0000, Val loss: 0.2085, Val Acc: 0.9601, Time: 62.8min (2.3 min/epoch)\n",
      "Epoch: 28, Train loss: 0.0015, Train Acc: 0.9993, Val loss: 0.2225, Val Acc: 0.9601, Time: 65.1min (2.3 min/epoch)\n",
      "Epoch: 29, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.2110, Val Acc: 0.9693, Time: 67.4min (2.3 min/epoch)\n",
      "Epoch: 30, Train loss: 0.0001, Train Acc: 1.0000, Val loss: 0.1977, Val Acc: 0.9663, Time: 69.7min (2.3 min/epoch)\n",
      "Epoch: 31, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1989, Val Acc: 0.9663, Time: 72.0min (2.3 min/epoch)\n",
      "Epoch: 32, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1910, Val Acc: 0.9663, Time: 74.4min (2.3 min/epoch)\n",
      "Epoch: 33, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1891, Val Acc: 0.9663, Time: 76.7min (2.3 min/epoch)\n",
      "Epoch: 34, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1913, Val Acc: 0.9663, Time: 79.0min (2.3 min/epoch)\n",
      "Epoch: 35, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1821, Val Acc: 0.9693, Time: 81.4min (2.3 min/epoch)\n",
      "Epoch: 36, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1863, Val Acc: 0.9693, Time: 83.8min (2.3 min/epoch)\n",
      "Epoch: 37, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1807, Val Acc: 0.9724, Time: 86.1min (2.3 min/epoch)\n",
      "Epoch: 38, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1754, Val Acc: 0.9724, Time: 88.4min (2.3 min/epoch)\n",
      "Epoch: 39, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1709, Val Acc: 0.9693, Time: 90.8min (2.3 min/epoch)\n",
      "Epoch: 40, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1708, Val Acc: 0.9724, Time: 93.1min (2.3 min/epoch)\n",
      "Epoch: 41, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1901, Val Acc: 0.9663, Time: 95.5min (2.3 min/epoch)\n",
      "Epoch: 42, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1645, Val Acc: 0.9663, Time: 97.9min (2.3 min/epoch)\n",
      "Epoch: 43, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1542, Val Acc: 0.9693, Time: 100.3min (2.3 min/epoch)\n",
      "Epoch: 44, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1620, Val Acc: 0.9724, Time: 102.6min (2.3 min/epoch)\n",
      "Epoch: 45, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1556, Val Acc: 0.9755, Time: 105.0min (2.3 min/epoch)\n",
      "Epoch: 46, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1649, Val Acc: 0.9785, Time: 107.3min (2.3 min/epoch)\n",
      "Epoch: 47, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1699, Val Acc: 0.9755, Time: 109.6min (2.3 min/epoch)\n",
      "Epoch: 48, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1583, Val Acc: 0.9785, Time: 111.9min (2.3 min/epoch)\n",
      "Epoch: 49, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1583, Val Acc: 0.9755, Time: 114.2min (2.3 min/epoch)\n",
      "Epoch: 50, Train loss: 0.0000, Train Acc: 1.0000, Val loss: 0.1613, Val Acc: 0.9816, Time: 116.5min (2.3 min/epoch)\n",
      "\n",
      "\n",
      "Test Loss: 0.34920839518844266 and Test Accuracy: 0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, optimizer, loss_fn, config['model_name'], device=device)\n",
    "trainer.train(train_loader, valid_loader, config['epochs'], scheduler, save_model_every_n_epochs=save_model_every_n_epochs)\n",
    "test_loss, test_acc = trainer.evaluate(test_loader)\n",
    "print(f\"\\n\\nTest Loss: {test_loss} and Test Accuracy: {test_acc}\")\n",
    "#wandb.log({\"Test Loss\": test_loss, \"Test Accuracy\": test_acc})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
