{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e3e5815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "DATA_PATH = os.path.join('/home/user/Alzheimer/PreprocessedData/adni1-complete-3yr-1-5t', 'ADNI')\n",
    "config = {\n",
    "    'img_size': 192,\n",
    "    'depth' : 192\n",
    "}\n",
    "labels_path = '/home/user/Alzheimer/Data/adni1-complete-3yr-1-5t/ADNI1_Complete_3Yr_1.5T_9_12_2023.csv'\n",
    "random.seed(37)\n",
    "\n",
    "class DataPaths():\n",
    "    def __init__(self, data_path=None, csv_path=None):\n",
    "        if data_path==None:\n",
    "            self.data_path = DATA_PATH\n",
    "        else:\n",
    "            self.data_path = data_path\n",
    "\n",
    "        if csv_path==None:\n",
    "            self.csv_path = labels_path\n",
    "        else:\n",
    "            self.csv_path = csv_path\n",
    "\n",
    "    def patient_id_loading(self):\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        print(\"Total number of images: \", len(df))\n",
    "        cn_mri_scan_list, mci_mri_scan_list, ad_mri_scan_list = [], [], []\n",
    "        idx = 1\n",
    "\n",
    "        for patient_dir in os.listdir(self.data_path):\n",
    "            patient_dir_path = os.path.join(self.data_path, patient_dir)\n",
    "            if os.path.isdir(patient_dir_path):\n",
    "                for des_dir in os.listdir(patient_dir_path):\n",
    "                    des_dir_path = os.path.join(patient_dir_path, des_dir)\n",
    "                    if os.path.isdir(des_dir_path):\n",
    "                        for visit in os.listdir(des_dir_path):\n",
    "                            visit_path = os.path.join(des_dir_path, visit)\n",
    "                            if os.path.isdir(visit_path):\n",
    "                                for image_data_dir in os.listdir(visit_path):\n",
    "                                    image_data_dir_path = os.path.join(visit_path, image_data_dir)\n",
    "                                    if os.path.isdir(image_data_dir_path):\n",
    "                                        for image in os.listdir(image_data_dir_path):\n",
    "                                            image_dict = {}\n",
    "                                            image_path = os.path.join(image_data_dir_path, image)\n",
    "                                            if image.endswith('mni_norm.nii.gz'):\n",
    "                                                image_dict['image_path'] = image_path\n",
    "                                                image_dict['patient_id'] = patient_dir\n",
    "                                                image_dict['image_id'] = image_data_dir\n",
    "                                                image_dict['label'] = df[df['Image Data ID']==image_data_dir]['Group'].values[0]\n",
    "\n",
    "                                                if image_dict['label']=='CN':\n",
    "                                                    cn_mri_scan_list.append(image_dict)\n",
    "                                                elif image_dict['label']=='MCI':\n",
    "                                                    mci_mri_scan_list.append(image_dict)\n",
    "                                                elif image_dict['label']=='AD':\n",
    "                                                    ad_mri_scan_list.append(image_dict)\n",
    "                                                    \n",
    "                                                if idx > 0:\n",
    "                                                    idx -= 1\n",
    "                                                    print(image_dict)\n",
    "            \n",
    "        \n",
    "        random.shuffle(cn_mri_scan_list)\n",
    "        random.shuffle(mci_mri_scan_list)\n",
    "        random.shuffle(ad_mri_scan_list)\n",
    "        no_of_images = {\n",
    "            'train_cn' : int(len(cn_mri_scan_list)*0.7),\n",
    "            'train_mci' : int(len(mci_mri_scan_list)*0.7),\n",
    "            'train_ad' : int(len(ad_mri_scan_list)*0.7),\n",
    "            'val_cn' : int(len(cn_mri_scan_list)*0.15),\n",
    "            'val_mci' : int(len(mci_mri_scan_list)*0.15),\n",
    "            'val_ad' : int(len(ad_mri_scan_list)*0.15),\n",
    "            'test_cn' : len(cn_mri_scan_list) - int(len(cn_mri_scan_list)*0.7) - int(len(cn_mri_scan_list)*0.15),\n",
    "            'test_mci' : len(mci_mri_scan_list) - int(len(mci_mri_scan_list)*0.7) - int(len(mci_mri_scan_list)*0.15),\n",
    "            'test_ad' : len(ad_mri_scan_list) - int(len(ad_mri_scan_list)*0.7) - int(len(ad_mri_scan_list)*0.15)\n",
    "        }\n",
    "        print(no_of_images)\n",
    "        len_train = no_of_images['train_cn'] + no_of_images['train_mci'] + no_of_images['train_ad']\n",
    "        len_val = no_of_images['val_cn'] + no_of_images['val_mci'] + no_of_images['val_ad']\n",
    "        len_test = no_of_images['test_cn'] + no_of_images['test_mci'] + no_of_images['test_ad']\n",
    "        print(\"Total number of train, validation and test images are {}, {} and {} respectively.\".format(len_train, len_val, len_test))\n",
    "        \n",
    "        save_path = os.path.join(os.getcwd(), 'data')\n",
    "        if os.path.exists(save_path)==False:\n",
    "            os.mkdir(save_path)\n",
    "\n",
    "        trin_img_df = pd.DataFrame(cn_mri_scan_list[:no_of_images['train_cn']]+\\\n",
    "                                   mci_mri_scan_list[:no_of_images['train_mci']]+\\\n",
    "                                   ad_mri_scan_list[:no_of_images['train_ad']])\n",
    "        trin_img_df_path = os.path.join(save_path, 'train_mri_scan_list.csv')\n",
    "        trin_img_df.to_csv(trin_img_df_path, index=False)\n",
    "\n",
    "        val_img_df = pd.DataFrame(cn_mri_scan_list[no_of_images['train_cn']:no_of_images['train_cn']+no_of_images['val_cn']]+\\\n",
    "                                   mci_mri_scan_list[no_of_images['train_mci']:no_of_images['train_mci']+no_of_images['val_mci']]+\\\n",
    "                                   ad_mri_scan_list[no_of_images['train_ad']:no_of_images['train_ad']+no_of_images['val_ad']])\n",
    "        val_img_df_path = os.path.join(save_path, 'val_mri_scan_list.csv')\n",
    "        val_img_df.to_csv(val_img_df_path, index=False)\n",
    "\n",
    "        test_img_df = pd.DataFrame(cn_mri_scan_list[no_of_images['train_cn']+no_of_images['val_cn']:]+\\\n",
    "                                   mci_mri_scan_list[no_of_images['train_mci']+no_of_images['val_mci']:]+\\\n",
    "                                   ad_mri_scan_list[no_of_images['train_ad']+no_of_images['val_ad']:])\n",
    "        test_img_df_path = os.path.join(save_path, 'test_mri_scan_list.csv')\n",
    "        test_img_df.to_csv(test_img_df_path, index=False)\n",
    "\n",
    "        return trin_img_df_path, val_img_df_path, test_img_df_path\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class ADNIAlzheimerDataset(Dataset):\n",
    "    def __init__(self, image_df_paths, transform=None):\n",
    "        self.image_df_paths = image_df_paths\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(self.image_df_paths)\n",
    "        self.desired_width = config['img_size']\n",
    "        self.desired_height = config['img_size']\n",
    "        self.desired_depth = config['depth']\n",
    "        self.transform = transforms.Compose([\n",
    "                                transforms.RandomHorizontalFlip(0.5),\n",
    "                                transforms.RandomVerticalFlip(p=0.5),\n",
    "                                transforms.RandomAffine(15),\n",
    "                                transforms.ToTensor()\n",
    "                                #transforms.functional.equalize\n",
    "                                #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                            ])\n",
    "        #labels = self.df['label'].values\n",
    "\n",
    "    def __label_extract(self, group):\n",
    "        if group=='CN':\n",
    "            return 0\n",
    "        elif group=='MCI':\n",
    "            return 1\n",
    "        elif group=='AD':\n",
    "            return 2\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = {}\n",
    "        image_filepath = self.df['image_path'][idx]\n",
    "        image = nib.as_closest_canonical(nib.load(image_filepath))\n",
    "        image = image.get_fdata()\n",
    "        xdim, ydim, zdim = image.shape\n",
    "        image = np.pad(image, [((256-xdim)//2, (256-xdim)//2), ((256-ydim)//2, (256-ydim)//2), ((256-zdim)//2, (256-zdim)//2)], 'constant', constant_values=0)\n",
    "        #image = image.reshape(image.shape[2], image.shape[1], image.shape[0])\n",
    "\n",
    "        width_factor = self.desired_width / image.shape[0]\n",
    "        height_factor = self.desired_height / image.shape[1]\n",
    "        depth_factor = self.desired_depth / image.shape[-1]\n",
    "\n",
    "        image = zoom(image, (width_factor, height_factor, depth_factor), order=1)\n",
    "        image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "        image = image.astype('float32')\n",
    "        image = torch.from_numpy(image)\n",
    "        \n",
    "        label = self.df['label'][idx]\n",
    "        label = self.__label_extract(label)\n",
    "        \n",
    "        return image, label\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "245e2385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images:  2182\n",
      "{'image_path': '/home/arindam/Alzheimer/PreprocessedData/adni1-complete-3yr-1-5t/ADNI/013_S_1035/MPR-R__GradWarp__B1_Correction__N3__Scaled/2008-11-18_14_09_52.0/I133937/ADNI_013_S_1035_MR_MPR-R__GradWarp__B1_Correction__N3__Scaled_Br_20090116082837270_S60960_I133937_mni_norm.nii.gz', 'patient_id': '013_S_1035', 'image_id': 'I133937', 'label': 'CN'}\n",
      "{'train_cn': 523, 'train_mci': 686, 'train_ad': 317, 'val_cn': 112, 'val_mci': 147, 'val_ad': 67, 'test_cn': 113, 'test_mci': 148, 'test_ad': 69}\n",
      "Total number of train, validation and test images are 1526, 326 and 330 respectively.\n"
     ]
    }
   ],
   "source": [
    "dataPath = DataPaths()\n",
    "trin_img_df_path, val_img_df_path, test_img_df_path = dataPath.patient_id_loading()\n",
    "\n",
    "train_dataset = ADNIAlzheimerDataset(trin_img_df_path)\n",
    "val_dataset = ADNIAlzheimerDataset(val_img_df_path)\n",
    "test_dataset = ADNIAlzheimerDataset(test_img_df_path)\n",
    "\n",
    "def saveTensors(dataset, data_type):\n",
    "    path = '/home/user/Alzheimer/ViT/data/3D (part II)'\n",
    "    data_path = os.path.join(path, data_type)\n",
    "    if os.path.exists(data_path)==False:\n",
    "        os.mkdir(data_path)\n",
    "    \n",
    "    labels = {\n",
    "        0 : 'CN',\n",
    "        1 : 'MCI',\n",
    "        2 : 'AD'\n",
    "    }\n",
    "    \n",
    "    for label in labels.keys():\n",
    "        os.mkdir(os.path.join(data_path, labels[label]))\n",
    "    \n",
    "    print(f\"Processing for {data_type} data is starting. Data will be saved at {data_path}\")\n",
    "    print(f\"Total number of images are: {len(dataset)}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    for idx in range(len(dataset)):\n",
    "        tensor, label = dataset.__getitem__(idx)\n",
    "        tensor_path = f\"{data_path}/{labels[label]}/{idx}.pt\"\n",
    "        torch.save(tensor, tensor_path)\n",
    "        \n",
    "        if (idx+1)%100==0:\n",
    "            print(f\"{idx+1} images done.\")\n",
    "    \n",
    "    req_time = time.time() - start\n",
    "    print(f\"Total time required for processing the data is {req_time// 60} minutes {req_time%60} sec.\")\n",
    "    print(f\"Processing of a single image took {req_time/(1.0*len(dataset))} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63fda111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for Test data is starting. Data will be saved at /home/arindam/Alzheimer/ViT/data/3D (part II)/Test\n",
      "Total number of images are: 330\n",
      "100 images done.\n",
      "200 images done.\n",
      "300 images done.\n",
      "Total time required for processing the data is 1.0 minutes 54.84212398529053 sec.\n",
      "Processing of a single image took 0.3480064363190622 sec.\n"
     ]
    }
   ],
   "source": [
    "saveTensors(test_dataset, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d1f73c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for Val data is starting. Data will be saved at /home/arindam/Alzheimer/ViT/data/3D (part II)/Val\n",
      "Total number of images are: 326\n",
      "100 images done.\n",
      "200 images done.\n",
      "300 images done.\n",
      "Total time required for processing the data is 1.0 minutes 52.27528405189514 sec.\n",
      "Processing of a single image took 0.34440271181562926 sec.\n"
     ]
    }
   ],
   "source": [
    "saveTensors(val_dataset, 'Val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f898f8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for Train data is starting. Data will be saved at /home/arindam/Alzheimer/ViT/data/3D (part II)/Train\n",
      "Total number of images are: 1526\n",
      "100 images done.\n",
      "200 images done.\n",
      "300 images done.\n",
      "400 images done.\n",
      "500 images done.\n",
      "600 images done.\n",
      "700 images done.\n",
      "800 images done.\n",
      "900 images done.\n",
      "1000 images done.\n",
      "1100 images done.\n",
      "1200 images done.\n",
      "1300 images done.\n",
      "1400 images done.\n",
      "1500 images done.\n",
      "Total time required for processing the data is 8.0 minutes 46.0275559425354 sec.\n",
      "Processing of a single image took 0.34471006287191047 sec.\n"
     ]
    }
   ],
   "source": [
    "saveTensors(train_dataset, 'Train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
