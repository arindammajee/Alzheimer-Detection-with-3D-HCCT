{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cef269d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch import optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import neighbors\n",
    "import time\n",
    "from prettytable import PrettyTable\n",
    "from adniLoader import *\n",
    "import glob\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81aee1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ViT Implementation 🔥\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "\n",
    "    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert the image into patches and then project them into a vector space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        # Calculate the number of patches from the image size and patch size\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        # Create a projection layer to convert the image into patches\n",
    "        # The layer projects each patch into a vector of size hidden_size\n",
    "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, num_channels, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine the patch embeddings with the class token and position embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        # Create a learnable [CLS] token\n",
    "        # Similar to BERT, the [CLS] token is added to the beginning of the input sequence\n",
    "        # and is used to classify the entire sequence\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
    "        # Create position embeddings for the [CLS] token and the patch embeddings\n",
    "        # Add 1 to the sequence length for the [CLS] token\n",
    "        self.position_embeddings = \\\n",
    "            nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        # Expand the [CLS] token to the batch size\n",
    "        # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # Concatenate the [CLS] token to the beginning of the input sequence\n",
    "        # This results in a sequence length of (num_patches + 1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single attention head.\n",
    "    This module is used in the MultiHeadAttention module.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "        # Create the query, key, and value projection layers\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project the input into query, key, and value\n",
    "        # The same input is used to generate the query, key, and value,\n",
    "        # so it's usually called self-attention.\n",
    "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, attention_head_size)\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        # Calculate the attention scores\n",
    "        # softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        # Calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module.\n",
    "    This module is used in the TransformerEncoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a list of attention heads\n",
    "        self.heads = nn.ModuleList([])\n",
    "        for _ in range(self.num_attention_heads):\n",
    "            head = AttentionHead(\n",
    "                self.hidden_size,\n",
    "                self.attention_head_size,\n",
    "                config[\"attention_probs_dropout_prob\"],\n",
    "                self.qkv_bias\n",
    "            )\n",
    "            self.heads.append(head)\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the attention output for each attention head\n",
    "        attention_outputs = [head(x) for head in self.heads]\n",
    "        # Concatenate the attention outputs from each attention head\n",
    "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
    "        # Project the concatenated attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
    "            return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class FasterMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module with some optimizations.\n",
    "    All the heads are processed simultaneously with merged query, key, and value projections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a linear layer to project the query, key, and value\n",
    "        self.qkv_projection = nn.Linear(self.hidden_size, self.all_head_size * 3, bias=self.qkv_bias)\n",
    "        self.attn_dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Project the query, key, and value\n",
    "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, all_head_size * 3)\n",
    "        qkv = self.qkv_projection(x)\n",
    "        # Split the projected query, key, and value into query, key, and value\n",
    "        # (batch_size, sequence_length, all_head_size * 3) -> (batch_size, sequence_length, all_head_size)\n",
    "        query, key, value = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Resize the query, key, and value to (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
    "        batch_size, sequence_length, _ = query.size()\n",
    "        query = query.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        key = key.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        value = value.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        # Calculate the attention scores\n",
    "        # softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        # Calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        # Resize the attention output\n",
    "        # from (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
    "        # To (batch_size, sequence_length, all_head_size)\n",
    "        attention_output = attention_output.transpose(1, 2) \\\n",
    "                                           .contiguous() \\\n",
    "                                           .view(batch_size, sequence_length, self.all_head_size)\n",
    "        # Project the attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-layer perceptron module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
    "        self.activation = NewGELUActivation()\n",
    "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.use_faster_attention = config.get(\"use_faster_attention\", False)\n",
    "        if self.use_faster_attention:\n",
    "            self.attention = FasterMultiHeadAttention(config)\n",
    "        else:\n",
    "            self.attention = MultiHeadAttention(config)\n",
    "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Self-attention\n",
    "        attention_output, attention_probs = \\\n",
    "            self.attention(self.layernorm_1(x), output_attentions=output_attentions)\n",
    "        # Skip connection\n",
    "        x = x + attention_output\n",
    "        # Feed-forward network\n",
    "        mlp_output = self.mlp(self.layernorm_2(x))\n",
    "        # Skip connection\n",
    "        x = x + mlp_output\n",
    "        # Return the transformer block's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, attention_probs)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The transformer encoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Create a list of transformer blocks\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config[\"num_hidden_layers\"]):\n",
    "            block = Block(config)\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the transformer block's output for each block\n",
    "        all_attentions = []\n",
    "        for block in self.blocks:\n",
    "            x, attention_probs = block(x, output_attentions=output_attentions)\n",
    "            if output_attentions:\n",
    "                all_attentions.append(attention_probs)\n",
    "        # Return the encoder's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, all_attentions)\n",
    "\n",
    "\n",
    "class ViTForClassfication(nn.Module):\n",
    "    \"\"\"\n",
    "    The ViT model for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        # Create the embedding module\n",
    "        self.embedding = Embeddings(config)\n",
    "        # Create the transformer encoder module\n",
    "        self.encoder = Encoder(config)\n",
    "        # Create a linear layer to project the encoder's output to the number of classes\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
    "        # Initialize the weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the embedding output\n",
    "        embedding_output = self.embedding(x)\n",
    "        # Calculate the encoder's output\n",
    "        encoder_output, all_attentions = self.encoder(embedding_output, output_attentions=output_attentions)\n",
    "        # Calculate the logits, take the [CLS] token's output as features for classification\n",
    "        logits = self.classifier(encoder_output[:, 0, :])\n",
    "        # Return the logits and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (logits, None)\n",
    "        else:\n",
    "            return (logits, all_attentions)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, Embeddings):\n",
    "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.position_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.position_embeddings.dtype)\n",
    "\n",
    "            module.cls_token.data = nn.init.trunc_normal_(\n",
    "                module.cls_token.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.cls_token.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "624cc242",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size' : 4,\n",
    "    'image_size' : 180,\n",
    "    'patch_size' : 15,\n",
    "    \"hidden_size\": 96,\n",
    "    \"num_hidden_layers\": 4,\n",
    "    \"num_attention_heads\": 6,\n",
    "    \"intermediate_size\": 2 * 96, # 4 * hidden_size\n",
    "    \"hidden_dropout_prob\": 0.2,\n",
    "    \"attention_probs_dropout_prob\": 0.2,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"num_classes\": 3, # num_classes\n",
    "    \"num_channels\": 1,\n",
    "    \"qkv_bias\": True,\n",
    "    \"use_faster_attention\": True,\n",
    "    'lr' : 1e-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14f64fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Prepare Data 📊\n",
    "# Import libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class FolderDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    "        self.image_paths = glob.glob(f'{self.folder}/*/*.pt')\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.load(f\"{self.folder}/{self.image_paths[idx]}\")\n",
    "    \n",
    "def prepare_data(batch_size=4, num_workers=2, train_sample_size=None, test_sample_size=None):\n",
    "    train_dataset = FolderDataset(folder='/home/arindam/Alzheimer/ViT/data/3D/Test')\n",
    "    val_dataset = FolderDataset(folder='/home/arindam/Alzheimer/ViT/data/3D/Test')\n",
    "    test_dataset = FolderDataset(folder='/home/arindam/Alzheimer/ViT/data/3D/Test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    classes = ('CN', 'MCI', 'AD')\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "249cb983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Utils 🛠️\n",
    "import json, os, math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def save_experiment(experiment_name, config, model, train_losses, test_losses, accuracies, base_dir=\"experiments\"):\n",
    "    outdir = os.path.join(base_dir, experiment_name)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Save the config\n",
    "    configfile = os.path.join(outdir, 'config.json')\n",
    "    with open(configfile, 'w') as f:\n",
    "        json.dump(config, f, sort_keys=True, indent=4)\n",
    "\n",
    "    # Save the metrics\n",
    "    jsonfile = os.path.join(outdir, 'metrics.json')\n",
    "    with open(jsonfile, 'w') as f:\n",
    "        data = {\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'accuracies': accuracies,\n",
    "        }\n",
    "        json.dump(data, f, sort_keys=True, indent=4)\n",
    "\n",
    "    # Save the model\n",
    "    save_checkpoint(experiment_name, model, \"final\", base_dir=base_dir)\n",
    "\n",
    "\n",
    "def save_checkpoint(experiment_name, model, epoch, base_dir=\"experiments\"):\n",
    "    outdir = os.path.join(base_dir, experiment_name)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    cpfile = os.path.join(outdir, f'model_{epoch}.pt')\n",
    "    torch.save(model.state_dict(), cpfile)\n",
    "\n",
    "\n",
    "def load_experiment(experiment_name, checkpoint_name=\"model_final.pt\", base_dir=\"experiments\"):\n",
    "    outdir = os.path.join(base_dir, experiment_name)\n",
    "    # Load the config\n",
    "    configfile = os.path.join(outdir, 'config.json')\n",
    "    with open(configfile, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    # Load the metrics\n",
    "    jsonfile = os.path.join(outdir, 'metrics.json')\n",
    "    with open(jsonfile, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    train_losses = data['train_losses']\n",
    "    test_losses = data['test_losses']\n",
    "    accuracies = data['accuracies']\n",
    "    # Load the model\n",
    "    model = ViTForClassfication(config)\n",
    "    cpfile = os.path.join(outdir, checkpoint_name)\n",
    "    model.load_state_dict(torch.load(cpfile))\n",
    "    return config, model, train_losses, test_losses, accuracies\n",
    "\n",
    "\n",
    "def visualize_images(dataset):\n",
    "    classes = ('CN', 'MCI', 'AD')\n",
    "\n",
    "    # Pick 4 samples randomly\n",
    "    indices = torch.randperm(len(dataset))[:4]\n",
    "    images = [np.asarray(dataset[i][0]) for i in indices]\n",
    "    labels = [dataset[i][1] for i in indices]\n",
    "    # Visualize the images using matplotlib\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    for i in range(30):\n",
    "        ax = fig.add_subplot(6, 5, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(images[i])\n",
    "        ax.set_title(classes[labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e4d40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1f144c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a5192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+------------+\n",
      "|                Modules                 | Parameters |\n",
      "+----------------------------------------+------------+\n",
      "|             pos_embedding              |   921856   |\n",
      "|               cls_token                |    256     |\n",
      "|      to_patch_embedding.1.weight       |    500     |\n",
      "|       to_patch_embedding.1.bias        |    500     |\n",
      "|      to_patch_embedding.2.weight       |   128000   |\n",
      "|       to_patch_embedding.2.bias        |    256     |\n",
      "|      to_patch_embedding.3.weight       |    256     |\n",
      "|       to_patch_embedding.3.bias        |    256     |\n",
      "|   transformer.layers.0.0.norm.weight   |    256     |\n",
      "|    transformer.layers.0.0.norm.bias    |    256     |\n",
      "|  transformer.layers.0.0.to_qkv.weight  |   196608   |\n",
      "| transformer.layers.0.0.to_out.0.weight |   65536    |\n",
      "|  transformer.layers.0.0.to_out.0.bias  |    256     |\n",
      "|  transformer.layers.0.1.net.0.weight   |    256     |\n",
      "|   transformer.layers.0.1.net.0.bias    |    256     |\n",
      "|  transformer.layers.0.1.net.1.weight   |   262144   |\n",
      "|   transformer.layers.0.1.net.1.bias    |    1024    |\n",
      "|  transformer.layers.0.1.net.4.weight   |   262144   |\n",
      "|   transformer.layers.0.1.net.4.bias    |    256     |\n",
      "|   transformer.layers.1.0.norm.weight   |    256     |\n",
      "|    transformer.layers.1.0.norm.bias    |    256     |\n",
      "|  transformer.layers.1.0.to_qkv.weight  |   196608   |\n",
      "| transformer.layers.1.0.to_out.0.weight |   65536    |\n",
      "|  transformer.layers.1.0.to_out.0.bias  |    256     |\n",
      "|  transformer.layers.1.1.net.0.weight   |    256     |\n",
      "|   transformer.layers.1.1.net.0.bias    |    256     |\n",
      "|  transformer.layers.1.1.net.1.weight   |   262144   |\n",
      "|   transformer.layers.1.1.net.1.bias    |    1024    |\n",
      "|  transformer.layers.1.1.net.4.weight   |   262144   |\n",
      "|   transformer.layers.1.1.net.4.bias    |    256     |\n",
      "|   transformer.layers.2.0.norm.weight   |    256     |\n",
      "|    transformer.layers.2.0.norm.bias    |    256     |\n",
      "|  transformer.layers.2.0.to_qkv.weight  |   196608   |\n",
      "| transformer.layers.2.0.to_out.0.weight |   65536    |\n",
      "|  transformer.layers.2.0.to_out.0.bias  |    256     |\n",
      "|  transformer.layers.2.1.net.0.weight   |    256     |\n",
      "|   transformer.layers.2.1.net.0.bias    |    256     |\n",
      "|  transformer.layers.2.1.net.1.weight   |   262144   |\n",
      "|   transformer.layers.2.1.net.1.bias    |    1024    |\n",
      "|  transformer.layers.2.1.net.4.weight   |   262144   |\n",
      "|   transformer.layers.2.1.net.4.bias    |    256     |\n",
      "|   transformer.layers.3.0.norm.weight   |    256     |\n",
      "|    transformer.layers.3.0.norm.bias    |    256     |\n",
      "|  transformer.layers.3.0.to_qkv.weight  |   196608   |\n",
      "| transformer.layers.3.0.to_out.0.weight |   65536    |\n",
      "|  transformer.layers.3.0.to_out.0.bias  |    256     |\n",
      "|  transformer.layers.3.1.net.0.weight   |    256     |\n",
      "|   transformer.layers.3.1.net.0.bias    |    256     |\n",
      "|  transformer.layers.3.1.net.1.weight   |   262144   |\n",
      "|   transformer.layers.3.1.net.1.bias    |    1024    |\n",
      "|  transformer.layers.3.1.net.4.weight   |   262144   |\n",
      "|   transformer.layers.3.1.net.4.bias    |    256     |\n",
      "|           mlp_head.0.weight            |    256     |\n",
      "|            mlp_head.0.bias             |    256     |\n",
      "|           mlp_head.1.weight            |    768     |\n",
      "|            mlp_head.1.bias             |     3      |\n",
      "+----------------------------------------+------------+\n",
      "Total Trainable Params: 4209131\n"
     ]
    }
   ],
   "source": [
    "# Get parameters for each layer of the model in a tabular format\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd667c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images:  2182\n",
      "Total number of train, validation and test images are 1745, 218 and 219 respectively.\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "DATA_PATH = os.path.join('/home/arindam/Alzheimer/Data/adni1-complete-3yr-1-5t', 'ADNI')\n",
    "config = {\n",
    "    'img_size': 128,\n",
    "    'depth' : 128,\n",
    "    'batch_size' : 16\n",
    "}\n",
    "\n",
    "# Modify the above config in the Dataloader to change the batch size, image size and depth of the model\n",
    "train_loader, valid_loader, test_loader = LoadDatasets(return_type='loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ca4155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images and labels of a signle batch is torch.Size([2, 1, 150, 150, 80]) and torch.Size([2]) respectively.\n"
     ]
    }
   ],
   "source": [
    "# Check a sample batch size\n",
    "for data in train_loader:\n",
    "    images, labels = data\n",
    "    print(f\"Shape of images and labels of a signle batch is {images.shape} and {labels.shape} respectively.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89b29b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=1e-3,\n",
    "            weight_decay=5e-6\n",
    "            )\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [20, 30], gamma=0.1)\n",
    "loss_fn = nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef5aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    total_time_iter = 0\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    train_loss, n_samples = 0, 0\n",
    "    correct = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device) #torch.squeeze(labels).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output, labels, reduction='mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        time_iter = time.time() - start\n",
    "        total_time_iter += time_iter\n",
    "        train_loss += loss.item() * len(output)\n",
    "        n_samples += len(output)\n",
    "        #print(output)\n",
    "        predicted = torch.argmax(output, dim=1)\n",
    "        #print(predicted, labels)\n",
    "        correct += (predicted == labels).sum()\n",
    "        if batch_idx % 50 == 0 or batch_idx == len(train_loader) - 1:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} (avg: {:.6f}) \\tsec/iter: {:.4f} s\\t Accuracy (avg) {:.3f}'.format(\n",
    "                epoch+1, n_samples, len(train_loader.dataset),\n",
    "                100. * (batch_idx + 1) / len(train_loader), train_loss, train_loss / n_samples, time_iter / (batch_idx + 1), 100*(correct/n_samples) ))\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(f\"Took {time.time() - start} sec for this epoch\")\n",
    "    \n",
    "    return train_loss / n_samples, 100*(correct/n_samples)\n",
    "\n",
    "\n",
    "def validation(valid_loader):\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    valid_loss, correct, n_samples = 0, 0, 0\n",
    "    for batch_idx, data in enumerate(valid_loader):\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device) #torch.squeeze(labels).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output, labels, reduction='sum')\n",
    "        valid_loss += loss.item()\n",
    "        n_samples += len(output)\n",
    "        pred = torch.argmax(output.data, dim=1)\n",
    "\n",
    "        correct += (pred == labels).sum()\n",
    "\n",
    "    time_iter = time.time() - start\n",
    "\n",
    "    #valid_loss /= n_samples\n",
    "\n",
    "    acc = 100. * correct / n_samples\n",
    "    print('Validation set (epoch {}): Loss: {:.4f}, Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%) Took {} sec\\n'.format(epoch+1, \n",
    "                                                                                          valid_loss,\n",
    "                                                                                          valid_loss / n_samples,\n",
    "                                                                                          correct, \n",
    "                                                                                          n_samples, acc, \n",
    "                                                                                          time_iter))\n",
    "    return valid_loss/n_samples, acc\n",
    "\n",
    "\n",
    "def test(test_loader):\n",
    "    print('Test model ...')\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    test_loss, correct, n_samples = 0, 0, 0\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device) #torch.squeeze(labels).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output, labels, reduction='sum')\n",
    "        test_loss += loss.item()\n",
    "        n_samples += len(output)\n",
    "        pred = torch.argmax(output.data, dim=1)\n",
    "\n",
    "        correct += (pred == labels).sum()\n",
    "\n",
    "    time_iter = time.time() - start\n",
    "\n",
    "    avg_test_loss = test_loss / n_samples\n",
    "\n",
    "    acc = 100. * (correct / n_samples)\n",
    "    print('Test set: Loss: {:.4f}, Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%). Took {} sec time'.format(test_loss, \n",
    "                                                                                avg_test_loss,\n",
    "                                                                                correct, \n",
    "                                                                                n_samples, acc, time_iter))\n",
    "    return avg_test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a701e95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [2/1745 (0%)]\tLoss: 2.529685 (avg: 1.264842) \tsec/iter: 0.1639 s\t Accuracy (avg) 0.000\n",
      "Train Epoch: 1 [102/1745 (6%)]\tLoss: 150.722987 (avg: 1.477676) \tsec/iter: 0.1988 s\t Accuracy (avg) 30.392\n",
      "Train Epoch: 1 [202/1745 (12%)]\tLoss: 267.565227 (avg: 1.324580) \tsec/iter: 0.1990 s\t Accuracy (avg) 29.703\n",
      "Train Epoch: 1 [302/1745 (17%)]\tLoss: 378.952671 (avg: 1.254810) \tsec/iter: 0.1982 s\t Accuracy (avg) 32.781\n",
      "Train Epoch: 1 [402/1745 (23%)]\tLoss: 491.607656 (avg: 1.222905) \tsec/iter: 0.1974 s\t Accuracy (avg) 34.328\n",
      "Train Epoch: 1 [502/1745 (29%)]\tLoss: 597.737885 (avg: 1.190713) \tsec/iter: 0.1975 s\t Accuracy (avg) 36.056\n",
      "Train Epoch: 1 [602/1745 (34%)]\tLoss: 711.997240 (avg: 1.182720) \tsec/iter: 0.1979 s\t Accuracy (avg) 37.209\n",
      "Train Epoch: 1 [702/1745 (40%)]\tLoss: 827.594879 (avg: 1.178910) \tsec/iter: 0.1982 s\t Accuracy (avg) 38.177\n",
      "Train Epoch: 1 [802/1745 (46%)]\tLoss: 940.343432 (avg: 1.172498) \tsec/iter: 0.1985 s\t Accuracy (avg) 37.656\n",
      "Train Epoch: 1 [902/1745 (52%)]\tLoss: 1050.590245 (avg: 1.164734) \tsec/iter: 0.1989 s\t Accuracy (avg) 37.583\n",
      "Train Epoch: 1 [1002/1745 (57%)]\tLoss: 1167.695089 (avg: 1.165364) \tsec/iter: 0.1988 s\t Accuracy (avg) 37.026\n",
      "Train Epoch: 1 [1102/1745 (63%)]\tLoss: 1277.411305 (avg: 1.159175) \tsec/iter: 0.1985 s\t Accuracy (avg) 37.296\n",
      "Train Epoch: 1 [1202/1745 (69%)]\tLoss: 1384.814240 (avg: 1.152092) \tsec/iter: 0.1984 s\t Accuracy (avg) 37.687\n",
      "Train Epoch: 1 [1302/1745 (75%)]\tLoss: 1497.302239 (avg: 1.150002) \tsec/iter: 0.1984 s\t Accuracy (avg) 37.942\n",
      "Train Epoch: 1 [1402/1745 (80%)]\tLoss: 1606.574756 (avg: 1.145916) \tsec/iter: 0.1982 s\t Accuracy (avg) 37.946\n",
      "Train Epoch: 1 [1502/1745 (86%)]\tLoss: 1716.478805 (avg: 1.142795) \tsec/iter: 0.1980 s\t Accuracy (avg) 38.349\n",
      "Train Epoch: 1 [1602/1745 (92%)]\tLoss: 1825.938133 (avg: 1.139787) \tsec/iter: 0.1979 s\t Accuracy (avg) 38.702\n",
      "Train Epoch: 1 [1702/1745 (97%)]\tLoss: 1933.616555 (avg: 1.136085) \tsec/iter: 0.1978 s\t Accuracy (avg) 38.954\n",
      "Train Epoch: 1 [1745/1745 (100%)]\tLoss: 1977.491113 (avg: 1.133233) \tsec/iter: 0.1977 s\t Accuracy (avg) 39.083\n",
      "Took 172.5844566822052 sec for this epoch\n",
      "Validation set (epoch 1): Loss: 229.2335, Average loss: 1.0515, Accuracy: 75/218 (34.40%) Took 17.730172634124756 sec\n",
      "\n",
      "Train Epoch: 2 [2/1745 (0%)]\tLoss: 1.750773 (avg: 0.875386) \tsec/iter: 0.1484 s\t Accuracy (avg) 50.000\n",
      "Train Epoch: 2 [102/1745 (6%)]\tLoss: 107.805024 (avg: 1.056912) \tsec/iter: 0.1973 s\t Accuracy (avg) 45.098\n",
      "Train Epoch: 2 [202/1745 (12%)]\tLoss: 222.502593 (avg: 1.101498) \tsec/iter: 0.1981 s\t Accuracy (avg) 38.119\n",
      "Train Epoch: 2 [302/1745 (17%)]\tLoss: 333.622604 (avg: 1.104711) \tsec/iter: 0.1985 s\t Accuracy (avg) 36.755\n",
      "Train Epoch: 2 [402/1745 (23%)]\tLoss: 441.311337 (avg: 1.097789) \tsec/iter: 0.1984 s\t Accuracy (avg) 38.060\n",
      "Train Epoch: 2 [502/1745 (29%)]\tLoss: 545.452010 (avg: 1.086558) \tsec/iter: 0.1981 s\t Accuracy (avg) 40.438\n",
      "Train Epoch: 2 [602/1745 (34%)]\tLoss: 654.944843 (avg: 1.087948) \tsec/iter: 0.1977 s\t Accuracy (avg) 38.372\n",
      "Train Epoch: 2 [702/1745 (40%)]\tLoss: 758.187317 (avg: 1.080039) \tsec/iter: 0.1973 s\t Accuracy (avg) 38.604\n",
      "Train Epoch: 2 [802/1745 (46%)]\tLoss: 863.639314 (avg: 1.076857) \tsec/iter: 0.1969 s\t Accuracy (avg) 39.152\n",
      "Train Epoch: 2 [902/1745 (52%)]\tLoss: 973.194056 (avg: 1.078929) \tsec/iter: 0.1965 s\t Accuracy (avg) 39.135\n",
      "Train Epoch: 2 [1002/1745 (57%)]\tLoss: 1079.212238 (avg: 1.077058) \tsec/iter: 0.1967 s\t Accuracy (avg) 39.521\n",
      "Train Epoch: 2 [1102/1745 (63%)]\tLoss: 1187.418074 (avg: 1.077512) \tsec/iter: 0.1966 s\t Accuracy (avg) 39.655\n",
      "Train Epoch: 2 [1202/1745 (69%)]\tLoss: 1295.824995 (avg: 1.078057) \tsec/iter: 0.1966 s\t Accuracy (avg) 40.017\n",
      "Train Epoch: 2 [1302/1745 (75%)]\tLoss: 1396.853093 (avg: 1.072852) \tsec/iter: 0.1964 s\t Accuracy (avg) 41.014\n",
      "Train Epoch: 2 [1402/1745 (80%)]\tLoss: 1504.007828 (avg: 1.072759) \tsec/iter: 0.1964 s\t Accuracy (avg) 41.655\n",
      "Train Epoch: 2 [1502/1745 (86%)]\tLoss: 1616.573651 (avg: 1.076281) \tsec/iter: 0.1964 s\t Accuracy (avg) 41.744\n",
      "Train Epoch: 2 [1602/1745 (92%)]\tLoss: 1729.537922 (avg: 1.079612) \tsec/iter: 0.1963 s\t Accuracy (avg) 41.635\n",
      "Train Epoch: 2 [1702/1745 (97%)]\tLoss: 1838.891403 (avg: 1.080430) \tsec/iter: 0.1964 s\t Accuracy (avg) 41.011\n",
      "Train Epoch: 2 [1745/1745 (100%)]\tLoss: 1889.243526 (avg: 1.082661) \tsec/iter: 0.1963 s\t Accuracy (avg) 40.745\n",
      "Took 171.35476088523865 sec for this epoch\n",
      "Validation set (epoch 2): Loss: 234.3152, Average loss: 1.0748, Accuracy: 102/218 (46.79%) Took 17.407451391220093 sec\n",
      "\n",
      "Train Epoch: 3 [2/1745 (0%)]\tLoss: 2.004838 (avg: 1.002419) \tsec/iter: 0.1486 s\t Accuracy (avg) 50.000\n",
      "Train Epoch: 3 [102/1745 (6%)]\tLoss: 109.718671 (avg: 1.075673) \tsec/iter: 0.1954 s\t Accuracy (avg) 37.255\n",
      "Train Epoch: 3 [202/1745 (12%)]\tLoss: 213.860282 (avg: 1.058714) \tsec/iter: 0.1966 s\t Accuracy (avg) 40.594\n",
      "Train Epoch: 3 [302/1745 (17%)]\tLoss: 326.982747 (avg: 1.082724) \tsec/iter: 0.1959 s\t Accuracy (avg) 41.060\n",
      "Train Epoch: 3 [402/1745 (23%)]\tLoss: 437.542422 (avg: 1.088414) \tsec/iter: 0.1959 s\t Accuracy (avg) 39.303\n",
      "Train Epoch: 3 [502/1745 (29%)]\tLoss: 548.601276 (avg: 1.092831) \tsec/iter: 0.1967 s\t Accuracy (avg) 38.446\n",
      "Train Epoch: 3 [602/1745 (34%)]\tLoss: 652.718451 (avg: 1.084250) \tsec/iter: 0.1964 s\t Accuracy (avg) 39.701\n",
      "Train Epoch: 3 [702/1745 (40%)]\tLoss: 760.446312 (avg: 1.083257) \tsec/iter: 0.1964 s\t Accuracy (avg) 39.886\n",
      "Train Epoch: 3 [802/1745 (46%)]\tLoss: 868.762791 (avg: 1.083245) \tsec/iter: 0.1964 s\t Accuracy (avg) 39.526\n",
      "Train Epoch: 3 [902/1745 (52%)]\tLoss: 979.875844 (avg: 1.086337) \tsec/iter: 0.1963 s\t Accuracy (avg) 39.024\n",
      "Train Epoch: 3 [1002/1745 (57%)]\tLoss: 1086.545248 (avg: 1.084376) \tsec/iter: 0.1964 s\t Accuracy (avg) 40.220\n",
      "Train Epoch: 3 [1102/1745 (63%)]\tLoss: 1198.159710 (avg: 1.087259) \tsec/iter: 0.1963 s\t Accuracy (avg) 39.746\n",
      "Train Epoch: 3 [1202/1745 (69%)]\tLoss: 1308.565020 (avg: 1.088656) \tsec/iter: 0.1962 s\t Accuracy (avg) 39.850\n",
      "Train Epoch: 3 [1302/1745 (75%)]\tLoss: 1418.370685 (avg: 1.089378) \tsec/iter: 0.1962 s\t Accuracy (avg) 40.015\n",
      "Train Epoch: 3 [1402/1745 (80%)]\tLoss: 1527.679502 (avg: 1.089643) \tsec/iter: 0.1963 s\t Accuracy (avg) 39.586\n",
      "Train Epoch: 3 [1502/1745 (86%)]\tLoss: 1637.954589 (avg: 1.090516) \tsec/iter: 0.1961 s\t Accuracy (avg) 39.680\n",
      "Train Epoch: 3 [1602/1745 (92%)]\tLoss: 1748.036461 (avg: 1.091159) \tsec/iter: 0.1961 s\t Accuracy (avg) 39.388\n",
      "Train Epoch: 3 [1702/1745 (97%)]\tLoss: 1857.071795 (avg: 1.091112) \tsec/iter: 0.1960 s\t Accuracy (avg) 39.424\n",
      "Train Epoch: 3 [1745/1745 (100%)]\tLoss: 1897.847484 (avg: 1.087592) \tsec/iter: 0.1959 s\t Accuracy (avg) 39.885\n",
      "Took 171.00751733779907 sec for this epoch\n",
      "Validation set (epoch 3): Loss: 238.8868, Average loss: 1.0958, Accuracy: 102/218 (46.79%) Took 17.389522790908813 sec\n",
      "\n",
      "Train Epoch: 4 [2/1745 (0%)]\tLoss: 1.660478 (avg: 0.830239) \tsec/iter: 0.1476 s\t Accuracy (avg) 50.000\n",
      "Train Epoch: 4 [102/1745 (6%)]\tLoss: 107.520016 (avg: 1.054118) \tsec/iter: 0.1953 s\t Accuracy (avg) 43.137\n",
      "Train Epoch: 4 [202/1745 (12%)]\tLoss: 216.830084 (avg: 1.073416) \tsec/iter: 0.1959 s\t Accuracy (avg) 42.079\n",
      "Train Epoch: 4 [302/1745 (17%)]\tLoss: 325.536491 (avg: 1.077935) \tsec/iter: 0.1955 s\t Accuracy (avg) 42.715\n",
      "Train Epoch: 4 [402/1745 (23%)]\tLoss: 432.018328 (avg: 1.074672) \tsec/iter: 0.1961 s\t Accuracy (avg) 43.781\n",
      "Train Epoch: 4 [502/1745 (29%)]\tLoss: 538.117581 (avg: 1.071947) \tsec/iter: 0.1958 s\t Accuracy (avg) 44.024\n",
      "Train Epoch: 4 [602/1745 (34%)]\tLoss: 630.106137 (avg: 1.046688) \tsec/iter: 0.1957 s\t Accuracy (avg) 46.678\n",
      "Train Epoch: 4 [702/1745 (40%)]\tLoss: 756.088611 (avg: 1.077049) \tsec/iter: 0.1955 s\t Accuracy (avg) 45.014\n",
      "Train Epoch: 4 [802/1745 (46%)]\tLoss: 865.952957 (avg: 1.079742) \tsec/iter: 0.1955 s\t Accuracy (avg) 44.514\n",
      "Train Epoch: 4 [902/1745 (52%)]\tLoss: 973.805083 (avg: 1.079607) \tsec/iter: 0.1954 s\t Accuracy (avg) 44.568\n",
      "Train Epoch: 4 [1002/1745 (57%)]\tLoss: 1089.559747 (avg: 1.087385) \tsec/iter: 0.1955 s\t Accuracy (avg) 43.313\n",
      "Train Epoch: 4 [1102/1745 (63%)]\tLoss: 1199.446337 (avg: 1.088427) \tsec/iter: 0.1956 s\t Accuracy (avg) 42.740\n",
      "Train Epoch: 4 [1202/1745 (69%)]\tLoss: 1303.109913 (avg: 1.084118) \tsec/iter: 0.1956 s\t Accuracy (avg) 43.261\n",
      "Train Epoch: 4 [1302/1745 (75%)]\tLoss: 1412.588359 (avg: 1.084937) \tsec/iter: 0.1955 s\t Accuracy (avg) 42.934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [1402/1745 (80%)]\tLoss: 1519.212795 (avg: 1.083604) \tsec/iter: 0.1957 s\t Accuracy (avg) 42.511\n",
      "Train Epoch: 4 [1502/1745 (86%)]\tLoss: 1625.231159 (avg: 1.082045) \tsec/iter: 0.1958 s\t Accuracy (avg) 42.477\n",
      "Train Epoch: 4 [1602/1745 (92%)]\tLoss: 1728.531962 (avg: 1.078984) \tsec/iter: 0.1958 s\t Accuracy (avg) 42.634\n",
      "Train Epoch: 4 [1702/1745 (97%)]\tLoss: 1832.248200 (avg: 1.076527) \tsec/iter: 0.1959 s\t Accuracy (avg) 43.184\n",
      "Train Epoch: 4 [1745/1745 (100%)]\tLoss: 1878.712613 (avg: 1.076626) \tsec/iter: 0.1958 s\t Accuracy (avg) 43.324\n",
      "Took 170.93700218200684 sec for this epoch\n",
      "Validation set (epoch 4): Loss: 229.2018, Average loss: 1.0514, Accuracy: 102/218 (46.79%) Took 17.414711236953735 sec\n",
      "\n",
      "Train Epoch: 5 [2/1745 (0%)]\tLoss: 2.724051 (avg: 1.362025) \tsec/iter: 0.1499 s\t Accuracy (avg) 0.000\n",
      "Train Epoch: 5 [102/1745 (6%)]\tLoss: 111.565578 (avg: 1.093780) \tsec/iter: 0.1952 s\t Accuracy (avg) 38.235\n",
      "Train Epoch: 5 [202/1745 (12%)]\tLoss: 219.679271 (avg: 1.087521) \tsec/iter: 0.1956 s\t Accuracy (avg) 42.574\n",
      "Train Epoch: 5 [302/1745 (17%)]\tLoss: 323.890076 (avg: 1.072484) \tsec/iter: 0.1951 s\t Accuracy (avg) 45.033\n",
      "Train Epoch: 5 [402/1745 (23%)]\tLoss: 434.785851 (avg: 1.081557) \tsec/iter: 0.1955 s\t Accuracy (avg) 43.035\n",
      "Train Epoch: 5 [502/1745 (29%)]\tLoss: 543.687532 (avg: 1.083043) \tsec/iter: 0.1956 s\t Accuracy (avg) 42.032\n",
      "Train Epoch: 5 [602/1745 (34%)]\tLoss: 652.058442 (avg: 1.083154) \tsec/iter: 0.1958 s\t Accuracy (avg) 42.525\n",
      "Train Epoch: 5 [702/1745 (40%)]\tLoss: 757.167304 (avg: 1.078586) \tsec/iter: 0.1958 s\t Accuracy (avg) 42.593\n",
      "Train Epoch: 5 [802/1745 (46%)]\tLoss: 862.007450 (avg: 1.074822) \tsec/iter: 0.1960 s\t Accuracy (avg) 43.641\n",
      "Train Epoch: 5 [902/1745 (52%)]\tLoss: 972.178711 (avg: 1.077803) \tsec/iter: 0.1958 s\t Accuracy (avg) 43.681\n",
      "Train Epoch: 5 [1002/1745 (57%)]\tLoss: 1082.538979 (avg: 1.080378) \tsec/iter: 0.1958 s\t Accuracy (avg) 43.014\n",
      "Train Epoch: 5 [1102/1745 (63%)]\tLoss: 1185.988600 (avg: 1.076215) \tsec/iter: 0.1957 s\t Accuracy (avg) 43.194\n",
      "Train Epoch: 5 [1202/1745 (69%)]\tLoss: 1292.864664 (avg: 1.075595) \tsec/iter: 0.1957 s\t Accuracy (avg) 43.261\n",
      "Train Epoch: 5 [1302/1745 (75%)]\tLoss: 1400.043895 (avg: 1.075303) \tsec/iter: 0.1958 s\t Accuracy (avg) 42.704\n",
      "Train Epoch: 5 [1402/1745 (80%)]\tLoss: 1507.952158 (avg: 1.075572) \tsec/iter: 0.1957 s\t Accuracy (avg) 42.582\n",
      "Train Epoch: 5 [1502/1745 (86%)]\tLoss: 1616.315495 (avg: 1.076109) \tsec/iter: 0.1958 s\t Accuracy (avg) 42.410\n",
      "Train Epoch: 5 [1602/1745 (92%)]\tLoss: 1718.871544 (avg: 1.072954) \tsec/iter: 0.1958 s\t Accuracy (avg) 42.634\n",
      "Train Epoch: 5 [1702/1745 (97%)]\tLoss: 1823.813005 (avg: 1.071571) \tsec/iter: 0.1958 s\t Accuracy (avg) 42.773\n",
      "Train Epoch: 5 [1745/1745 (100%)]\tLoss: 1870.641628 (avg: 1.072001) \tsec/iter: 0.1957 s\t Accuracy (avg) 42.808\n",
      "Took 170.87346267700195 sec for this epoch\n",
      "Validation set (epoch 5): Loss: 226.9662, Average loss: 1.0411, Accuracy: 102/218 (46.79%) Took 17.508517503738403 sec\n",
      "\n",
      "Train Epoch: 6 [2/1745 (0%)]\tLoss: 1.930502 (avg: 0.965251) \tsec/iter: 0.1496 s\t Accuracy (avg) 50.000\n",
      "Train Epoch: 6 [102/1745 (6%)]\tLoss: 105.688736 (avg: 1.036164) \tsec/iter: 0.1956 s\t Accuracy (avg) 40.196\n",
      "Train Epoch: 6 [202/1745 (12%)]\tLoss: 217.104481 (avg: 1.074775) \tsec/iter: 0.1952 s\t Accuracy (avg) 40.099\n",
      "Train Epoch: 6 [302/1745 (17%)]\tLoss: 323.100292 (avg: 1.069869) \tsec/iter: 0.1948 s\t Accuracy (avg) 40.728\n",
      "Train Epoch: 6 [402/1745 (23%)]\tLoss: 426.530271 (avg: 1.061021) \tsec/iter: 0.1955 s\t Accuracy (avg) 42.786\n",
      "Train Epoch: 6 [502/1745 (29%)]\tLoss: 540.933658 (avg: 1.077557) \tsec/iter: 0.1954 s\t Accuracy (avg) 40.438\n",
      "Train Epoch: 6 [602/1745 (34%)]\tLoss: 649.531637 (avg: 1.078956) \tsec/iter: 0.1958 s\t Accuracy (avg) 39.535\n",
      "Train Epoch: 6 [702/1745 (40%)]\tLoss: 752.614497 (avg: 1.072100) \tsec/iter: 0.1962 s\t Accuracy (avg) 40.883\n",
      "Train Epoch: 6 [802/1745 (46%)]\tLoss: 864.322146 (avg: 1.077708) \tsec/iter: 0.1961 s\t Accuracy (avg) 40.274\n",
      "Train Epoch: 6 [902/1745 (52%)]\tLoss: 972.511558 (avg: 1.078172) \tsec/iter: 0.1960 s\t Accuracy (avg) 40.909\n",
      "Train Epoch: 6 [1002/1745 (57%)]\tLoss: 1080.927668 (avg: 1.078770) \tsec/iter: 0.1960 s\t Accuracy (avg) 40.619\n",
      "Train Epoch: 6 [1102/1745 (63%)]\tLoss: 1183.958069 (avg: 1.074372) \tsec/iter: 0.1960 s\t Accuracy (avg) 40.926\n",
      "Train Epoch: 6 [1202/1745 (69%)]\tLoss: 1290.501166 (avg: 1.073628) \tsec/iter: 0.1958 s\t Accuracy (avg) 41.098\n",
      "Train Epoch: 6 [1302/1745 (75%)]\tLoss: 1399.389313 (avg: 1.074800) \tsec/iter: 0.1957 s\t Accuracy (avg) 40.476\n",
      "Train Epoch: 6 [1402/1745 (80%)]\tLoss: 1501.699814 (avg: 1.071113) \tsec/iter: 0.1957 s\t Accuracy (avg) 40.870\n",
      "Train Epoch: 6 [1502/1745 (86%)]\tLoss: 1609.633561 (avg: 1.071660) \tsec/iter: 0.1957 s\t Accuracy (avg) 41.012\n",
      "Train Epoch: 6 [1602/1745 (92%)]\tLoss: 1716.386401 (avg: 1.071402) \tsec/iter: 0.1958 s\t Accuracy (avg) 41.511\n",
      "Train Epoch: 6 [1702/1745 (97%)]\tLoss: 1823.198058 (avg: 1.071209) \tsec/iter: 0.1958 s\t Accuracy (avg) 41.716\n",
      "Train Epoch: 6 [1745/1745 (100%)]\tLoss: 1866.934510 (avg: 1.069877) \tsec/iter: 0.1957 s\t Accuracy (avg) 41.891\n",
      "Took 170.85209608078003 sec for this epoch\n",
      "Validation set (epoch 6): Loss: 226.4400, Average loss: 1.0387, Accuracy: 102/218 (46.79%) Took 17.40999460220337 sec\n",
      "\n",
      "Train Epoch: 7 [2/1745 (0%)]\tLoss: 2.551087 (avg: 1.275544) \tsec/iter: 0.1496 s\t Accuracy (avg) 50.000\n",
      "Train Epoch: 7 [102/1745 (6%)]\tLoss: 109.877833 (avg: 1.077234) \tsec/iter: 0.1931 s\t Accuracy (avg) 44.118\n",
      "Train Epoch: 7 [202/1745 (12%)]\tLoss: 219.232841 (avg: 1.085311) \tsec/iter: 0.1939 s\t Accuracy (avg) 40.594\n",
      "Train Epoch: 7 [302/1745 (17%)]\tLoss: 325.406245 (avg: 1.077504) \tsec/iter: 0.1951 s\t Accuracy (avg) 41.391\n",
      "Train Epoch: 7 [402/1745 (23%)]\tLoss: 432.282911 (avg: 1.075331) \tsec/iter: 0.1953 s\t Accuracy (avg) 40.547\n",
      "Train Epoch: 7 [502/1745 (29%)]\tLoss: 535.319597 (avg: 1.066374) \tsec/iter: 0.1955 s\t Accuracy (avg) 43.028\n",
      "Train Epoch: 7 [602/1745 (34%)]\tLoss: 643.573424 (avg: 1.069059) \tsec/iter: 0.1953 s\t Accuracy (avg) 42.857\n",
      "Train Epoch: 7 [702/1745 (40%)]\tLoss: 750.899542 (avg: 1.069657) \tsec/iter: 0.1954 s\t Accuracy (avg) 42.593\n",
      "Train Epoch: 7 [802/1745 (46%)]\tLoss: 855.136557 (avg: 1.066255) \tsec/iter: 0.1957 s\t Accuracy (avg) 43.267\n",
      "Train Epoch: 7 [902/1745 (52%)]\tLoss: 963.651593 (avg: 1.068350) \tsec/iter: 0.1958 s\t Accuracy (avg) 43.902\n",
      "Train Epoch: 7 [1002/1745 (57%)]\tLoss: 1069.194621 (avg: 1.067060) \tsec/iter: 0.1955 s\t Accuracy (avg) 43.413\n",
      "Train Epoch: 7 [1102/1745 (63%)]\tLoss: 1177.951721 (avg: 1.068922) \tsec/iter: 0.1955 s\t Accuracy (avg) 42.831\n",
      "Train Epoch: 7 [1202/1745 (69%)]\tLoss: 1282.084218 (avg: 1.066626) \tsec/iter: 0.1955 s\t Accuracy (avg) 42.845\n",
      "Train Epoch: 7 [1302/1745 (75%)]\tLoss: 1389.272626 (avg: 1.067030) \tsec/iter: 0.1956 s\t Accuracy (avg) 42.934\n",
      "Train Epoch: 7 [1402/1745 (80%)]\tLoss: 1496.846647 (avg: 1.067651) \tsec/iter: 0.1957 s\t Accuracy (avg) 43.224\n",
      "Train Epoch: 7 [1502/1745 (86%)]\tLoss: 1605.453346 (avg: 1.068877) \tsec/iter: 0.1957 s\t Accuracy (avg) 43.076\n",
      "Train Epoch: 7 [1602/1745 (92%)]\tLoss: 1711.191877 (avg: 1.068160) \tsec/iter: 0.1957 s\t Accuracy (avg) 43.196\n",
      "Train Epoch: 7 [1702/1745 (97%)]\tLoss: 1817.025638 (avg: 1.067583) \tsec/iter: 0.1957 s\t Accuracy (avg) 43.302\n",
      "Train Epoch: 7 [1745/1745 (100%)]\tLoss: 1862.333995 (avg: 1.067240) \tsec/iter: 0.1956 s\t Accuracy (avg) 43.381\n",
      "Took 170.75432801246643 sec for this epoch\n",
      "Validation set (epoch 7): Loss: 226.7366, Average loss: 1.0401, Accuracy: 102/218 (46.79%) Took 17.453594207763672 sec\n",
      "\n",
      "Train Epoch: 8 [2/1745 (0%)]\tLoss: 2.305804 (avg: 1.152902) \tsec/iter: 0.1485 s\t Accuracy (avg) 0.000\n",
      "Train Epoch: 8 [102/1745 (6%)]\tLoss: 113.510813 (avg: 1.112851) \tsec/iter: 0.1946 s\t Accuracy (avg) 28.431\n",
      "Train Epoch: 8 [202/1745 (12%)]\tLoss: 218.688791 (avg: 1.082618) \tsec/iter: 0.1955 s\t Accuracy (avg) 38.119\n",
      "Train Epoch: 8 [302/1745 (17%)]\tLoss: 326.373246 (avg: 1.080706) \tsec/iter: 0.1958 s\t Accuracy (avg) 35.430\n",
      "Train Epoch: 8 [402/1745 (23%)]\tLoss: 429.585807 (avg: 1.068621) \tsec/iter: 0.1957 s\t Accuracy (avg) 39.303\n",
      "Train Epoch: 8 [502/1745 (29%)]\tLoss: 528.301473 (avg: 1.052393) \tsec/iter: 0.1962 s\t Accuracy (avg) 42.032\n",
      "Train Epoch: 8 [602/1745 (34%)]\tLoss: 635.785232 (avg: 1.056122) \tsec/iter: 0.1961 s\t Accuracy (avg) 42.027\n",
      "Train Epoch: 8 [702/1745 (40%)]\tLoss: 741.455516 (avg: 1.056204) \tsec/iter: 0.1960 s\t Accuracy (avg) 42.023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [802/1745 (46%)]\tLoss: 850.223743 (avg: 1.060129) \tsec/iter: 0.1958 s\t Accuracy (avg) 41.272\n",
      "Train Epoch: 8 [902/1745 (52%)]\tLoss: 960.706496 (avg: 1.065085) \tsec/iter: 0.1959 s\t Accuracy (avg) 41.020\n",
      "Train Epoch: 8 [1002/1745 (57%)]\tLoss: 1069.757295 (avg: 1.067622) \tsec/iter: 0.1958 s\t Accuracy (avg) 41.417\n",
      "Train Epoch: 8 [1102/1745 (63%)]\tLoss: 1177.736862 (avg: 1.068727) \tsec/iter: 0.1958 s\t Accuracy (avg) 41.016\n",
      "Train Epoch: 8 [1202/1745 (69%)]\tLoss: 1277.861837 (avg: 1.063113) \tsec/iter: 0.1959 s\t Accuracy (avg) 41.764\n",
      "Train Epoch: 8 [1302/1745 (75%)]\tLoss: 1388.468785 (avg: 1.066412) \tsec/iter: 0.1959 s\t Accuracy (avg) 41.705\n",
      "Train Epoch: 8 [1402/1745 (80%)]\tLoss: 1498.478083 (avg: 1.068815) \tsec/iter: 0.1960 s\t Accuracy (avg) 40.585\n",
      "Train Epoch: 8 [1502/1745 (86%)]\tLoss: 1606.696418 (avg: 1.069705) \tsec/iter: 0.1960 s\t Accuracy (avg) 40.812\n",
      "Train Epoch: 8 [1602/1745 (92%)]\tLoss: 1713.784979 (avg: 1.069778) \tsec/iter: 0.1959 s\t Accuracy (avg) 40.637\n",
      "Train Epoch: 8 [1702/1745 (97%)]\tLoss: 1817.074287 (avg: 1.067611) \tsec/iter: 0.1959 s\t Accuracy (avg) 40.952\n",
      "Train Epoch: 8 [1745/1745 (100%)]\tLoss: 1860.488624 (avg: 1.066183) \tsec/iter: 0.1957 s\t Accuracy (avg) 41.318\n",
      "Took 170.87572288513184 sec for this epoch\n",
      "Validation set (epoch 8): Loss: 234.8543, Average loss: 1.0773, Accuracy: 102/218 (46.79%) Took 17.45076823234558 sec\n",
      "\n",
      "Train Epoch: 9 [2/1745 (0%)]\tLoss: 3.161248 (avg: 1.580624) \tsec/iter: 0.1468 s\t Accuracy (avg) 0.000\n",
      "Train Epoch: 9 [102/1745 (6%)]\tLoss: 109.000121 (avg: 1.068629) \tsec/iter: 0.1937 s\t Accuracy (avg) 47.059\n",
      "Train Epoch: 9 [202/1745 (12%)]\tLoss: 219.789223 (avg: 1.088065) \tsec/iter: 0.1939 s\t Accuracy (avg) 42.574\n",
      "Train Epoch: 9 [302/1745 (17%)]\tLoss: 325.103912 (avg: 1.076503) \tsec/iter: 0.1945 s\t Accuracy (avg) 43.709\n",
      "Train Epoch: 9 [402/1745 (23%)]\tLoss: 429.984118 (avg: 1.069612) \tsec/iter: 0.1948 s\t Accuracy (avg) 43.284\n",
      "Train Epoch: 9 [502/1745 (29%)]\tLoss: 540.271634 (avg: 1.076238) \tsec/iter: 0.1950 s\t Accuracy (avg) 43.227\n",
      "Train Epoch: 9 [602/1745 (34%)]\tLoss: 650.100568 (avg: 1.079901) \tsec/iter: 0.1949 s\t Accuracy (avg) 43.189\n",
      "Train Epoch: 9 [702/1745 (40%)]\tLoss: 760.122988 (avg: 1.082796) \tsec/iter: 0.1950 s\t Accuracy (avg) 43.447\n",
      "Train Epoch: 9 [802/1745 (46%)]\tLoss: 859.842458 (avg: 1.072123) \tsec/iter: 0.1952 s\t Accuracy (avg) 44.888\n",
      "Train Epoch: 9 [902/1745 (52%)]\tLoss: 971.749562 (avg: 1.077328) \tsec/iter: 0.1956 s\t Accuracy (avg) 43.902\n",
      "Train Epoch: 9 [1002/1745 (57%)]\tLoss: 1081.446699 (avg: 1.079288) \tsec/iter: 0.1958 s\t Accuracy (avg) 43.214\n",
      "Train Epoch: 9 [1102/1745 (63%)]\tLoss: 1185.416401 (avg: 1.075695) \tsec/iter: 0.1956 s\t Accuracy (avg) 42.922\n",
      "Train Epoch: 9 [1202/1745 (69%)]\tLoss: 1290.158974 (avg: 1.073344) \tsec/iter: 0.1958 s\t Accuracy (avg) 43.178\n",
      "Train Epoch: 9 [1302/1745 (75%)]\tLoss: 1392.426033 (avg: 1.069452) \tsec/iter: 0.1959 s\t Accuracy (avg) 43.395\n",
      "Train Epoch: 9 [1402/1745 (80%)]\tLoss: 1496.499591 (avg: 1.067403) \tsec/iter: 0.1958 s\t Accuracy (avg) 43.295\n",
      "Train Epoch: 9 [1502/1745 (86%)]\tLoss: 1603.049029 (avg: 1.067276) \tsec/iter: 0.1958 s\t Accuracy (avg) 43.076\n",
      "Train Epoch: 9 [1602/1745 (92%)]\tLoss: 1709.899755 (avg: 1.067353) \tsec/iter: 0.1957 s\t Accuracy (avg) 43.258\n",
      "Train Epoch: 9 [1702/1745 (97%)]\tLoss: 1816.038226 (avg: 1.067002) \tsec/iter: 0.1956 s\t Accuracy (avg) 43.361\n",
      "Train Epoch: 9 [1745/1745 (100%)]\tLoss: 1859.906483 (avg: 1.065849) \tsec/iter: 0.1955 s\t Accuracy (avg) 43.668\n",
      "Took 170.71589946746826 sec for this epoch\n",
      "Validation set (epoch 9): Loss: 226.0383, Average loss: 1.0369, Accuracy: 102/218 (46.79%) Took 17.358460187911987 sec\n",
      "\n",
      "Train Epoch: 10 [2/1745 (0%)]\tLoss: 1.512688 (avg: 0.756344) \tsec/iter: 0.1486 s\t Accuracy (avg) 100.000\n",
      "Train Epoch: 10 [102/1745 (6%)]\tLoss: 109.186893 (avg: 1.070460) \tsec/iter: 0.1923 s\t Accuracy (avg) 46.078\n",
      "Train Epoch: 10 [202/1745 (12%)]\tLoss: 219.481541 (avg: 1.086542) \tsec/iter: 0.1937 s\t Accuracy (avg) 41.584\n",
      "Train Epoch: 10 [302/1745 (17%)]\tLoss: 324.282681 (avg: 1.073784) \tsec/iter: 0.1944 s\t Accuracy (avg) 42.715\n",
      "Train Epoch: 10 [402/1745 (23%)]\tLoss: 430.247755 (avg: 1.070268) \tsec/iter: 0.1944 s\t Accuracy (avg) 43.781\n",
      "Train Epoch: 10 [502/1745 (29%)]\tLoss: 535.662919 (avg: 1.067058) \tsec/iter: 0.1946 s\t Accuracy (avg) 43.825\n",
      "Train Epoch: 10 [602/1745 (34%)]\tLoss: 634.933462 (avg: 1.054707) \tsec/iter: 0.1947 s\t Accuracy (avg) 45.017\n",
      "Train Epoch: 10 [702/1745 (40%)]\tLoss: 742.778377 (avg: 1.058089) \tsec/iter: 0.1946 s\t Accuracy (avg) 45.014\n",
      "Train Epoch: 10 [802/1745 (46%)]\tLoss: 851.030576 (avg: 1.061135) \tsec/iter: 0.1946 s\t Accuracy (avg) 44.514\n",
      "Train Epoch: 10 [902/1745 (52%)]\tLoss: 956.389951 (avg: 1.060299) \tsec/iter: 0.1946 s\t Accuracy (avg) 44.346\n",
      "Train Epoch: 10 [1002/1745 (57%)]\tLoss: 1063.573446 (avg: 1.061451) \tsec/iter: 0.1947 s\t Accuracy (avg) 44.611\n",
      "Train Epoch: 10 [1102/1745 (63%)]\tLoss: 1172.660663 (avg: 1.064120) \tsec/iter: 0.1943 s\t Accuracy (avg) 43.648\n",
      "Train Epoch: 10 [1202/1745 (69%)]\tLoss: 1278.996367 (avg: 1.064057) \tsec/iter: 0.1941 s\t Accuracy (avg) 43.760\n",
      "Train Epoch: 10 [1302/1745 (75%)]\tLoss: 1388.280342 (avg: 1.066268) \tsec/iter: 0.1940 s\t Accuracy (avg) 42.780\n",
      "Train Epoch: 10 [1402/1745 (80%)]\tLoss: 1490.046210 (avg: 1.062800) \tsec/iter: 0.1942 s\t Accuracy (avg) 43.795\n",
      "Train Epoch: 10 [1502/1745 (86%)]\tLoss: 1596.741422 (avg: 1.063077) \tsec/iter: 0.1943 s\t Accuracy (avg) 44.008\n",
      "Train Epoch: 10 [1602/1745 (92%)]\tLoss: 1704.539955 (avg: 1.064007) \tsec/iter: 0.1944 s\t Accuracy (avg) 43.633\n",
      "Train Epoch: 10 [1702/1745 (97%)]\tLoss: 1810.827065 (avg: 1.063941) \tsec/iter: 0.1945 s\t Accuracy (avg) 43.478\n",
      "Train Epoch: 10 [1745/1745 (100%)]\tLoss: 1858.626802 (avg: 1.065116) \tsec/iter: 0.1944 s\t Accuracy (avg) 43.266\n",
      "Took 169.7118444442749 sec for this epoch\n",
      "Validation set (epoch 10): Loss: 228.7719, Average loss: 1.0494, Accuracy: 102/218 (46.79%) Took 17.373456954956055 sec\n",
      "\n",
      "Train Epoch: 11 [2/1745 (0%)]\tLoss: 2.409233 (avg: 1.204617) \tsec/iter: 0.1498 s\t Accuracy (avg) 0.000\n",
      "Train Epoch: 11 [102/1745 (6%)]\tLoss: 111.092582 (avg: 1.089143) \tsec/iter: 0.1941 s\t Accuracy (avg) 40.196\n",
      "Train Epoch: 11 [202/1745 (12%)]\tLoss: 216.651326 (avg: 1.072531) \tsec/iter: 0.1955 s\t Accuracy (avg) 41.584\n",
      "Train Epoch: 11 [302/1745 (17%)]\tLoss: 321.285405 (avg: 1.063859) \tsec/iter: 0.1958 s\t Accuracy (avg) 42.053\n",
      "Train Epoch: 11 [402/1745 (23%)]\tLoss: 422.988185 (avg: 1.052209) \tsec/iter: 0.1957 s\t Accuracy (avg) 44.776\n",
      "Train Epoch: 11 [502/1745 (29%)]\tLoss: 526.562168 (avg: 1.048929) \tsec/iter: 0.1956 s\t Accuracy (avg) 43.227\n",
      "Train Epoch: 11 [602/1745 (34%)]\tLoss: 638.080169 (avg: 1.059934) \tsec/iter: 0.1956 s\t Accuracy (avg) 42.359\n",
      "Train Epoch: 11 [702/1745 (40%)]\tLoss: 743.868942 (avg: 1.059642) \tsec/iter: 0.1954 s\t Accuracy (avg) 43.020\n",
      "Train Epoch: 11 [802/1745 (46%)]\tLoss: 852.246656 (avg: 1.062652) \tsec/iter: 0.1955 s\t Accuracy (avg) 42.394\n",
      "Train Epoch: 11 [902/1745 (52%)]\tLoss: 957.101694 (avg: 1.061088) \tsec/iter: 0.1954 s\t Accuracy (avg) 43.570\n",
      "Train Epoch: 11 [1002/1745 (57%)]\tLoss: 1068.330611 (avg: 1.066198) \tsec/iter: 0.1955 s\t Accuracy (avg) 43.114\n",
      "Train Epoch: 11 [1102/1745 (63%)]\tLoss: 1174.244900 (avg: 1.065558) \tsec/iter: 0.1954 s\t Accuracy (avg) 43.103\n",
      "Train Epoch: 11 [1202/1745 (69%)]\tLoss: 1279.497388 (avg: 1.064474) \tsec/iter: 0.1954 s\t Accuracy (avg) 43.178\n",
      "Train Epoch: 11 [1302/1745 (75%)]\tLoss: 1386.818017 (avg: 1.065144) \tsec/iter: 0.1952 s\t Accuracy (avg) 43.395\n",
      "Train Epoch: 11 [1402/1745 (80%)]\tLoss: 1494.873480 (avg: 1.066244) \tsec/iter: 0.1951 s\t Accuracy (avg) 43.367\n",
      "Train Epoch: 11 [1502/1745 (86%)]\tLoss: 1605.047303 (avg: 1.068607) \tsec/iter: 0.1949 s\t Accuracy (avg) 42.876\n",
      "Train Epoch: 11 [1602/1745 (92%)]\tLoss: 1708.880982 (avg: 1.066717) \tsec/iter: 0.1948 s\t Accuracy (avg) 43.196\n",
      "Train Epoch: 11 [1702/1745 (97%)]\tLoss: 1814.807610 (avg: 1.066279) \tsec/iter: 0.1947 s\t Accuracy (avg) 43.008\n",
      "Train Epoch: 11 [1745/1745 (100%)]\tLoss: 1860.439837 (avg: 1.066155) \tsec/iter: 0.1946 s\t Accuracy (avg) 42.923\n",
      "Took 169.9351065158844 sec for this epoch\n",
      "Validation set (epoch 11): Loss: 226.2734, Average loss: 1.0380, Accuracy: 102/218 (46.79%) Took 17.271530628204346 sec\n",
      "\n",
      "Train Epoch: 12 [2/1745 (0%)]\tLoss: 1.777148 (avg: 0.888574) \tsec/iter: 0.1454 s\t Accuracy (avg) 50.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [102/1745 (6%)]\tLoss: 106.752707 (avg: 1.046595) \tsec/iter: 0.1913 s\t Accuracy (avg) 46.078\n",
      "Train Epoch: 12 [202/1745 (12%)]\tLoss: 213.792094 (avg: 1.058377) \tsec/iter: 0.1912 s\t Accuracy (avg) 41.584\n",
      "Train Epoch: 12 [302/1745 (17%)]\tLoss: 323.112982 (avg: 1.069911) \tsec/iter: 0.1920 s\t Accuracy (avg) 38.742\n",
      "Train Epoch: 12 [402/1745 (23%)]\tLoss: 426.491799 (avg: 1.060925) \tsec/iter: 0.1923 s\t Accuracy (avg) 38.308\n",
      "Train Epoch: 12 [502/1745 (29%)]\tLoss: 535.179215 (avg: 1.066094) \tsec/iter: 0.1929 s\t Accuracy (avg) 38.645\n",
      "Train Epoch: 12 [602/1745 (34%)]\tLoss: 641.762461 (avg: 1.066051) \tsec/iter: 0.1932 s\t Accuracy (avg) 40.199\n",
      "Train Epoch: 12 [702/1745 (40%)]\tLoss: 744.032539 (avg: 1.059875) \tsec/iter: 0.1930 s\t Accuracy (avg) 41.880\n",
      "Train Epoch: 12 [802/1745 (46%)]\tLoss: 844.722002 (avg: 1.053269) \tsec/iter: 0.1931 s\t Accuracy (avg) 43.017\n",
      "Train Epoch: 12 [902/1745 (52%)]\tLoss: 953.337765 (avg: 1.056915) \tsec/iter: 0.1930 s\t Accuracy (avg) 42.905\n",
      "Train Epoch: 12 [1002/1745 (57%)]\tLoss: 1058.510785 (avg: 1.056398) \tsec/iter: 0.1929 s\t Accuracy (avg) 43.214\n",
      "Train Epoch: 12 [1102/1745 (63%)]\tLoss: 1167.695451 (avg: 1.059615) \tsec/iter: 0.1929 s\t Accuracy (avg) 42.559\n",
      "Train Epoch: 12 [1202/1745 (69%)]\tLoss: 1274.083224 (avg: 1.059969) \tsec/iter: 0.1928 s\t Accuracy (avg) 42.596\n",
      "Train Epoch: 12 [1302/1745 (75%)]\tLoss: 1381.497516 (avg: 1.061058) \tsec/iter: 0.1927 s\t Accuracy (avg) 43.164\n",
      "Train Epoch: 12 [1402/1745 (80%)]\tLoss: 1488.760249 (avg: 1.061883) \tsec/iter: 0.1926 s\t Accuracy (avg) 43.224\n",
      "Train Epoch: 12 [1502/1745 (86%)]\tLoss: 1595.388127 (avg: 1.062176) \tsec/iter: 0.1926 s\t Accuracy (avg) 43.076\n",
      "Train Epoch: 12 [1602/1745 (92%)]\tLoss: 1702.453236 (avg: 1.062705) \tsec/iter: 0.1926 s\t Accuracy (avg) 42.135\n",
      "Train Epoch: 12 [1702/1745 (97%)]\tLoss: 1806.462863 (avg: 1.061377) \tsec/iter: 0.1925 s\t Accuracy (avg) 42.891\n",
      "Train Epoch: 12 [1745/1745 (100%)]\tLoss: 1852.402223 (avg: 1.061549) \tsec/iter: 0.1924 s\t Accuracy (avg) 42.980\n",
      "Took 167.9944715499878 sec for this epoch\n",
      "Validation set (epoch 12): Loss: 227.0575, Average loss: 1.0415, Accuracy: 102/218 (46.79%) Took 17.092297315597534 sec\n",
      "\n",
      "Train Epoch: 13 [2/1745 (0%)]\tLoss: 1.739413 (avg: 0.869706) \tsec/iter: 0.1488 s\t Accuracy (avg) 50.000\n",
      "Train Epoch: 13 [102/1745 (6%)]\tLoss: 109.730274 (avg: 1.075787) \tsec/iter: 0.1912 s\t Accuracy (avg) 42.157\n",
      "Train Epoch: 13 [202/1745 (12%)]\tLoss: 213.824308 (avg: 1.058536) \tsec/iter: 0.1915 s\t Accuracy (avg) 45.050\n",
      "Train Epoch: 13 [302/1745 (17%)]\tLoss: 322.948578 (avg: 1.069366) \tsec/iter: 0.1917 s\t Accuracy (avg) 42.715\n",
      "Train Epoch: 13 [402/1745 (23%)]\tLoss: 426.639355 (avg: 1.061292) \tsec/iter: 0.1924 s\t Accuracy (avg) 45.025\n",
      "Train Epoch: 13 [502/1745 (29%)]\tLoss: 528.365119 (avg: 1.052520) \tsec/iter: 0.1924 s\t Accuracy (avg) 45.618\n",
      "Train Epoch: 13 [602/1745 (34%)]\tLoss: 638.552242 (avg: 1.060718) \tsec/iter: 0.1924 s\t Accuracy (avg) 45.349\n",
      "Train Epoch: 13 [702/1745 (40%)]\tLoss: 746.494121 (avg: 1.063382) \tsec/iter: 0.1929 s\t Accuracy (avg) 44.872\n",
      "Train Epoch: 13 [802/1745 (46%)]\tLoss: 853.469318 (avg: 1.064176) \tsec/iter: 0.1927 s\t Accuracy (avg) 45.511\n",
      "Train Epoch: 13 [902/1745 (52%)]\tLoss: 961.054782 (avg: 1.065471) \tsec/iter: 0.1925 s\t Accuracy (avg) 44.900\n",
      "Train Epoch: 13 [1002/1745 (57%)]\tLoss: 1067.161864 (avg: 1.065032) \tsec/iter: 0.1926 s\t Accuracy (avg) 44.810\n",
      "Train Epoch: 13 [1102/1745 (63%)]\tLoss: 1174.059354 (avg: 1.065390) \tsec/iter: 0.1925 s\t Accuracy (avg) 44.737\n",
      "Train Epoch: 13 [1202/1745 (69%)]\tLoss: 1280.324794 (avg: 1.065162) \tsec/iter: 0.1926 s\t Accuracy (avg) 44.925\n",
      "Train Epoch: 13 [1302/1745 (75%)]\tLoss: 1386.727109 (avg: 1.065075) \tsec/iter: 0.1927 s\t Accuracy (avg) 44.547\n",
      "Train Epoch: 13 [1402/1745 (80%)]\tLoss: 1491.112328 (avg: 1.063561) \tsec/iter: 0.1927 s\t Accuracy (avg) 44.508\n",
      "Train Epoch: 13 [1502/1745 (86%)]\tLoss: 1597.312550 (avg: 1.063457) \tsec/iter: 0.1927 s\t Accuracy (avg) 44.341\n",
      "Train Epoch: 13 [1602/1745 (92%)]\tLoss: 1704.868310 (avg: 1.064212) \tsec/iter: 0.1926 s\t Accuracy (avg) 44.320\n",
      "Train Epoch: 13 [1702/1745 (97%)]\tLoss: 1810.616612 (avg: 1.063817) \tsec/iter: 0.1926 s\t Accuracy (avg) 44.536\n",
      "Train Epoch: 13 [1745/1745 (100%)]\tLoss: 1856.775293 (avg: 1.064055) \tsec/iter: 0.1926 s\t Accuracy (avg) 44.527\n",
      "Took 168.14068460464478 sec for this epoch\n",
      "Validation set (epoch 13): Loss: 226.3860, Average loss: 1.0385, Accuracy: 102/218 (46.79%) Took 17.107948064804077 sec\n",
      "\n",
      "Train Epoch: 14 [2/1745 (0%)]\tLoss: 1.937221 (avg: 0.968610) \tsec/iter: 0.1462 s\t Accuracy (avg) 50.000\n",
      "Train Epoch: 14 [102/1745 (6%)]\tLoss: 106.685132 (avg: 1.045933) \tsec/iter: 0.1902 s\t Accuracy (avg) 49.020\n",
      "Train Epoch: 14 [202/1745 (12%)]\tLoss: 212.359686 (avg: 1.051286) \tsec/iter: 0.1916 s\t Accuracy (avg) 45.050\n",
      "Train Epoch: 14 [302/1745 (17%)]\tLoss: 321.298002 (avg: 1.063901) \tsec/iter: 0.1918 s\t Accuracy (avg) 42.715\n",
      "Train Epoch: 14 [402/1745 (23%)]\tLoss: 422.844491 (avg: 1.051852) \tsec/iter: 0.1921 s\t Accuracy (avg) 44.776\n",
      "Train Epoch: 14 [502/1745 (29%)]\tLoss: 532.541397 (avg: 1.060839) \tsec/iter: 0.1920 s\t Accuracy (avg) 43.227\n",
      "Train Epoch: 14 [602/1745 (34%)]\tLoss: 641.633596 (avg: 1.065837) \tsec/iter: 0.1921 s\t Accuracy (avg) 42.359\n",
      "Train Epoch: 14 [702/1745 (40%)]\tLoss: 752.172369 (avg: 1.071471) \tsec/iter: 0.1925 s\t Accuracy (avg) 42.593\n",
      "Train Epoch: 14 [802/1745 (46%)]\tLoss: 860.096979 (avg: 1.072440) \tsec/iter: 0.1925 s\t Accuracy (avg) 42.768\n",
      "Train Epoch: 14 [902/1745 (52%)]\tLoss: 965.823717 (avg: 1.070758) \tsec/iter: 0.1924 s\t Accuracy (avg) 43.459\n",
      "Train Epoch: 14 [1002/1745 (57%)]\tLoss: 1077.914502 (avg: 1.075763) \tsec/iter: 0.1924 s\t Accuracy (avg) 42.615\n",
      "Train Epoch: 14 [1102/1745 (63%)]\tLoss: 1184.138744 (avg: 1.074536) \tsec/iter: 0.1924 s\t Accuracy (avg) 42.196\n",
      "Train Epoch: 14 [1202/1745 (69%)]\tLoss: 1292.329257 (avg: 1.075149) \tsec/iter: 0.1924 s\t Accuracy (avg) 41.847\n",
      "Train Epoch: 14 [1302/1745 (75%)]\tLoss: 1397.995699 (avg: 1.073729) \tsec/iter: 0.1924 s\t Accuracy (avg) 42.012\n",
      "Train Epoch: 14 [1402/1745 (80%)]\tLoss: 1502.838765 (avg: 1.071925) \tsec/iter: 0.1923 s\t Accuracy (avg) 42.297\n",
      "Train Epoch: 14 [1502/1745 (86%)]\tLoss: 1609.636975 (avg: 1.071662) \tsec/iter: 0.1922 s\t Accuracy (avg) 42.344\n",
      "Train Epoch: 14 [1602/1745 (92%)]\tLoss: 1707.687127 (avg: 1.065972) \tsec/iter: 0.1923 s\t Accuracy (avg) 43.071\n",
      "Train Epoch: 14 [1702/1745 (97%)]\tLoss: 1813.098473 (avg: 1.065275) \tsec/iter: 0.1924 s\t Accuracy (avg) 42.832\n",
      "Train Epoch: 14 [1745/1745 (100%)]\tLoss: 1861.673577 (avg: 1.066862) \tsec/iter: 0.1924 s\t Accuracy (avg) 42.865\n",
      "Took 167.95915150642395 sec for this epoch\n",
      "Validation set (epoch 14): Loss: 228.6260, Average loss: 1.0487, Accuracy: 102/218 (46.79%) Took 17.410542726516724 sec\n",
      "\n",
      "Train Epoch: 15 [2/1745 (0%)]\tLoss: 1.840139 (avg: 0.920069) \tsec/iter: 0.1521 s\t Accuracy (avg) 50.000\n",
      "Train Epoch: 15 [102/1745 (6%)]\tLoss: 111.060107 (avg: 1.088825) \tsec/iter: 0.1959 s\t Accuracy (avg) 35.294\n",
      "Train Epoch: 15 [202/1745 (12%)]\tLoss: 213.568711 (avg: 1.057271) \tsec/iter: 0.1971 s\t Accuracy (avg) 37.624\n",
      "Train Epoch: 15 [302/1745 (17%)]\tLoss: 321.715001 (avg: 1.065281) \tsec/iter: 0.1971 s\t Accuracy (avg) 41.060\n",
      "Train Epoch: 15 [402/1745 (23%)]\tLoss: 432.164974 (avg: 1.075037) \tsec/iter: 0.1969 s\t Accuracy (avg) 39.055\n",
      "Train Epoch: 15 [502/1745 (29%)]\tLoss: 537.991919 (avg: 1.071697) \tsec/iter: 0.1965 s\t Accuracy (avg) 41.235\n",
      "Train Epoch: 15 [602/1745 (34%)]\tLoss: 648.840964 (avg: 1.077809) \tsec/iter: 0.1961 s\t Accuracy (avg) 41.196\n",
      "Train Epoch: 15 [702/1745 (40%)]\tLoss: 750.931866 (avg: 1.069704) \tsec/iter: 0.1958 s\t Accuracy (avg) 42.450\n",
      "Train Epoch: 15 [802/1745 (46%)]\tLoss: 861.607350 (avg: 1.074323) \tsec/iter: 0.1958 s\t Accuracy (avg) 42.145\n",
      "Train Epoch: 15 [902/1745 (52%)]\tLoss: 962.695418 (avg: 1.067290) \tsec/iter: 0.1956 s\t Accuracy (avg) 42.794\n",
      "Train Epoch: 15 [1002/1745 (57%)]\tLoss: 1071.223472 (avg: 1.069085) \tsec/iter: 0.1956 s\t Accuracy (avg) 42.615\n",
      "Train Epoch: 15 [1102/1745 (63%)]\tLoss: 1172.328779 (avg: 1.063819) \tsec/iter: 0.1954 s\t Accuracy (avg) 43.829\n",
      "Train Epoch: 15 [1202/1745 (69%)]\tLoss: 1280.474863 (avg: 1.065287) \tsec/iter: 0.1953 s\t Accuracy (avg) 43.261\n",
      "Train Epoch: 15 [1302/1745 (75%)]\tLoss: 1386.598844 (avg: 1.064976) \tsec/iter: 0.1954 s\t Accuracy (avg) 43.088\n",
      "Train Epoch: 15 [1402/1745 (80%)]\tLoss: 1492.185757 (avg: 1.064327) \tsec/iter: 0.1955 s\t Accuracy (avg) 42.725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [1502/1745 (86%)]\tLoss: 1594.482874 (avg: 1.061573) \tsec/iter: 0.1954 s\t Accuracy (avg) 43.209\n",
      "Train Epoch: 15 [1602/1745 (92%)]\tLoss: 1700.856305 (avg: 1.061708) \tsec/iter: 0.1955 s\t Accuracy (avg) 43.508\n",
      "Train Epoch: 15 [1702/1745 (97%)]\tLoss: 1809.302332 (avg: 1.063045) \tsec/iter: 0.1958 s\t Accuracy (avg) 43.478\n",
      "Train Epoch: 15 [1745/1745 (100%)]\tLoss: 1855.920032 (avg: 1.063564) \tsec/iter: 0.1957 s\t Accuracy (avg) 43.381\n",
      "Took 170.84360241889954 sec for this epoch\n",
      "Validation set (epoch 15): Loss: 228.2971, Average loss: 1.0472, Accuracy: 102/218 (46.79%) Took 17.635475397109985 sec\n",
      "\n",
      "Train Epoch: 16 [2/1745 (0%)]\tLoss: 1.930571 (avg: 0.965285) \tsec/iter: 0.1506 s\t Accuracy (avg) 50.000\n",
      "Train Epoch: 16 [102/1745 (6%)]\tLoss: 109.873444 (avg: 1.077191) \tsec/iter: 0.1963 s\t Accuracy (avg) 39.216\n",
      "Train Epoch: 16 [202/1745 (12%)]\tLoss: 216.334436 (avg: 1.070963) \tsec/iter: 0.1963 s\t Accuracy (avg) 39.604\n",
      "Train Epoch: 16 [302/1745 (17%)]\tLoss: 324.357090 (avg: 1.074030) \tsec/iter: 0.1968 s\t Accuracy (avg) 39.404\n",
      "Train Epoch: 16 [402/1745 (23%)]\tLoss: 431.256482 (avg: 1.072777) \tsec/iter: 0.1964 s\t Accuracy (avg) 39.303\n",
      "Train Epoch: 16 [502/1745 (29%)]\tLoss: 537.851284 (avg: 1.071417) \tsec/iter: 0.1959 s\t Accuracy (avg) 40.837\n",
      "Train Epoch: 16 [602/1745 (34%)]\tLoss: 640.062438 (avg: 1.063227) \tsec/iter: 0.1958 s\t Accuracy (avg) 41.362\n",
      "Train Epoch: 16 [702/1745 (40%)]\tLoss: 743.322566 (avg: 1.058864) \tsec/iter: 0.1957 s\t Accuracy (avg) 42.735\n",
      "Train Epoch: 16 [802/1745 (46%)]\tLoss: 850.796748 (avg: 1.060844) \tsec/iter: 0.1959 s\t Accuracy (avg) 43.766\n",
      "Train Epoch: 16 [902/1745 (52%)]\tLoss: 959.748025 (avg: 1.064022) \tsec/iter: 0.1955 s\t Accuracy (avg) 43.459\n",
      "Train Epoch: 16 [1002/1745 (57%)]\tLoss: 1067.778120 (avg: 1.065647) \tsec/iter: 0.1954 s\t Accuracy (avg) 43.114\n",
      "Train Epoch: 16 [1102/1745 (63%)]\tLoss: 1174.259272 (avg: 1.065571) \tsec/iter: 0.1957 s\t Accuracy (avg) 43.103\n",
      "Train Epoch: 16 [1202/1745 (69%)]\tLoss: 1283.427742 (avg: 1.067744) \tsec/iter: 0.1957 s\t Accuracy (avg) 43.095\n",
      "Train Epoch: 16 [1302/1745 (75%)]\tLoss: 1389.129018 (avg: 1.066919) \tsec/iter: 0.1956 s\t Accuracy (avg) 43.088\n",
      "Train Epoch: 16 [1402/1745 (80%)]\tLoss: 1491.687950 (avg: 1.063971) \tsec/iter: 0.1954 s\t Accuracy (avg) 43.224\n",
      "Train Epoch: 16 [1502/1745 (86%)]\tLoss: 1597.614283 (avg: 1.063658) \tsec/iter: 0.1953 s\t Accuracy (avg) 43.609\n",
      "Train Epoch: 16 [1602/1745 (92%)]\tLoss: 1707.159160 (avg: 1.065642) \tsec/iter: 0.1950 s\t Accuracy (avg) 43.446\n",
      "Train Epoch: 16 [1702/1745 (97%)]\tLoss: 1814.037973 (avg: 1.065827) \tsec/iter: 0.1949 s\t Accuracy (avg) 43.126\n",
      "Train Epoch: 16 [1745/1745 (100%)]\tLoss: 1857.831143 (avg: 1.064660) \tsec/iter: 0.1948 s\t Accuracy (avg) 43.266\n",
      "Took 170.0574769973755 sec for this epoch\n",
      "Validation set (epoch 16): Loss: 226.0839, Average loss: 1.0371, Accuracy: 102/218 (46.79%) Took 17.152417421340942 sec\n",
      "\n",
      "Train Epoch: 17 [2/1745 (0%)]\tLoss: 1.865448 (avg: 0.932724) \tsec/iter: 0.1443 s\t Accuracy (avg) 50.000\n",
      "Train Epoch: 17 [102/1745 (6%)]\tLoss: 107.702709 (avg: 1.055909) \tsec/iter: 0.1919 s\t Accuracy (avg) 41.176\n",
      "Train Epoch: 17 [202/1745 (12%)]\tLoss: 214.061497 (avg: 1.059710) \tsec/iter: 0.1934 s\t Accuracy (avg) 44.059\n",
      "Train Epoch: 17 [302/1745 (17%)]\tLoss: 321.673834 (avg: 1.065145) \tsec/iter: 0.1946 s\t Accuracy (avg) 42.053\n",
      "Train Epoch: 17 [402/1745 (23%)]\tLoss: 428.799940 (avg: 1.066667) \tsec/iter: 0.1956 s\t Accuracy (avg) 43.035\n",
      "Train Epoch: 17 [502/1745 (29%)]\tLoss: 533.605277 (avg: 1.062959) \tsec/iter: 0.1961 s\t Accuracy (avg) 44.422\n",
      "Train Epoch: 17 [602/1745 (34%)]\tLoss: 640.278859 (avg: 1.063586) \tsec/iter: 0.1965 s\t Accuracy (avg) 44.186\n",
      "Train Epoch: 17 [702/1745 (40%)]\tLoss: 751.773623 (avg: 1.070903) \tsec/iter: 0.1966 s\t Accuracy (avg) 43.875\n",
      "Train Epoch: 17 [802/1745 (46%)]\tLoss: 857.161551 (avg: 1.068780) \tsec/iter: 0.1966 s\t Accuracy (avg) 44.514\n",
      "Train Epoch: 17 [902/1745 (52%)]\tLoss: 963.125280 (avg: 1.067766) \tsec/iter: 0.1964 s\t Accuracy (avg) 44.457\n",
      "Train Epoch: 17 [1002/1745 (57%)]\tLoss: 1063.251653 (avg: 1.061129) \tsec/iter: 0.1961 s\t Accuracy (avg) 45.309\n",
      "Train Epoch: 17 [1102/1745 (63%)]\tLoss: 1169.261916 (avg: 1.061036) \tsec/iter: 0.1958 s\t Accuracy (avg) 45.281\n",
      "Train Epoch: 17 [1202/1745 (69%)]\tLoss: 1272.473803 (avg: 1.058630) \tsec/iter: 0.1958 s\t Accuracy (avg) 44.842\n",
      "Train Epoch: 17 [1302/1745 (75%)]\tLoss: 1378.209242 (avg: 1.058532) \tsec/iter: 0.1955 s\t Accuracy (avg) 44.777\n",
      "Train Epoch: 17 [1402/1745 (80%)]\tLoss: 1491.317526 (avg: 1.063707) \tsec/iter: 0.1953 s\t Accuracy (avg) 43.652\n",
      "Train Epoch: 17 [1502/1745 (86%)]\tLoss: 1597.962005 (avg: 1.063889) \tsec/iter: 0.1953 s\t Accuracy (avg) 43.409\n",
      "Train Epoch: 17 [1602/1745 (92%)]\tLoss: 1701.295660 (avg: 1.061982) \tsec/iter: 0.1952 s\t Accuracy (avg) 43.321\n",
      "Train Epoch: 17 [1702/1745 (97%)]\tLoss: 1806.618288 (avg: 1.061468) \tsec/iter: 0.1951 s\t Accuracy (avg) 43.655\n",
      "Train Epoch: 17 [1745/1745 (100%)]\tLoss: 1853.856582 (avg: 1.062382) \tsec/iter: 0.1950 s\t Accuracy (avg) 43.610\n",
      "Took 170.21950340270996 sec for this epoch\n",
      "Validation set (epoch 17): Loss: 226.8249, Average loss: 1.0405, Accuracy: 102/218 (46.79%) Took 17.22439670562744 sec\n",
      "\n",
      "Train Epoch: 18 [2/1745 (0%)]\tLoss: 1.920265 (avg: 0.960133) \tsec/iter: 0.1476 s\t Accuracy (avg) 50.000\n",
      "Train Epoch: 18 [102/1745 (6%)]\tLoss: 111.579261 (avg: 1.093914) \tsec/iter: 0.1935 s\t Accuracy (avg) 40.196\n",
      "Train Epoch: 18 [202/1745 (12%)]\tLoss: 220.846548 (avg: 1.093300) \tsec/iter: 0.1935 s\t Accuracy (avg) 39.604\n",
      "Train Epoch: 18 [302/1745 (17%)]\tLoss: 325.259010 (avg: 1.077017) \tsec/iter: 0.1945 s\t Accuracy (avg) 42.384\n",
      "Train Epoch: 18 [402/1745 (23%)]\tLoss: 432.508053 (avg: 1.075891) \tsec/iter: 0.1940 s\t Accuracy (avg) 42.289\n",
      "Train Epoch: 18 [502/1745 (29%)]\tLoss: 532.800313 (avg: 1.061355) \tsec/iter: 0.1940 s\t Accuracy (avg) 45.219\n",
      "Train Epoch: 18 [602/1745 (34%)]\tLoss: 644.199067 (avg: 1.070098) \tsec/iter: 0.1938 s\t Accuracy (avg) 43.522\n",
      "Train Epoch: 18 [702/1745 (40%)]\tLoss: 748.489541 (avg: 1.066224) \tsec/iter: 0.1939 s\t Accuracy (avg) 44.444\n",
      "Train Epoch: 18 [802/1745 (46%)]\tLoss: 857.940880 (avg: 1.069752) \tsec/iter: 0.1936 s\t Accuracy (avg) 43.890\n",
      "Train Epoch: 18 [902/1745 (52%)]\tLoss: 960.104531 (avg: 1.064417) \tsec/iter: 0.1938 s\t Accuracy (avg) 44.568\n",
      "Train Epoch: 18 [1002/1745 (57%)]\tLoss: 1065.158071 (avg: 1.063032) \tsec/iter: 0.1941 s\t Accuracy (avg) 44.012\n",
      "Train Epoch: 18 [1102/1745 (63%)]\tLoss: 1170.062960 (avg: 1.061763) \tsec/iter: 0.1943 s\t Accuracy (avg) 43.648\n",
      "Train Epoch: 18 [1202/1745 (69%)]\tLoss: 1277.339263 (avg: 1.062678) \tsec/iter: 0.1943 s\t Accuracy (avg) 42.845\n",
      "Train Epoch: 18 [1302/1745 (75%)]\tLoss: 1380.277090 (avg: 1.060121) \tsec/iter: 0.1944 s\t Accuracy (avg) 43.779\n",
      "Train Epoch: 18 [1402/1745 (80%)]\tLoss: 1490.497380 (avg: 1.063122) \tsec/iter: 0.1944 s\t Accuracy (avg) 43.581\n",
      "Train Epoch: 18 [1502/1745 (86%)]\tLoss: 1595.147033 (avg: 1.062015) \tsec/iter: 0.1945 s\t Accuracy (avg) 43.941\n",
      "Train Epoch: 18 [1602/1745 (92%)]\tLoss: 1702.341684 (avg: 1.062635) \tsec/iter: 0.1946 s\t Accuracy (avg) 43.945\n",
      "Train Epoch: 18 [1702/1745 (97%)]\tLoss: 1808.067947 (avg: 1.062320) \tsec/iter: 0.1946 s\t Accuracy (avg) 44.066\n",
      "Train Epoch: 18 [1745/1745 (100%)]\tLoss: 1850.391885 (avg: 1.060396) \tsec/iter: 0.1945 s\t Accuracy (avg) 44.183\n",
      "Took 169.83249688148499 sec for this epoch\n",
      "Validation set (epoch 18): Loss: 226.2927, Average loss: 1.0380, Accuracy: 102/218 (46.79%) Took 17.428093433380127 sec\n",
      "\n",
      "Train Epoch: 19 [2/1745 (0%)]\tLoss: 1.803381 (avg: 0.901691) \tsec/iter: 0.1498 s\t Accuracy (avg) 50.000\n",
      "Train Epoch: 19 [102/1745 (6%)]\tLoss: 109.458785 (avg: 1.073125) \tsec/iter: 0.1964 s\t Accuracy (avg) 44.118\n",
      "Train Epoch: 19 [202/1745 (12%)]\tLoss: 217.539217 (avg: 1.076927) \tsec/iter: 0.1964 s\t Accuracy (avg) 41.584\n",
      "Train Epoch: 19 [302/1745 (17%)]\tLoss: 324.396295 (avg: 1.074160) \tsec/iter: 0.1961 s\t Accuracy (avg) 40.066\n",
      "Train Epoch: 19 [402/1745 (23%)]\tLoss: 429.384638 (avg: 1.068121) \tsec/iter: 0.1961 s\t Accuracy (avg) 41.542\n",
      "Train Epoch: 19 [502/1745 (29%)]\tLoss: 537.871122 (avg: 1.071456) \tsec/iter: 0.1957 s\t Accuracy (avg) 42.032\n",
      "Train Epoch: 19 [602/1745 (34%)]\tLoss: 646.875713 (avg: 1.074544) \tsec/iter: 0.1956 s\t Accuracy (avg) 40.698\n",
      "Train Epoch: 19 [702/1745 (40%)]\tLoss: 753.896597 (avg: 1.073927) \tsec/iter: 0.1958 s\t Accuracy (avg) 40.171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 19 [802/1745 (46%)]\tLoss: 859.901320 (avg: 1.072196) \tsec/iter: 0.1957 s\t Accuracy (avg) 40.150\n",
      "Train Epoch: 19 [902/1745 (52%)]\tLoss: 961.752265 (avg: 1.066244) \tsec/iter: 0.1955 s\t Accuracy (avg) 41.131\n",
      "Train Epoch: 19 [1002/1745 (57%)]\tLoss: 1070.790357 (avg: 1.068653) \tsec/iter: 0.1955 s\t Accuracy (avg) 41.317\n",
      "Train Epoch: 19 [1102/1745 (63%)]\tLoss: 1176.091482 (avg: 1.067234) \tsec/iter: 0.1954 s\t Accuracy (avg) 41.924\n",
      "Train Epoch: 19 [1202/1745 (69%)]\tLoss: 1281.217624 (avg: 1.065905) \tsec/iter: 0.1954 s\t Accuracy (avg) 42.679\n",
      "Train Epoch: 19 [1302/1745 (75%)]\tLoss: 1389.747684 (avg: 1.067395) \tsec/iter: 0.1952 s\t Accuracy (avg) 42.473\n",
      "Train Epoch: 19 [1402/1745 (80%)]\tLoss: 1500.669000 (avg: 1.070377) \tsec/iter: 0.1953 s\t Accuracy (avg) 42.439\n",
      "Train Epoch: 19 [1502/1745 (86%)]\tLoss: 1603.184542 (avg: 1.067367) \tsec/iter: 0.1952 s\t Accuracy (avg) 43.076\n",
      "Train Epoch: 19 [1602/1745 (92%)]\tLoss: 1705.327257 (avg: 1.064499) \tsec/iter: 0.1952 s\t Accuracy (avg) 43.571\n",
      "Train Epoch: 19 [1702/1745 (97%)]\tLoss: 1806.775252 (avg: 1.061560) \tsec/iter: 0.1951 s\t Accuracy (avg) 43.655\n",
      "Train Epoch: 19 [1745/1745 (100%)]\tLoss: 1851.628973 (avg: 1.061105) \tsec/iter: 0.1950 s\t Accuracy (avg) 43.725\n",
      "Took 170.23045992851257 sec for this epoch\n",
      "Validation set (epoch 19): Loss: 227.1256, Average loss: 1.0419, Accuracy: 102/218 (46.79%) Took 17.371288776397705 sec\n",
      "\n",
      "Train Epoch: 20 [2/1745 (0%)]\tLoss: 1.649259 (avg: 0.824629) \tsec/iter: 0.1485 s\t Accuracy (avg) 100.000\n",
      "Train Epoch: 20 [102/1745 (6%)]\tLoss: 110.958982 (avg: 1.087833) \tsec/iter: 0.1946 s\t Accuracy (avg) 39.216\n",
      "Train Epoch: 20 [202/1745 (12%)]\tLoss: 220.080730 (avg: 1.089509) \tsec/iter: 0.1970 s\t Accuracy (avg) 37.129\n",
      "Train Epoch: 20 [302/1745 (17%)]\tLoss: 324.796189 (avg: 1.075484) \tsec/iter: 0.1958 s\t Accuracy (avg) 38.742\n",
      "Train Epoch: 20 [402/1745 (23%)]\tLoss: 437.092783 (avg: 1.087295) \tsec/iter: 0.1959 s\t Accuracy (avg) 38.060\n",
      "Train Epoch: 20 [502/1745 (29%)]\tLoss: 540.742030 (avg: 1.077175) \tsec/iter: 0.1958 s\t Accuracy (avg) 41.235\n",
      "Train Epoch: 20 [602/1745 (34%)]\tLoss: 647.263174 (avg: 1.075188) \tsec/iter: 0.1955 s\t Accuracy (avg) 41.860\n",
      "Train Epoch: 20 [702/1745 (40%)]\tLoss: 749.242778 (avg: 1.067297) \tsec/iter: 0.1953 s\t Accuracy (avg) 43.305\n",
      "Train Epoch: 20 [802/1745 (46%)]\tLoss: 859.518766 (avg: 1.071719) \tsec/iter: 0.1952 s\t Accuracy (avg) 42.893\n",
      "Train Epoch: 20 [902/1745 (52%)]\tLoss: 967.798722 (avg: 1.072948) \tsec/iter: 0.1954 s\t Accuracy (avg) 42.350\n",
      "Train Epoch: 20 [1002/1745 (57%)]\tLoss: 1073.270183 (avg: 1.071128) \tsec/iter: 0.1955 s\t Accuracy (avg) 42.814\n",
      "Train Epoch: 20 [1102/1745 (63%)]\tLoss: 1175.761529 (avg: 1.066934) \tsec/iter: 0.1953 s\t Accuracy (avg) 42.196\n",
      "Train Epoch: 20 [1202/1745 (69%)]\tLoss: 1281.850121 (avg: 1.066431) \tsec/iter: 0.1952 s\t Accuracy (avg) 42.928\n",
      "Train Epoch: 20 [1302/1745 (75%)]\tLoss: 1391.127076 (avg: 1.068454) \tsec/iter: 0.1951 s\t Accuracy (avg) 42.934\n",
      "Train Epoch: 20 [1402/1745 (80%)]\tLoss: 1496.471904 (avg: 1.067384) \tsec/iter: 0.1952 s\t Accuracy (avg) 43.153\n",
      "Train Epoch: 20 [1502/1745 (86%)]\tLoss: 1599.285741 (avg: 1.064771) \tsec/iter: 0.1952 s\t Accuracy (avg) 43.542\n",
      "Train Epoch: 20 [1602/1745 (92%)]\tLoss: 1701.927362 (avg: 1.062377) \tsec/iter: 0.1952 s\t Accuracy (avg) 44.007\n",
      "Train Epoch: 20 [1702/1745 (97%)]\tLoss: 1808.378895 (avg: 1.062502) \tsec/iter: 0.1952 s\t Accuracy (avg) 43.948\n",
      "Train Epoch: 20 [1745/1745 (100%)]\tLoss: 1851.999647 (avg: 1.061318) \tsec/iter: 0.1951 s\t Accuracy (avg) 44.183\n",
      "Took 170.37030482292175 sec for this epoch\n",
      "Validation set (epoch 20): Loss: 226.4703, Average loss: 1.0389, Accuracy: 102/218 (46.79%) Took 17.359925746917725 sec\n",
      "\n",
      "Train Epoch: 21 [2/1745 (0%)]\tLoss: 2.214934 (avg: 1.107467) \tsec/iter: 0.1484 s\t Accuracy (avg) 0.000\n",
      "Train Epoch: 21 [102/1745 (6%)]\tLoss: 107.093138 (avg: 1.049933) \tsec/iter: 0.1946 s\t Accuracy (avg) 46.078\n",
      "Train Epoch: 21 [202/1745 (12%)]\tLoss: 214.198898 (avg: 1.060391) \tsec/iter: 0.1982 s\t Accuracy (avg) 46.535\n",
      "Train Epoch: 21 [302/1745 (17%)]\tLoss: 323.470293 (avg: 1.071094) \tsec/iter: 0.1985 s\t Accuracy (avg) 44.040\n",
      "Train Epoch: 21 [402/1745 (23%)]\tLoss: 428.074962 (avg: 1.064863) \tsec/iter: 0.1975 s\t Accuracy (avg) 44.030\n",
      "Train Epoch: 21 [502/1745 (29%)]\tLoss: 539.997681 (avg: 1.075693) \tsec/iter: 0.1978 s\t Accuracy (avg) 43.825\n",
      "Train Epoch: 21 [602/1745 (34%)]\tLoss: 649.464972 (avg: 1.078845) \tsec/iter: 0.1967 s\t Accuracy (avg) 43.688\n",
      "Train Epoch: 21 [702/1745 (40%)]\tLoss: 749.314272 (avg: 1.067399) \tsec/iter: 0.1966 s\t Accuracy (avg) 45.157\n",
      "Train Epoch: 21 [802/1745 (46%)]\tLoss: 851.866326 (avg: 1.062177) \tsec/iter: 0.1963 s\t Accuracy (avg) 45.511\n",
      "Train Epoch: 21 [902/1745 (52%)]\tLoss: 955.474979 (avg: 1.059285) \tsec/iter: 0.1962 s\t Accuracy (avg) 45.565\n",
      "Train Epoch: 21 [1002/1745 (57%)]\tLoss: 1055.840381 (avg: 1.053733) \tsec/iter: 0.1960 s\t Accuracy (avg) 46.407\n",
      "Train Epoch: 21 [1102/1745 (63%)]\tLoss: 1162.372171 (avg: 1.054784) \tsec/iter: 0.1959 s\t Accuracy (avg) 46.279\n",
      "Train Epoch: 21 [1202/1745 (69%)]\tLoss: 1269.261877 (avg: 1.055958) \tsec/iter: 0.1958 s\t Accuracy (avg) 45.923\n",
      "Train Epoch: 21 [1302/1745 (75%)]\tLoss: 1376.067178 (avg: 1.056887) \tsec/iter: 0.1958 s\t Accuracy (avg) 45.776\n",
      "Train Epoch: 21 [1402/1745 (80%)]\tLoss: 1483.481212 (avg: 1.058118) \tsec/iter: 0.1958 s\t Accuracy (avg) 45.292\n",
      "Train Epoch: 21 [1502/1745 (86%)]\tLoss: 1590.779579 (avg: 1.059108) \tsec/iter: 0.1959 s\t Accuracy (avg) 44.740\n",
      "Train Epoch: 21 [1602/1745 (92%)]\tLoss: 1693.560572 (avg: 1.057154) \tsec/iter: 0.1960 s\t Accuracy (avg) 44.757\n",
      "Train Epoch: 21 [1702/1745 (97%)]\tLoss: 1796.278291 (avg: 1.055393) \tsec/iter: 0.1966 s\t Accuracy (avg) 44.712\n",
      "Train Epoch: 21 [1745/1745 (100%)]\tLoss: 1842.263409 (avg: 1.055738) \tsec/iter: 0.1967 s\t Accuracy (avg) 44.585\n",
      "Took 171.76086330413818 sec for this epoch\n",
      "Validation set (epoch 21): Loss: 226.0733, Average loss: 1.0370, Accuracy: 102/218 (46.79%) Took 18.53914737701416 sec\n",
      "\n",
      "Train Epoch: 22 [2/1745 (0%)]\tLoss: 1.619431 (avg: 0.809715) \tsec/iter: 0.1571 s\t Accuracy (avg) 100.000\n",
      "Train Epoch: 22 [102/1745 (6%)]\tLoss: 101.801304 (avg: 0.998052) \tsec/iter: 0.2033 s\t Accuracy (avg) 54.902\n",
      "Train Epoch: 22 [202/1745 (12%)]\tLoss: 208.708315 (avg: 1.033209) \tsec/iter: 0.2044 s\t Accuracy (avg) 47.030\n",
      "Train Epoch: 22 [302/1745 (17%)]\tLoss: 314.345442 (avg: 1.040879) \tsec/iter: 0.2052 s\t Accuracy (avg) 48.013\n",
      "Train Epoch: 22 [402/1745 (23%)]\tLoss: 420.637463 (avg: 1.046362) \tsec/iter: 0.2053 s\t Accuracy (avg) 46.766\n",
      "Train Epoch: 22 [502/1745 (29%)]\tLoss: 522.762142 (avg: 1.041359) \tsec/iter: 0.2049 s\t Accuracy (avg) 46.813\n",
      "Train Epoch: 22 [602/1745 (34%)]\tLoss: 635.191540 (avg: 1.055135) \tsec/iter: 0.2047 s\t Accuracy (avg) 45.515\n",
      "Train Epoch: 22 [702/1745 (40%)]\tLoss: 744.408812 (avg: 1.060411) \tsec/iter: 0.2042 s\t Accuracy (avg) 44.302\n",
      "Train Epoch: 22 [802/1745 (46%)]\tLoss: 848.015480 (avg: 1.057376) \tsec/iter: 0.2042 s\t Accuracy (avg) 44.638\n",
      "Train Epoch: 22 [902/1745 (52%)]\tLoss: 953.897092 (avg: 1.057536) \tsec/iter: 0.2040 s\t Accuracy (avg) 44.900\n",
      "Train Epoch: 22 [1002/1745 (57%)]\tLoss: 1061.005971 (avg: 1.058888) \tsec/iter: 0.2041 s\t Accuracy (avg) 44.810\n",
      "Train Epoch: 22 [1102/1745 (63%)]\tLoss: 1165.922825 (avg: 1.058006) \tsec/iter: 0.2042 s\t Accuracy (avg) 44.918\n",
      "Train Epoch: 22 [1202/1745 (69%)]\tLoss: 1274.853469 (avg: 1.060610) \tsec/iter: 0.2044 s\t Accuracy (avg) 44.592\n",
      "Train Epoch: 22 [1302/1745 (75%)]\tLoss: 1381.446503 (avg: 1.061019) \tsec/iter: 0.2046 s\t Accuracy (avg) 44.547\n",
      "Train Epoch: 22 [1402/1745 (80%)]\tLoss: 1484.085001 (avg: 1.058549) \tsec/iter: 0.2044 s\t Accuracy (avg) 44.508\n",
      "Train Epoch: 22 [1502/1745 (86%)]\tLoss: 1588.193157 (avg: 1.057386) \tsec/iter: 0.2045 s\t Accuracy (avg) 44.807\n",
      "Train Epoch: 22 [1602/1745 (92%)]\tLoss: 1695.401461 (avg: 1.058303) \tsec/iter: 0.2045 s\t Accuracy (avg) 44.569\n",
      "Train Epoch: 22 [1702/1745 (97%)]\tLoss: 1796.991637 (avg: 1.055812) \tsec/iter: 0.2045 s\t Accuracy (avg) 44.771\n",
      "Train Epoch: 22 [1745/1745 (100%)]\tLoss: 1842.937604 (avg: 1.056125) \tsec/iter: 0.2044 s\t Accuracy (avg) 44.642\n",
      "Took 178.47098398208618 sec for this epoch\n",
      "Validation set (epoch 22): Loss: 226.0664, Average loss: 1.0370, Accuracy: 102/218 (46.79%) Took 18.447630405426025 sec\n",
      "\n",
      "Train Epoch: 23 [2/1745 (0%)]\tLoss: 2.329123 (avg: 1.164562) \tsec/iter: 0.1568 s\t Accuracy (avg) 50.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 23 [102/1745 (6%)]\tLoss: 104.670922 (avg: 1.026186) \tsec/iter: 0.2042 s\t Accuracy (avg) 50.980\n",
      "Train Epoch: 23 [202/1745 (12%)]\tLoss: 208.938457 (avg: 1.034349) \tsec/iter: 0.2048 s\t Accuracy (avg) 50.990\n"
     ]
    }
   ],
   "source": [
    "epochs=50 # Number of epochs\n",
    "# Train the model. Save the model with the best validation accuracy. Only last epoch execution is shown\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(train_loader)\n",
    "    valid_loss, val_acc = validation(valid_loader)\n",
    "    #wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc, \"valid_loss\": valid_loss, \"val_acc\": val_acc})\n",
    "    \n",
    "    \n",
    "# Test the model\n",
    "test_loss, test_acc = test(test_loader)\n",
    "#wandb.log({\"test_loss\": test_loss, \"test_acc\": test_acc})\n",
    "# Mark the run as finished\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13344ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
