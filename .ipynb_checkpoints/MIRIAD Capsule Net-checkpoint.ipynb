{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d799bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 14:32:29.922830: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-11 14:32:29.941602: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-11 14:32:30.258813: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22497bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6c9b584f10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random seed (for reproducibility)\n",
    "seed = 1\n",
    "# set random seed for numpy\n",
    "np.random.seed(seed)\n",
    "# set random seed for pytorch\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed11e0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12a8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(os.getcwd(), 'Data/MIRIAD/miriad')\n",
    "config = {\n",
    "    'img_size': 128,\n",
    "    'depth' : 64,\n",
    "    'batch_size' : 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "551493e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images are: 708\n",
      "\n",
      "Shape inconsistancy found! (256, 256, 123) for /home/arindam/Alzheimer/Data/MIRIAD/miriad/miriad_192_AD_M/miriad_192_AD_M_06_MR_2/miriad_192_AD_M_06_MR_2.nii\n",
      "\n",
      "\n",
      "Remove shape insconsitent images.\n",
      "After Removing shape inconsistent images total number of images 707.\n",
      "Number of Alzheimer infected MRI scans: 464\n",
      "Number of Healthy MRI scans: 243\n"
     ]
    }
   ],
   "source": [
    "AD_path, HC_path = [], []\n",
    "for scan_dir in os.listdir(DATA_PATH):\n",
    "    scan_dir_path = os.path.join(DATA_PATH, scan_dir)\n",
    "    if os.path.isdir(scan_dir_path):\n",
    "        for visit in os.listdir(scan_dir_path):\n",
    "            scan_visit = os.path.join(scan_dir_path, visit)\n",
    "            for f in os.listdir(scan_visit):\n",
    "                f_path = os.path.join(scan_visit, f)\n",
    "                if f.endswith(\".nii\"):\n",
    "                    if 'AD' in f:\n",
    "                        AD_path.append(f_path)\n",
    "                    else:\n",
    "                        HC_path.append(f_path)\n",
    "                        \n",
    "        \n",
    "print(\"Total number of images are: {}\\n\".format(len(AD_path)+len(HC_path)))     \n",
    "incons = []\n",
    "for path in AD_path+HC_path:\n",
    "    scan = nib.load(path)\n",
    "    data = scan.get_fdata()\n",
    "    if data.shape != (256, 256, 124):\n",
    "        print('Shape inconsistancy found! {} for {}'.format(data.shape, path))\n",
    "        incons.append(path)\n",
    "        \n",
    "print(\"\\n\\nRemove shape insconsitent images.\")\n",
    "for path in incons:\n",
    "    if 'AD' in path:\n",
    "        AD_path.remove(path)\n",
    "    else:\n",
    "        HC_path.remove(path)\n",
    "        \n",
    "print(\"After Removing shape inconsistent images total number of images {}.\".format(len(AD_path)+len(HC_path)))\n",
    "print(\"Number of Alzheimer infected MRI scans: {}\".format(len(AD_path)))\n",
    "print(\"Number of Healthy MRI scans: {}\".format(len(HC_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42f1dd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is fine. No of images in train, val and test set is 348, 69 and 47 respectively.\n",
      "Everything is fine. No of images in train, val and test set is 182, 36 and 25 respectively.\n",
      "Total no of train, validation and test images are 530, 105 and 72 respectively.\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(path_list, train_split_ratio=0.75, val_split_ratio=0.15):\n",
    "    test_split_ratio = 1 - (train_split_ratio + val_split_ratio)\n",
    "    random.shuffle(path_list)\n",
    "    \n",
    "    train_image_paths = path_list[:int(train_split_ratio*len(path_list))]\n",
    "    valid_image_paths = path_list[int(train_split_ratio*len(path_list)):int((train_split_ratio+val_split_ratio)*len(path_list))]\n",
    "    test_image_paths = path_list[int((train_split_ratio+val_split_ratio)*len(path_list)):]\n",
    "    \n",
    "    if len(train_image_paths)+len(valid_image_paths)+len(test_image_paths)==len(path_list):\n",
    "        print(\"Everything is fine. No of images in train, val and test set is {}, {} and {} respectively.\".format(len(train_image_paths), len(valid_image_paths), len(test_image_paths)))\n",
    "    else:\n",
    "        print(\"Something wrong. Go and start debug!\")\n",
    "    \n",
    "    return train_image_paths, valid_image_paths, test_image_paths\n",
    "\n",
    "\n",
    "train_ad_image_paths, val_ad_image_paths, test_ad_image_paths = train_test_split(AD_path)\n",
    "train_hc_image_paths, val_hc_image_paths, test_hc_image_paths = train_test_split(HC_path)\n",
    "\n",
    "train_img_paths = train_ad_image_paths + train_hc_image_paths\n",
    "val_img_paths = val_ad_image_paths + val_hc_image_paths\n",
    "test_img_paths = test_ad_image_paths + test_hc_image_paths\n",
    "\n",
    "print(\"Total no of train, validation and test images are {}, {} and {} respectively.\".format(len(train_img_paths), len(val_img_paths), len(test_img_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fac9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessScan:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def read_nifti_file(self, filepath):\n",
    "        \"\"\"Read and load volume\"\"\"\n",
    "        scan = nib.load(filepath)\n",
    "        scan = scan.get_fdata()\n",
    "        return scan\n",
    "\n",
    "\n",
    "    def normalize(self, volume):\n",
    "        \"\"\"Normalize the volume\"\"\"\n",
    "        volume = (volume - np.min(volume)) / (np.max(volume) - np.min(volume))\n",
    "        return volume.astype('float32')\n",
    "\n",
    "\n",
    "    def resize_volume(self, img, desired_width=256, desired_height=256, desired_depth=64):\n",
    "        \"\"\"Resize the volume\"\"\"\n",
    "        width_factor = desired_width / img.shape[0]\n",
    "        height_factor = desired_height / img.shape[1]\n",
    "        depth_factor = desired_depth / img.shape[-1]\n",
    "\n",
    "        #img = ndimage.rotate(img, 90, reshape=False)\n",
    "        img = zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def process_scan(self, path):\n",
    "        \"\"\"Read and resize volume\"\"\"\n",
    "        volume = self.read_nifti_file(path)\n",
    "        volume = self.normalize(volume)\n",
    "        volume = self.resize_volume(volume, config['img_size'], config['img_size'], config['depth'])\n",
    "\n",
    "        return volume\n",
    "    \n",
    "    def label_extract(self, path):\n",
    "        \"\"\"Label Extraction\"\"\"\n",
    "        path = path.split('/')[-1]\n",
    "        if 'AD' in path:\n",
    "            return 1\n",
    "        elif 'HC' in path:\n",
    "            return 0\n",
    "    \n",
    "scan_process = ProcessScan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cbcaf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIRIADAlzheimerDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filepath = self.image_paths[idx]\n",
    "        image = scan_process.process_scan(image_filepath)\n",
    "        image = image[:, :, int(image.shape[-1]/2)]   # Get the middle slice\n",
    "        image = image.reshape(1, image.shape[0], image.shape[1]) # image.shape[-1])\n",
    "        \n",
    "        label = scan_process.label_extract(image_filepath)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "\n",
    "\n",
    "train_dataset = MIRIADAlzheimerDataset(train_img_paths)\n",
    "valid_dataset = MIRIADAlzheimerDataset(val_img_paths)\n",
    "test_dataset = MIRIADAlzheimerDataset(test_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cd3d3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of tensor for 50th image in train dataset:  (1, 128, 128)\n",
      "The label for 50th image in train dataset:  1\n"
     ]
    }
   ],
   "source": [
    "print('The shape of tensor for 50th image in train dataset: ',train_dataset[49][0].shape)\n",
    "print('The label for 50th image in train dataset: ',train_dataset[49][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f3e7b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of one batch ---> torch.Size([16, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=config['batch_size'], shuffle=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=config['batch_size'], shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=config['batch_size'], shuffle=False\n",
    ")\n",
    "\n",
    "#batch of image tensor\n",
    "print(\"Size of one batch ---> {}\".format(next(iter(train_loader))[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "517cb94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "#dataiter = iter(train_loader)\n",
    "#images, labels = next(dataiter)\n",
    "#images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "#fig = plt.figure(figsize=(25, 4))\n",
    "#for idx in np.arange(config['batch_size']):\n",
    "#    ax = fig.add_subplot(2, config['batch_size']//2, idx+1, xticks=[], yticks=[])\n",
    "#    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "#    # print out the correct label for each image\n",
    "#    # .item() gets the value contained in a Tensor\n",
    "#    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a093a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def squash(x, dim=-1):\n",
    "    squared_norm = (x ** 2).sum(dim=dim, keepdim=True)\n",
    "    scale = squared_norm / (1 + squared_norm)\n",
    "    return scale * x / (squared_norm.sqrt() + 1e-8)\n",
    "\n",
    "\n",
    "class PrimaryCaps(nn.Module):\n",
    "    \"\"\"Primary capsule layer.\"\"\"\n",
    "\n",
    "    def __init__(self, num_conv_units, in_channels, out_channels, kernel_size, stride):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        # Each conv unit stands for a single capsule.\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                              out_channels=out_channels * num_conv_units,\n",
    "                              kernel_size=kernel_size,\n",
    "                              stride=stride)\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape of x: (batch_size, in_channels, height, weight)\n",
    "        # Shape of out: out_capsules * (batch_size, out_channels, height, weight)\n",
    "        out = self.conv(x)\n",
    "        # Flatten out: (batch_size, out_capsules * height * weight, out_channels)\n",
    "        batch_size = out.shape[0]\n",
    "        return squash(out.contiguous().view(batch_size, -1, self.out_channels), dim=-1)\n",
    "\n",
    "\n",
    "class DigitCaps(nn.Module):\n",
    "    \"\"\"Digit capsule layer.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, in_caps, out_caps, out_dim, num_routing):\n",
    "        \"\"\"\n",
    "        Initialize the layer.\n",
    "\n",
    "        Args:\n",
    "            in_dim: \t\tDimensionality of each capsule vector.\n",
    "            in_caps: \t\tNumber of input capsules if digits layer.\n",
    "            out_caps: \t\tNumber of capsules in the capsule layer\n",
    "            out_dim: \t\tDimensionality, of the output capsule vector.\n",
    "            num_routing:\tNumber of iterations during routing algorithm\n",
    "        \"\"\"\n",
    "        super(DigitCaps, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.in_caps = in_caps\n",
    "        self.out_caps = out_caps\n",
    "        self.out_dim = out_dim\n",
    "        self.num_routing = num_routing\n",
    "        self.device = device\n",
    "        self.W = nn.Parameter(0.01 * torch.randn(1, out_caps, in_caps, out_dim, in_dim),\n",
    "                              requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # (batch_size, in_caps, in_dim) -> (batch_size, 1, in_caps, in_dim, 1)\n",
    "        x = x.unsqueeze(1).unsqueeze(4)\n",
    "        # W @ x =\n",
    "        # (1, out_caps, in_caps, out_dim, in_dim) @ (batch_size, 1, in_caps, in_dim, 1) =\n",
    "        # (batch_size, out_caps, in_caps, out_dims, 1)\n",
    "        u_hat = torch.matmul(self.W, x)\n",
    "        # (batch_size, out_caps, in_caps, out_dim)\n",
    "        u_hat = u_hat.squeeze(-1)\n",
    "        # detach u_hat during routing iterations to prevent gradients from flowing\n",
    "        temp_u_hat = u_hat.detach()\n",
    "\n",
    "        b = torch.zeros(batch_size, self.out_caps, self.in_caps, 1).to(self.device)\n",
    "\n",
    "        for route_iter in range(self.num_routing - 1):\n",
    "            # (batch_size, out_caps, in_caps, 1) -> Softmax along out_caps\n",
    "            c = b.softmax(dim=1)\n",
    "\n",
    "            # element-wise multiplication\n",
    "            # (batch_size, out_caps, in_caps, 1) * (batch_size, in_caps, out_caps, out_dim) ->\n",
    "            # (batch_size, out_caps, in_caps, out_dim) sum across in_caps ->\n",
    "            # (batch_size, out_caps, out_dim)\n",
    "            s = (c * temp_u_hat).sum(dim=2)\n",
    "            # apply \"squashing\" non-linearity along out_dim\n",
    "            v = squash(s)\n",
    "            # dot product agreement between the current output vj and the prediction uj|i\n",
    "            # (batch_size, out_caps, in_caps, out_dim) @ (batch_size, out_caps, out_dim, 1)\n",
    "            # -> (batch_size, out_caps, in_caps, 1)\n",
    "            uv = torch.matmul(temp_u_hat, v.unsqueeze(-1))\n",
    "            b += uv\n",
    "\n",
    "        # last iteration is done on the original u_hat, without the routing weights update\n",
    "        c = b.softmax(dim=1)\n",
    "        s = (c * u_hat).sum(dim=2)\n",
    "        # apply \"squashing\" non-linearity along out_dim\n",
    "        v = squash(s)\n",
    "\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "533fead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ch_conv1 = 1\n",
    "out_ch_conv1 = 128\n",
    "kernel_conv1 = 9\n",
    "\n",
    "in_ch_primary = out_ch_conv1\n",
    "out_ch_primary = 16\n",
    "num_conv_units_primary = 16\n",
    "\n",
    "digit_caps = out_ch_primary\n",
    "\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    \"\"\"Basic implementation of capsule network layer.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CapsNet, self).__init__()\n",
    "\n",
    "        # Conv2d layer\n",
    "        self.conv = nn.Conv2d(in_ch_conv1, out_ch_conv1, kernel_conv1, stride=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "        # Primary capsule\n",
    "        self.primary_caps = PrimaryCaps(num_conv_units=num_conv_units_primary,\n",
    "                                        in_channels=in_ch_primary,\n",
    "                                        out_channels=out_ch_primary,\n",
    "                                        kernel_size=kernel_conv1,\n",
    "                                        stride=2)\n",
    "\n",
    "        # Digit capsule\n",
    "        self.digit_caps = DigitCaps(in_dim=digit_caps,\n",
    "                                    in_caps=num_conv_units_primary * 11 * 11,\n",
    "                                    out_caps=2,\n",
    "                                    out_dim=256,\n",
    "                                    num_routing=3)\n",
    "        \n",
    "\n",
    "        # Reconstruction layer\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2* 256, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.Linear(256, 1024),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 128*128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.conv(x))\n",
    "        out = self.relu(self.maxpool(out))\n",
    "        out = self.primary_caps(out)\n",
    "        out = self.digit_caps(out)\n",
    "\n",
    "        # Shape of logits: (batch_size, out_capsules)\n",
    "        logits = torch.norm(out, dim=-1)\n",
    "        pred = torch.eye(2).to(device).index_select(dim=0, index=torch.argmax(logits, dim=1))\n",
    "\n",
    "        # Reconstruction\n",
    "        batch_size = out.shape[0]\n",
    "        reconstruction = self.decoder((out * pred.unsqueeze(2)).contiguous().view(batch_size, -1))\n",
    "\n",
    "        return logits , reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3a9cb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLoss(nn.Module):\n",
    "    \"\"\"Combine margin loss & reconstruction loss of capsule network.\"\"\"\n",
    "\n",
    "    def __init__(self, upper_bound=0.9, lower_bound=0.1, lmda=0.5):\n",
    "        super(CapsuleLoss, self).__init__()\n",
    "        self.upper = upper_bound\n",
    "        self.lower = lower_bound\n",
    "        self.lmda = lmda\n",
    "        self.reconstruction_loss_scalar = 5e-4\n",
    "        self.mse = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    def forward(self, images, labels, logits, reconstructions):\n",
    "        # Shape of left / right / labels: (batch_size, num_classes)\n",
    "        left = (self.upper - logits).relu() ** 2  # True negative\n",
    "        right = (logits - self.lower).relu() ** 2  # False positive\n",
    "        margin_loss = torch.sum(labels * left) + self.lmda * torch.sum((1 - labels) * right)\n",
    "\n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = self.mse(reconstructions.contiguous().view(images.shape), images)\n",
    "\n",
    "        # Combine two losses\n",
    "        return margin_loss + self.reconstruction_loss_scalar * reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28f75991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = CapsNet().to(device)\n",
    "criterion = CapsuleLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f00fda5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No of Parameters 35843584 \n",
      "Total no of trainable parameters 35843584\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Total No of Parameters {} \\nTotal no of trainable parameters {}\".format(total_params, total_trainable_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ddb224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10368\n",
      "128\n",
      "2654208\n",
      "256\n",
      "15859712\n",
      "524288\n",
      "1024\n",
      "16777216\n",
      "16384\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "091ed1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 1, loss: 26.686803817749023, accuracy: 0.5\n",
      "Epoch 1, batch 2, loss: 26.844375610351562, accuracy: 0.5625\n",
      "Epoch 1, batch 3, loss: 26.78057861328125, accuracy: 0.6041666666666666\n",
      "Epoch 1, batch 4, loss: 26.483919143676758, accuracy: 0.609375\n",
      "Epoch 1, batch 5, loss: 26.033588409423828, accuracy: 0.575\n",
      "Epoch 1, batch 6, loss: 25.49132537841797, accuracy: 0.5833333333333334\n",
      "Epoch 1, batch 7, loss: 24.90448760986328, accuracy: 0.5982142857142857\n",
      "Epoch 1, batch 8, loss: 24.246593475341797, accuracy: 0.6015625\n",
      "Epoch 1, batch 9, loss: 23.480009078979492, accuracy: 0.6180555555555556\n",
      "Epoch 1, batch 10, loss: 22.681442260742188, accuracy: 0.625\n",
      "Epoch 1, batch 11, loss: 21.871740341186523, accuracy: 0.6306818181818182\n",
      "Epoch 1, batch 12, loss: 21.068254470825195, accuracy: 0.6354166666666666\n",
      "Epoch 1, batch 13, loss: 20.29008674621582, accuracy: 0.6442307692307693\n",
      "Epoch 1, batch 14, loss: 19.56147575378418, accuracy: 0.65625\n",
      "Epoch 1, batch 15, loss: 18.882957458496094, accuracy: 0.6458333333333334\n",
      "Epoch 1, batch 16, loss: 18.26601791381836, accuracy: 0.65234375\n",
      "Epoch 1, batch 17, loss: 17.704917907714844, accuracy: 0.6654411764705882\n",
      "Epoch 1, batch 18, loss: 17.199493408203125, accuracy: 0.65625\n",
      "Epoch 1, batch 19, loss: 16.736839294433594, accuracy: 0.6480263157894737\n",
      "Epoch 1, batch 20, loss: 16.319623947143555, accuracy: 0.646875\n",
      "Epoch 1, batch 21, loss: 15.945290565490723, accuracy: 0.6398809523809523\n",
      "Epoch 1, batch 22, loss: 15.603285789489746, accuracy: 0.6505681818181818\n",
      "Epoch 1, batch 23, loss: 15.283330917358398, accuracy: 0.6521739130434783\n",
      "Epoch 1, batch 24, loss: 14.990968704223633, accuracy: 0.6484375\n",
      "Epoch 1, batch 25, loss: 14.71426010131836, accuracy: 0.65\n",
      "Epoch 1, batch 26, loss: 14.458539009094238, accuracy: 0.6490384615384616\n",
      "Epoch 1, batch 27, loss: 14.218528747558594, accuracy: 0.6504629629629629\n",
      "Epoch 1, batch 28, loss: 13.992448806762695, accuracy: 0.6450892857142857\n",
      "Epoch 1, batch 29, loss: 13.784299850463867, accuracy: 0.6551724137931034\n",
      "Epoch 1, batch 30, loss: 13.586913108825684, accuracy: 0.6604166666666667\n",
      "Epoch 1, batch 31, loss: 13.40434455871582, accuracy: 0.6612903225806451\n",
      "Epoch 1, batch 32, loss: 13.232948303222656, accuracy: 0.65625\n",
      "Epoch 1, batch 33, loss: 13.066986083984375, accuracy: 0.6571969696969697\n",
      "Epoch 1, batch 34, loss: 12.712038040161133, accuracy: 0.6547169811320754\n",
      "Total loss for epoch 1: 432.20928955078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arindam/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, batch 1, loss: 7.895901679992676, accuracy: 0.875\n",
      "Epoch 2, batch 2, loss: 7.820241928100586, accuracy: 0.71875\n",
      "Epoch 2, batch 3, loss: 7.824572563171387, accuracy: 0.7083333333333334\n",
      "Epoch 2, batch 4, loss: 7.768609523773193, accuracy: 0.75\n",
      "Epoch 2, batch 5, loss: 7.805556774139404, accuracy: 0.725\n",
      "Epoch 2, batch 6, loss: 7.797515869140625, accuracy: 0.71875\n",
      "Epoch 2, batch 7, loss: 7.791528224945068, accuracy: 0.6785714285714286\n",
      "Epoch 2, batch 8, loss: 7.771053791046143, accuracy: 0.6796875\n",
      "Epoch 2, batch 9, loss: 7.7586669921875, accuracy: 0.6805555555555556\n",
      "Epoch 2, batch 10, loss: 7.737706661224365, accuracy: 0.675\n",
      "Epoch 2, batch 11, loss: 7.720767974853516, accuracy: 0.6704545454545454\n",
      "Epoch 2, batch 12, loss: 7.728679656982422, accuracy: 0.6614583333333334\n",
      "Epoch 2, batch 13, loss: 7.723946571350098, accuracy: 0.6634615384615384\n",
      "Epoch 2, batch 14, loss: 7.722270965576172, accuracy: 0.6473214285714286\n",
      "Epoch 2, batch 15, loss: 7.713057994842529, accuracy: 0.65\n",
      "Epoch 2, batch 16, loss: 7.704967498779297, accuracy: 0.65234375\n",
      "Epoch 2, batch 17, loss: 7.690959453582764, accuracy: 0.6580882352941176\n",
      "Epoch 2, batch 18, loss: 7.693930149078369, accuracy: 0.65625\n",
      "Epoch 2, batch 19, loss: 7.686397552490234, accuracy: 0.6480263157894737\n",
      "Epoch 2, batch 20, loss: 7.680385589599609, accuracy: 0.64375\n",
      "Epoch 2, batch 21, loss: 7.676893711090088, accuracy: 0.6577380952380952\n",
      "Epoch 2, batch 22, loss: 7.668342113494873, accuracy: 0.6590909090909091\n",
      "Epoch 2, batch 23, loss: 7.666975975036621, accuracy: 0.6657608695652174\n",
      "Epoch 2, batch 24, loss: 7.6685709953308105, accuracy: 0.6666666666666666\n",
      "Epoch 2, batch 25, loss: 7.669925212860107, accuracy: 0.6625\n",
      "Epoch 2, batch 26, loss: 7.66502571105957, accuracy: 0.6610576923076923\n",
      "Epoch 2, batch 27, loss: 7.664253234863281, accuracy: 0.6620370370370371\n",
      "Epoch 2, batch 28, loss: 7.662731170654297, accuracy: 0.6584821428571429\n",
      "Epoch 2, batch 29, loss: 7.660330295562744, accuracy: 0.6551724137931034\n",
      "Epoch 2, batch 30, loss: 7.6556572914123535, accuracy: 0.65625\n",
      "Epoch 2, batch 31, loss: 7.653693199157715, accuracy: 0.6532258064516129\n",
      "Epoch 2, batch 32, loss: 7.653397083282471, accuracy: 0.65234375\n",
      "Epoch 2, batch 33, loss: 7.651308059692383, accuracy: 0.6553030303030303\n",
      "Epoch 2, batch 34, loss: 7.453845024108887, accuracy: 0.6566037735849056\n",
      "Total loss for epoch 2: 253.43072509765625\n",
      "Epoch 3, batch 1, loss: 7.550971031188965, accuracy: 0.75\n",
      "Epoch 3, batch 2, loss: 7.542962074279785, accuracy: 0.6875\n",
      "Epoch 3, batch 3, loss: 7.587608337402344, accuracy: 0.7083333333333334\n",
      "Epoch 3, batch 4, loss: 7.58962345123291, accuracy: 0.671875\n",
      "Epoch 3, batch 5, loss: 7.605834484100342, accuracy: 0.65\n",
      "Epoch 3, batch 6, loss: 7.606534481048584, accuracy: 0.6458333333333334\n",
      "Epoch 3, batch 7, loss: 7.6124043464660645, accuracy: 0.6517857142857143\n",
      "Epoch 3, batch 8, loss: 7.610556602478027, accuracy: 0.6484375\n",
      "Epoch 3, batch 9, loss: 7.608467102050781, accuracy: 0.6458333333333334\n",
      "Epoch 3, batch 10, loss: 7.5937700271606445, accuracy: 0.6375\n",
      "Epoch 3, batch 11, loss: 7.593305587768555, accuracy: 0.6534090909090909\n",
      "Epoch 3, batch 12, loss: 7.594417572021484, accuracy: 0.65625\n",
      "Epoch 3, batch 13, loss: 7.594882011413574, accuracy: 0.6490384615384616\n",
      "Epoch 3, batch 14, loss: 7.600277900695801, accuracy: 0.6428571428571429\n",
      "Epoch 3, batch 15, loss: 7.603943347930908, accuracy: 0.6458333333333334\n",
      "Epoch 3, batch 16, loss: 7.60483455657959, accuracy: 0.64453125\n",
      "Epoch 3, batch 17, loss: 7.606055736541748, accuracy: 0.6507352941176471\n",
      "Epoch 3, batch 18, loss: 7.60355806350708, accuracy: 0.65625\n",
      "Epoch 3, batch 19, loss: 7.596120834350586, accuracy: 0.6513157894736842\n",
      "Epoch 3, batch 20, loss: 7.595818996429443, accuracy: 0.65\n",
      "Epoch 3, batch 21, loss: 7.593835830688477, accuracy: 0.6517857142857143\n",
      "Epoch 3, batch 22, loss: 7.591538906097412, accuracy: 0.6448863636363636\n",
      "Epoch 3, batch 23, loss: 7.590842247009277, accuracy: 0.6494565217391305\n",
      "Epoch 3, batch 24, loss: 7.59564208984375, accuracy: 0.6536458333333334\n",
      "Epoch 3, batch 25, loss: 7.589996337890625, accuracy: 0.655\n",
      "Epoch 3, batch 26, loss: 7.591564178466797, accuracy: 0.6586538461538461\n",
      "Epoch 3, batch 27, loss: 7.590480804443359, accuracy: 0.6597222222222222\n",
      "Epoch 3, batch 28, loss: 7.590181350708008, accuracy: 0.6607142857142857\n",
      "Epoch 3, batch 29, loss: 7.5899577140808105, accuracy: 0.6551724137931034\n",
      "Epoch 3, batch 30, loss: 7.587732791900635, accuracy: 0.6583333333333333\n",
      "Epoch 3, batch 31, loss: 7.585044860839844, accuracy: 0.6532258064516129\n",
      "Epoch 3, batch 32, loss: 7.5895304679870605, accuracy: 0.65625\n",
      "Epoch 3, batch 33, loss: 7.5860466957092285, accuracy: 0.6553030303030303\n",
      "Epoch 3, batch 34, loss: 7.3907365798950195, accuracy: 0.6566037735849056\n",
      "Total loss for epoch 3: 251.28504943847656\n",
      "Epoch 4, batch 1, loss: 7.705648899078369, accuracy: 0.625\n",
      "Epoch 4, batch 2, loss: 7.632774353027344, accuracy: 0.65625\n",
      "Epoch 4, batch 3, loss: 7.5916852951049805, accuracy: 0.7083333333333334\n",
      "Epoch 4, batch 4, loss: 7.581423282623291, accuracy: 0.71875\n",
      "Epoch 4, batch 5, loss: 7.5751543045043945, accuracy: 0.7\n",
      "Epoch 4, batch 6, loss: 7.571776866912842, accuracy: 0.6770833333333334\n",
      "Epoch 4, batch 7, loss: 7.572977066040039, accuracy: 0.6696428571428571\n",
      "Epoch 4, batch 8, loss: 7.560569763183594, accuracy: 0.6640625\n",
      "Epoch 4, batch 9, loss: 7.571714878082275, accuracy: 0.6666666666666666\n",
      "Epoch 4, batch 10, loss: 7.565127849578857, accuracy: 0.675\n",
      "Epoch 4, batch 11, loss: 7.572866439819336, accuracy: 0.6590909090909091\n",
      "Epoch 4, batch 12, loss: 7.569287300109863, accuracy: 0.65625\n",
      "Epoch 4, batch 13, loss: 7.571002960205078, accuracy: 0.6442307692307693\n",
      "Epoch 4, batch 14, loss: 7.566629409790039, accuracy: 0.6517857142857143\n",
      "Epoch 4, batch 15, loss: 7.573970317840576, accuracy: 0.6458333333333334\n",
      "Epoch 4, batch 16, loss: 7.5714240074157715, accuracy: 0.65234375\n",
      "Epoch 4, batch 17, loss: 7.567875385284424, accuracy: 0.6654411764705882\n",
      "Epoch 4, batch 18, loss: 7.566503047943115, accuracy: 0.65625\n",
      "Epoch 4, batch 19, loss: 7.570499897003174, accuracy: 0.6546052631578947\n",
      "Epoch 4, batch 20, loss: 7.58086633682251, accuracy: 0.64375\n",
      "Epoch 4, batch 21, loss: 7.574032783508301, accuracy: 0.6517857142857143\n",
      "Epoch 4, batch 22, loss: 7.572571754455566, accuracy: 0.6505681818181818\n",
      "Epoch 4, batch 23, loss: 7.5749945640563965, accuracy: 0.6467391304347826\n",
      "Epoch 4, batch 24, loss: 7.573957443237305, accuracy: 0.6510416666666666\n",
      "Epoch 4, batch 25, loss: 7.575258731842041, accuracy: 0.65\n",
      "Epoch 4, batch 26, loss: 7.573707103729248, accuracy: 0.6466346153846154\n",
      "Epoch 4, batch 27, loss: 7.56896448135376, accuracy: 0.6527777777777778\n",
      "Epoch 4, batch 28, loss: 7.571091175079346, accuracy: 0.65625\n",
      "Epoch 4, batch 29, loss: 7.573163032531738, accuracy: 0.6551724137931034\n",
      "Epoch 4, batch 30, loss: 7.577239513397217, accuracy: 0.6541666666666667\n",
      "Epoch 4, batch 31, loss: 7.579209804534912, accuracy: 0.6532258064516129\n",
      "Epoch 4, batch 32, loss: 7.576388359069824, accuracy: 0.654296875\n",
      "Epoch 4, batch 33, loss: 7.576314926147461, accuracy: 0.6571969696969697\n",
      "Epoch 4, batch 34, loss: 7.381049156188965, accuracy: 0.6566037735849056\n",
      "Total loss for epoch 4: 250.95567321777344\n",
      "Epoch 5, batch 1, loss: 7.590865135192871, accuracy: 0.5625\n",
      "Epoch 5, batch 2, loss: 7.546992301940918, accuracy: 0.5\n",
      "Epoch 5, batch 3, loss: 7.5685553550720215, accuracy: 0.5625\n",
      "Epoch 5, batch 4, loss: 7.605348587036133, accuracy: 0.5625\n",
      "Epoch 5, batch 5, loss: 7.623409271240234, accuracy: 0.6125\n",
      "Epoch 5, batch 6, loss: 7.6093597412109375, accuracy: 0.6354166666666666\n",
      "Epoch 5, batch 7, loss: 7.584712982177734, accuracy: 0.6428571428571429\n",
      "Epoch 5, batch 8, loss: 7.5716352462768555, accuracy: 0.6484375\n",
      "Epoch 5, batch 9, loss: 7.576648235321045, accuracy: 0.6458333333333334\n",
      "Epoch 5, batch 10, loss: 7.565104961395264, accuracy: 0.65\n",
      "Epoch 5, batch 11, loss: 7.569140434265137, accuracy: 0.6761363636363636\n",
      "Epoch 5, batch 12, loss: 7.577662467956543, accuracy: 0.65625\n",
      "Epoch 5, batch 13, loss: 7.571894645690918, accuracy: 0.6682692307692307\n",
      "Epoch 5, batch 14, loss: 7.569691181182861, accuracy: 0.6651785714285714\n",
      "Epoch 5, batch 15, loss: 7.574577331542969, accuracy: 0.675\n",
      "Epoch 5, batch 16, loss: 7.563313007354736, accuracy: 0.6796875\n",
      "Epoch 5, batch 17, loss: 7.567128658294678, accuracy: 0.6764705882352942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, batch 18, loss: 7.567408084869385, accuracy: 0.6701388888888888\n",
      "Epoch 5, batch 19, loss: 7.565521240234375, accuracy: 0.6677631578947368\n",
      "Epoch 5, batch 20, loss: 7.567768096923828, accuracy: 0.66875\n",
      "Epoch 5, batch 21, loss: 7.568479537963867, accuracy: 0.6696428571428571\n",
      "Epoch 5, batch 22, loss: 7.567031383514404, accuracy: 0.6619318181818182\n",
      "Epoch 5, batch 23, loss: 7.568179607391357, accuracy: 0.6657608695652174\n",
      "Epoch 5, batch 24, loss: 7.568295955657959, accuracy: 0.6666666666666666\n",
      "Epoch 5, batch 25, loss: 7.5697021484375, accuracy: 0.655\n",
      "Epoch 5, batch 26, loss: 7.570771217346191, accuracy: 0.6538461538461539\n",
      "Epoch 5, batch 27, loss: 7.57048225402832, accuracy: 0.6504629629629629\n",
      "Epoch 5, batch 28, loss: 7.571534633636475, accuracy: 0.6540178571428571\n",
      "Epoch 5, batch 29, loss: 7.570863246917725, accuracy: 0.6530172413793104\n",
      "Epoch 5, batch 30, loss: 7.572089195251465, accuracy: 0.65\n",
      "Epoch 5, batch 31, loss: 7.569310665130615, accuracy: 0.6451612903225806\n",
      "Epoch 5, batch 32, loss: 7.573505878448486, accuracy: 0.6484375\n",
      "Epoch 5, batch 33, loss: 7.572884559631348, accuracy: 0.6553030303030303\n",
      "Epoch 5, batch 34, loss: 7.379158973693848, accuracy: 0.6566037735849056\n",
      "Total loss for epoch 5: 250.8914031982422\n",
      "Epoch 6, batch 1, loss: 7.596120834350586, accuracy: 0.6875\n",
      "Epoch 6, batch 2, loss: 7.593195915222168, accuracy: 0.625\n",
      "Epoch 6, batch 3, loss: 7.596919059753418, accuracy: 0.625\n",
      "Epoch 6, batch 4, loss: 7.594076156616211, accuracy: 0.6875\n",
      "Epoch 6, batch 5, loss: 7.597812175750732, accuracy: 0.6875\n",
      "Epoch 6, batch 6, loss: 7.574798107147217, accuracy: 0.65625\n",
      "Epoch 6, batch 7, loss: 7.583718776702881, accuracy: 0.625\n",
      "Epoch 6, batch 8, loss: 7.58203125, accuracy: 0.625\n",
      "Epoch 6, batch 9, loss: 7.5960540771484375, accuracy: 0.6388888888888888\n",
      "Epoch 6, batch 10, loss: 7.601452827453613, accuracy: 0.65\n",
      "Epoch 6, batch 11, loss: 7.60589599609375, accuracy: 0.6647727272727273\n",
      "Epoch 6, batch 12, loss: 7.590136528015137, accuracy: 0.6666666666666666\n",
      "Epoch 6, batch 13, loss: 7.593423843383789, accuracy: 0.6682692307692307\n",
      "Epoch 6, batch 14, loss: 7.596268653869629, accuracy: 0.6741071428571429\n",
      "Epoch 6, batch 15, loss: 7.594029426574707, accuracy: 0.6791666666666667\n",
      "Epoch 6, batch 16, loss: 7.587674140930176, accuracy: 0.65234375\n",
      "Epoch 6, batch 17, loss: 7.581519603729248, accuracy: 0.6654411764705882\n",
      "Epoch 6, batch 18, loss: 7.581324100494385, accuracy: 0.6527777777777778\n",
      "Epoch 6, batch 19, loss: 7.578609943389893, accuracy: 0.6611842105263158\n",
      "Epoch 6, batch 20, loss: 7.582805633544922, accuracy: 0.659375\n",
      "Epoch 6, batch 21, loss: 7.582601547241211, accuracy: 0.6607142857142857\n",
      "Epoch 6, batch 22, loss: 7.581264495849609, accuracy: 0.6647727272727273\n",
      "Epoch 6, batch 23, loss: 7.578963279724121, accuracy: 0.6657608695652174\n",
      "Epoch 6, batch 24, loss: 7.577315807342529, accuracy: 0.6666666666666666\n",
      "Epoch 6, batch 25, loss: 7.576033592224121, accuracy: 0.6625\n",
      "Epoch 6, batch 26, loss: 7.573222637176514, accuracy: 0.6610576923076923\n",
      "Epoch 6, batch 27, loss: 7.574207305908203, accuracy: 0.6620370370370371\n",
      "Epoch 6, batch 28, loss: 7.573732376098633, accuracy: 0.6674107142857143\n",
      "Epoch 6, batch 29, loss: 7.574300765991211, accuracy: 0.665948275862069\n",
      "Epoch 6, batch 30, loss: 7.572387218475342, accuracy: 0.6583333333333333\n",
      "Epoch 6, batch 31, loss: 7.5771403312683105, accuracy: 0.6512096774193549\n",
      "Epoch 6, batch 32, loss: 7.575695514678955, accuracy: 0.650390625\n",
      "Epoch 6, batch 33, loss: 7.574703693389893, accuracy: 0.6571969696969697\n",
      "Epoch 6, batch 34, loss: 7.380241870880127, accuracy: 0.6566037735849056\n",
      "Total loss for epoch 6: 250.92822265625\n",
      "Epoch 7, batch 1, loss: 7.564426422119141, accuracy: 0.6875\n",
      "Epoch 7, batch 2, loss: 7.511645317077637, accuracy: 0.6875\n",
      "Epoch 7, batch 3, loss: 7.561239242553711, accuracy: 0.6458333333333334\n",
      "Epoch 7, batch 4, loss: 7.550123691558838, accuracy: 0.578125\n",
      "Epoch 7, batch 5, loss: 7.54932165145874, accuracy: 0.575\n",
      "Epoch 7, batch 6, loss: 7.570734977722168, accuracy: 0.6145833333333334\n",
      "Epoch 7, batch 7, loss: 7.56955099105835, accuracy: 0.6339285714285714\n",
      "Epoch 7, batch 8, loss: 7.562817573547363, accuracy: 0.6484375\n",
      "Epoch 7, batch 9, loss: 7.564262390136719, accuracy: 0.6597222222222222\n",
      "Epoch 7, batch 10, loss: 7.559609889984131, accuracy: 0.65625\n",
      "Epoch 7, batch 11, loss: 7.554641246795654, accuracy: 0.6477272727272727\n",
      "Epoch 7, batch 12, loss: 7.5381059646606445, accuracy: 0.6510416666666666\n",
      "Epoch 7, batch 13, loss: 7.53562593460083, accuracy: 0.6442307692307693\n",
      "Epoch 7, batch 14, loss: 7.53772497177124, accuracy: 0.6517857142857143\n",
      "Epoch 7, batch 15, loss: 7.544587135314941, accuracy: 0.6583333333333333\n",
      "Epoch 7, batch 16, loss: 7.544353485107422, accuracy: 0.6484375\n",
      "Epoch 7, batch 17, loss: 7.542674541473389, accuracy: 0.6507352941176471\n",
      "Epoch 7, batch 18, loss: 7.547234535217285, accuracy: 0.6527777777777778\n",
      "Epoch 7, batch 19, loss: 7.551580429077148, accuracy: 0.6447368421052632\n",
      "Epoch 7, batch 20, loss: 7.549198150634766, accuracy: 0.64375\n",
      "Epoch 7, batch 21, loss: 7.551994800567627, accuracy: 0.6488095238095238\n",
      "Epoch 7, batch 22, loss: 7.551730155944824, accuracy: 0.6505681818181818\n",
      "Epoch 7, batch 23, loss: 7.550618648529053, accuracy: 0.6494565217391305\n",
      "Epoch 7, batch 24, loss: 7.556251049041748, accuracy: 0.6380208333333334\n",
      "Epoch 7, batch 25, loss: 7.5604705810546875, accuracy: 0.645\n",
      "Epoch 7, batch 26, loss: 7.565457344055176, accuracy: 0.6514423076923077\n",
      "Epoch 7, batch 27, loss: 7.563937664031982, accuracy: 0.6527777777777778\n",
      "Epoch 7, batch 28, loss: 7.562953948974609, accuracy: 0.6540178571428571\n",
      "Epoch 7, batch 29, loss: 7.564451217651367, accuracy: 0.6530172413793104\n",
      "Epoch 7, batch 30, loss: 7.567774295806885, accuracy: 0.6520833333333333\n",
      "Epoch 7, batch 31, loss: 7.565430164337158, accuracy: 0.655241935483871\n",
      "Epoch 7, batch 32, loss: 7.56712532043457, accuracy: 0.65625\n",
      "Epoch 7, batch 33, loss: 7.568708896636963, accuracy: 0.6553030303030303\n",
      "Epoch 7, batch 34, loss: 7.375149250030518, accuracy: 0.6566037735849056\n",
      "Total loss for epoch 7: 250.7550811767578\n",
      "Epoch 8, batch 1, loss: 7.539127349853516, accuracy: 0.6875\n",
      "Epoch 8, batch 2, loss: 7.590473175048828, accuracy: 0.65625\n",
      "Epoch 8, batch 3, loss: 7.567531585693359, accuracy: 0.7083333333333334\n",
      "Epoch 8, batch 4, loss: 7.561671733856201, accuracy: 0.640625\n",
      "Epoch 8, batch 5, loss: 7.563145637512207, accuracy: 0.6625\n",
      "Epoch 8, batch 6, loss: 7.574779033660889, accuracy: 0.65625\n",
      "Epoch 8, batch 7, loss: 7.567068099975586, accuracy: 0.6875\n",
      "Epoch 8, batch 8, loss: 7.5817036628723145, accuracy: 0.6640625\n",
      "Epoch 8, batch 9, loss: 7.572699546813965, accuracy: 0.6319444444444444\n",
      "Epoch 8, batch 10, loss: 7.5725884437561035, accuracy: 0.625\n",
      "Epoch 8, batch 11, loss: 7.570315361022949, accuracy: 0.6306818181818182\n",
      "Epoch 8, batch 12, loss: 7.573808193206787, accuracy: 0.640625\n",
      "Epoch 8, batch 13, loss: 7.580265998840332, accuracy: 0.6442307692307693\n",
      "Epoch 8, batch 14, loss: 7.584625244140625, accuracy: 0.6339285714285714\n",
      "Epoch 8, batch 15, loss: 7.582961082458496, accuracy: 0.6458333333333334\n",
      "Epoch 8, batch 16, loss: 7.57884407043457, accuracy: 0.63671875\n",
      "Epoch 8, batch 17, loss: 7.572339057922363, accuracy: 0.6397058823529411\n",
      "Epoch 8, batch 18, loss: 7.571687698364258, accuracy: 0.6493055555555556\n",
      "Epoch 8, batch 19, loss: 7.571356773376465, accuracy: 0.6546052631578947\n",
      "Epoch 8, batch 20, loss: 7.573128700256348, accuracy: 0.65\n",
      "Epoch 8, batch 21, loss: 7.58038330078125, accuracy: 0.6458333333333334\n",
      "Epoch 8, batch 22, loss: 7.577412128448486, accuracy: 0.6505681818181818\n",
      "Epoch 8, batch 23, loss: 7.574325084686279, accuracy: 0.654891304347826\n",
      "Epoch 8, batch 24, loss: 7.576964855194092, accuracy: 0.6536458333333334\n",
      "Epoch 8, batch 25, loss: 7.577156066894531, accuracy: 0.66\n",
      "Epoch 8, batch 26, loss: 7.572231292724609, accuracy: 0.65625\n",
      "Epoch 8, batch 27, loss: 7.565735340118408, accuracy: 0.6504629629629629\n",
      "Epoch 8, batch 28, loss: 7.564730167388916, accuracy: 0.6517857142857143\n",
      "Epoch 8, batch 29, loss: 7.565900802612305, accuracy: 0.6508620689655172\n",
      "Epoch 8, batch 30, loss: 7.5645551681518555, accuracy: 0.6541666666666667\n",
      "Epoch 8, batch 31, loss: 7.56486701965332, accuracy: 0.6512096774193549\n",
      "Epoch 8, batch 32, loss: 7.565257549285889, accuracy: 0.65234375\n",
      "Epoch 8, batch 33, loss: 7.563708305358887, accuracy: 0.6553030303030303\n",
      "Epoch 8, batch 34, loss: 7.369052886962891, accuracy: 0.6566037735849056\n",
      "Total loss for epoch 8: 250.54779052734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, batch 1, loss: 7.648087501525879, accuracy: 0.5625\n",
      "Epoch 9, batch 2, loss: 7.615144729614258, accuracy: 0.625\n",
      "Epoch 9, batch 3, loss: 7.613123893737793, accuracy: 0.6458333333333334\n",
      "Epoch 9, batch 4, loss: 7.594395637512207, accuracy: 0.59375\n",
      "Epoch 9, batch 5, loss: 7.588812351226807, accuracy: 0.6125\n",
      "Epoch 9, batch 6, loss: 7.5758819580078125, accuracy: 0.6458333333333334\n",
      "Epoch 9, batch 7, loss: 7.5697712898254395, accuracy: 0.6428571428571429\n",
      "Epoch 9, batch 8, loss: 7.554168224334717, accuracy: 0.65625\n",
      "Epoch 9, batch 9, loss: 7.563313007354736, accuracy: 0.6666666666666666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m batch_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m correct, total, total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mMIRIADAlzheimerDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     10\u001b[0m     image_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]\n\u001b[0;32m---> 11\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mscan_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_filepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     image \u001b[38;5;241m=\u001b[39m image[:, :, \u001b[38;5;28mint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)]   \u001b[38;5;66;03m# Get the middle slice\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;66;03m# image.shape[-1])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mProcessScan.process_scan\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_scan\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read and resize volume\"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     volume \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_nifti_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     volume \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(volume)\n\u001b[1;32m     33\u001b[0m     volume \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize_volume(volume, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_size\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_size\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mProcessScan.read_nifti_file\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_nifti_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, filepath):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read and load volume\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     scan \u001b[38;5;241m=\u001b[39m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     scan \u001b[38;5;241m=\u001b[39m scan\u001b[38;5;241m.\u001b[39mget_fdata()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scan\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/loadsave.py:110\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     is_valid, sniff \u001b[38;5;241m=\u001b[39m image_klass\u001b[38;5;241m.\u001b[39mpath_maybe_image(filename, sniff)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid:\n\u001b[0;32m--> 110\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mimage_klass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[1;32m    113\u001b[0m matches, msg \u001b[38;5;241m=\u001b[39m _signature_matches_extension(filename)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/dataobj_images.py:503\u001b[0m, in \u001b[0;36mDataobjImage.from_filename\u001b[0;34m(klass, filename, mmap, keep_file_open)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap should be one of \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mTrue, False, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    502\u001b[0m file_map \u001b[38;5;241m=\u001b[39m klass\u001b[38;5;241m.\u001b[39mfilespec_to_file_map(filename)\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_file_open\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_file_open\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/analyze.py:960\u001b[0m, in \u001b[0;36mAnalyzeImage.from_file_map\u001b[0;34m(klass, file_map, mmap, keep_file_open)\u001b[0m\n\u001b[1;32m    958\u001b[0m hdr_fh, img_fh \u001b[38;5;241m=\u001b[39m klass\u001b[38;5;241m.\u001b[39m_get_fileholders(file_map)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hdr_fh\u001b[38;5;241m.\u001b[39mget_prepare_fileobj(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hdrf:\n\u001b[0;32m--> 960\u001b[0m     header \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheader_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_fileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdrf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m hdr_copy \u001b[38;5;241m=\u001b[39m header\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    962\u001b[0m imgf \u001b[38;5;241m=\u001b[39m img_fh\u001b[38;5;241m.\u001b[39mfileobj\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/nifti1.py:707\u001b[0m, in \u001b[0;36mNifti1Header.from_fileobj\u001b[0;34m(klass, fileobj, endianness, check)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_fileobj\u001b[39m(klass, fileobj, endianness\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    706\u001b[0m     raw_str \u001b[38;5;241m=\u001b[39m fileobj\u001b[38;5;241m.\u001b[39mread(klass\u001b[38;5;241m.\u001b[39mtemplate_dtype\u001b[38;5;241m.\u001b[39mitemsize)\n\u001b[0;32m--> 707\u001b[0m     hdr \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendianness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;66;03m# Read next 4 bytes to see if we have extensions.  The nifti standard\u001b[39;00m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;66;03m# has this as a 4 byte string; if the first value is not zero, then we\u001b[39;00m\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;66;03m# have extensions.\u001b[39;00m\n\u001b[1;32m    711\u001b[0m     extension_status \u001b[38;5;241m=\u001b[39m fileobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/nifti1.py:694\u001b[0m, in \u001b[0;36mNifti1Header.__init__\u001b[0;34m(self, binaryblock, endianness, check, extensions)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, binaryblock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, endianness\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extensions\u001b[38;5;241m=\u001b[39m()):\n\u001b[1;32m    693\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize header from binary data block and extensions\"\"\"\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbinaryblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendianness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexts_klass(extensions)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/analyze.py:252\u001b[0m, in \u001b[0;36mAnalyzeHeader.__init__\u001b[0;34m(self, binaryblock, endianness, check)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, binaryblock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, endianness\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize header from binary data block\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbinaryblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendianness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/wrapstruct.py:169\u001b[0m, in \u001b[0;36mWrapStruct.__init__\u001b[0;34m(self, binaryblock, endianness, check)\u001b[0m\n\u001b[1;32m    167\u001b[0m     dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_dtype\u001b[38;5;241m.\u001b[39mnewbyteorder(endianness)\n\u001b[1;32m    168\u001b[0m     wstr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray(shape\u001b[38;5;241m=\u001b[39m(), dtype\u001b[38;5;241m=\u001b[39mdt, buffer\u001b[38;5;241m=\u001b[39mbinaryblock)\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structarr \u001b[38;5;241m=\u001b[39m \u001b[43mwstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_fix()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "EPOCHES = 50\n",
    "model.train()\n",
    "for ep in range(EPOCHES):\n",
    "    batch_id = 1\n",
    "    correct, total, total_loss = 0, 0, 0.\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        labels = torch.eye(2).index_select(dim=0, index=labels).to(device)\n",
    "        logits, reconstruction = model(images)\n",
    "        #logits = model(images)\n",
    "\n",
    "        # Compute loss & accuracy\n",
    "        loss = criterion(images, labels, logits, reconstruction)\n",
    "        correct += torch.sum(\n",
    "            torch.argmax(logits, dim=1) == torch.argmax(labels, dim=1)).item()\n",
    "        total += len(labels)\n",
    "        accuracy = correct / total\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch {}, batch {}, loss: {}, accuracy: {}'.format(ep + 1,\n",
    "                                                                  batch_id,\n",
    "                                                                  total_loss / batch_id,\n",
    "                                                                  accuracy))\n",
    "        batch_id += 1\n",
    "    scheduler.step(ep)\n",
    "    print('Total loss for epoch {}: {}'.format(ep + 1, total_loss))\n",
    "    model.eval()\n",
    "    vcorrect, vtotal = 0, 0\n",
    "    for vimages, vlabels in val_loader:\n",
    "        # Add channels = 1\n",
    "        vimages = vimages.to(device)\n",
    "        # Categogrical encoding\n",
    "        vlabels = torch.eye(10).index_select(dim=0, index=vlabels).to(device)\n",
    "        vlogits, vreconstructions = model(vimages)\n",
    "        vpred_labels = torch.argmax(logits, dim=1)\n",
    "        vcorrect += torch.sum(vpred_labels == torch.argmax(vlabels, dim=1)).item()\n",
    "        vtotal += len(vlabels)\n",
    "    print('Validation Accuracy: {}\\n'.format(vcorrect / vtotal))\n",
    "    model.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e637414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "for images, labels in test_loader:\n",
    "    # Add channels = 1\n",
    "    images = images.to(device)\n",
    "    # Categogrical encoding\n",
    "    labels = torch.eye(10).index_select(dim=0, index=labels).to(device)\n",
    "    logits, reconstructions = model(images)\n",
    "    pred_labels = torch.argmax(logits, dim=1)\n",
    "    correct += torch.sum(pred_labels == torch.argmax(labels, dim=1)).item()\n",
    "    total += len(labels)\n",
    "print('Accuracy: {}'.format(correct / total))\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), './model/capsnet_ep{}_acc{}.pt'.format(EPOCHES, correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a65391e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
