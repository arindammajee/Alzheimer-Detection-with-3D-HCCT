{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529dcecd-9176-45ab-bb9f-6aad1af86a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdc743d3e30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from einops import rearrange\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "from sklearn import neighbors\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462bd1cb-e34e-4b51-a4c7-7e53e1f8bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ViT Implementation ðŸ”¥\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv =  nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.act = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.maxpool(self.act((self.bn(self.conv(x)))))\n",
    "\n",
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "\n",
    "    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert the image into patches and then project them into a vector space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        # Calculate the number of patches from the image size and patch size\n",
    "        # self.num_patches = (self.image_size // self.patch_size) ** 3\n",
    "        # Create a projection layer to convert the image into patches\n",
    "        # The layer projects each patch into a vector of size hidden_size\n",
    "        self.conv_1 = ConvBlock(self.num_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_2 = ConvBlock(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_3 = ConvBlock(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_4 = ConvBlock(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_5 = ConvBlock(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.num_patches = 512\n",
    "        #self.projection = nn.Conv3d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, num_channels, image_depth, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "        x = self.conv_4(x)\n",
    "        x = self.conv_5(x)\n",
    "        #x = self.projection(x)\n",
    "        x = rearrange(x, 'b c d w h -> b c (d w h)')\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine the patch embeddings with the class token and position embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        # Create a learnable [CLS] token\n",
    "        # Similar to BERT, the [CLS] token is added to the beginning of the input sequence\n",
    "        # and is used to classify the entire sequence\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
    "        # Create position embeddings for the [CLS] token and the patch embeddings\n",
    "        # Add 1 to the sequence length for the [CLS] token\n",
    "        self.position_embeddings = \\\n",
    "            nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        # Expand the [CLS] token to the batch size\n",
    "        # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # Concatenate the [CLS] token to the beginning of the input sequence\n",
    "        # This results in a sequence length of (num_patches + 1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single attention head.\n",
    "    This module is used in the MultiHeadAttention module.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "        # Create the query, key, and value projection layers\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project the input into query, key, and value\n",
    "        # The same input is used to generate the query, key, and value,\n",
    "        # so it's usually called self-attention.\n",
    "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, attention_head_size)\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        # Calculate the attention scores\n",
    "        # softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        # Calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module.\n",
    "    This module is used in the TransformerEncoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a list of attention heads\n",
    "        self.heads = nn.ModuleList([])\n",
    "        for _ in range(self.num_attention_heads):\n",
    "            head = AttentionHead(\n",
    "                self.hidden_size,\n",
    "                self.attention_head_size,\n",
    "                config[\"attention_probs_dropout_prob\"],\n",
    "                self.qkv_bias\n",
    "            )\n",
    "            self.heads.append(head)\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the attention output for each attention head\n",
    "        attention_outputs = [head(x) for head in self.heads]\n",
    "        # Concatenate the attention outputs from each attention head\n",
    "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
    "        # Project the concatenated attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
    "            return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class FasterMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module with some optimizations.\n",
    "    All the heads are processed simultaneously with merged query, key, and value projections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a linear layer to project the query, key, and value\n",
    "        self.qkv_projection = nn.Linear(self.hidden_size, self.all_head_size * 3, bias=self.qkv_bias)\n",
    "        self.attn_dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Project the query, key, and value\n",
    "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, all_head_size * 3)\n",
    "        qkv = self.qkv_projection(x)\n",
    "        # Split the projected query, key, and value into query, key, and value\n",
    "        # (batch_size, sequence_length, all_head_size * 3) -> (batch_size, sequence_length, all_head_size)\n",
    "        query, key, value = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Resize the query, key, and value to (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
    "        batch_size, sequence_length, _ = query.size()\n",
    "        query = query.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        key = key.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        value = value.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        # Calculate the attention scores\n",
    "        # softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        # Calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        # Resize the attention output\n",
    "        # from (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
    "        # To (batch_size, sequence_length, all_head_size)\n",
    "        attention_output = attention_output.transpose(1, 2) \\\n",
    "                                           .contiguous() \\\n",
    "                                           .view(batch_size, sequence_length, self.all_head_size)\n",
    "        # Project the attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-layer perceptron module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
    "        self.activation = NewGELUActivation()\n",
    "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.use_faster_attention = config.get(\"use_faster_attention\", False)\n",
    "        if self.use_faster_attention:\n",
    "            self.attention = FasterMultiHeadAttention(config)\n",
    "        else:\n",
    "            self.attention = MultiHeadAttention(config)\n",
    "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Self-attention\n",
    "        attention_output, attention_probs = \\\n",
    "            self.attention(self.layernorm_1(x), output_attentions=output_attentions)\n",
    "        # Skip connection\n",
    "        x = x + attention_output\n",
    "        # Feed-forward network\n",
    "        mlp_output = self.mlp(self.layernorm_2(x))\n",
    "        # Skip connection\n",
    "        x = x + mlp_output\n",
    "        # Return the transformer block's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, attention_probs)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The transformer encoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Create a list of transformer blocks\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config[\"num_hidden_layers\"]):\n",
    "            block = Block(config)\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the transformer block's output for each block\n",
    "        all_attentions = []\n",
    "        for block in self.blocks:\n",
    "            x, attention_probs = block(x, output_attentions=output_attentions)\n",
    "            if output_attentions:\n",
    "                all_attentions.append(attention_probs)\n",
    "        # Return the encoder's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, all_attentions)\n",
    "\n",
    "\n",
    "class ViTForClassfication(nn.Module):\n",
    "    \"\"\"\n",
    "    The ViT model for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        # Create the embedding module\n",
    "        self.embedding = Embeddings(config)\n",
    "        # Create the transformer encoder module\n",
    "        self.encoder = Encoder(config)\n",
    "        # Create a linear layer to project the encoder's output to the number of classes\n",
    "        self.attention_pool = nn.Linear(self.hidden_size, 1)\n",
    "        self.classifier = nn.Linear(2*self.hidden_size, self.num_classes)\n",
    "        # Initialize the weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the embedding output\n",
    "        embedding_output = self.embedding(x)\n",
    "        # Calculate the encoder's output\n",
    "        encoder_output, all_attentions = self.encoder(embedding_output, output_attentions=output_attentions)\n",
    "        # Calculate the logits, take the [CLS] token's output as features for classification\n",
    "        cls_logits, activation_logits = encoder_output[:, 0, :], encoder_output[:, 1:, :]\n",
    "        activation_logits = torch.matmul(nn.functional.softmax(self.attention_pool(activation_logits), dim=1).transpose(-1, -2), activation_logits).squeeze(-2)\n",
    "        logits = torch.cat((cls_logits, activation_logits), dim=1)\n",
    "        logits = self.classifier(logits)\n",
    "        # Return the logits and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (logits, None)\n",
    "        else:\n",
    "            return (logits, all_attentions)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, Embeddings):\n",
    "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.position_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.position_embeddings.dtype)\n",
    "\n",
    "            module.cls_token.data = nn.init.trunc_normal_(\n",
    "                module.cls_token.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.cls_token.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbad8fce-3b37-4c67-a27f-a967af29496c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#@title Prepare Data ðŸ“Š\n",
    "# Import libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchio as tio\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class FolderDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    "        self.image_paths = glob.glob(f'{self.folder}/*/*.pt')\n",
    "        self.labels = {\n",
    "            'CN' : 0,\n",
    "            'MCI' : 1,\n",
    "            'AD' : 2\n",
    "        }\n",
    "        self.transform = False #tio.transforms.Compose(\n",
    "            #[tio.transforms.RandomAffine(degrees=5)\n",
    "            #tio.transforms.RandomBiasField()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __label_dist__(self):\n",
    "        cn,mci, ad = 0, 0, 0\n",
    "        for path in self.image_paths:\n",
    "            if self.__label_extract__(path) == 0:\n",
    "                cn += 1\n",
    "            elif self.__label_extract__(path) == 1:\n",
    "                mci += 1\n",
    "            elif self.__label_extract__(path) == 2:\n",
    "                ad += 1\n",
    "        \n",
    "        return {'CN': cn, 'MCI': mci, 'AD': ad}\n",
    "    \n",
    "    def __label_extract__(self, path):\n",
    "        if 'CN' in path:\n",
    "            return 0\n",
    "        elif 'MCI' in path:\n",
    "            return 1\n",
    "        elif 'AD' in path:\n",
    "            return 2\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tensor, label = torch.load(self.image_paths[idx]), self.__label_extract__(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            tensor = self.transform(tensor)\n",
    "        \n",
    "        return tensor, label\n",
    "    \n",
    "def prepare_data(batch_size=4, num_workers=2, train_sample_size=None, test_sample_size=None):\n",
    "    train_dataset = FolderDataset(folder='/home/admin1/Arindam/Alzheimer/ViT/data/3D (part II)/Train')\n",
    "    val_dataset = FolderDataset(folder='/home/admin1/Arindam/Alzheimer/ViT/data/3D (part II)/Val')\n",
    "    test_dataset = FolderDataset(folder='/home/admin1/Arindam/Alzheimer/ViT/data/3D (part II)/Test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    classes = ('CN', 'MCI', 'AD')\n",
    "    class_dist = {\n",
    "        'Train': train_dataset.__label_dist__(),\n",
    "        'Val': val_dataset.__label_dist__(),\n",
    "        'Test': test_dataset.__label_dist__()\n",
    "    }\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader, class_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7460d52f-1f9b-489b-95af-6e3a92a3af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size' : 3,\n",
    "    'image_size' : 192,\n",
    "    'patch_size' : 6,\n",
    "    \"hidden_size\": 216,\n",
    "    \"num_hidden_layers\": None,\n",
    "    \"num_attention_heads\": 8,\n",
    "    \"intermediate_size\": 3 * 216, # 3 * hidden_size\n",
    "    \"hidden_dropout_prob\": 0.25,\n",
    "    \"attention_probs_dropout_prob\": 0.25,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"num_classes\": 3, # num_classes\n",
    "    \"num_channels\": 1,\n",
    "    \"qkv_bias\": True,\n",
    "    \"use_faster_attention\": True,\n",
    "    'save_model_every' : 0,\n",
    "    'exp_name' : 'HCCT Models Evaluation'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2f678e-0468-4b81-ae76-2f35434d45b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in train, val and test set are, 1526, 326, 330\n",
      "\t\tCN\tMCI\tAD\n",
      "Train\t: \t523\t686\t317\n",
      "Val\t: \t112\t147\t67\n",
      "Test\t: \t113\t148\t69\n",
      "\n",
      "Shape of images and labels of a signle batch is torch.Size([3, 1, 192, 192, 192]) and torch.Size([3]) respectively.\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader, class_dist = prepare_data()\n",
    "\n",
    "print(f\"Total number of images in train, val and test set are, {len(train_loader.dataset)}, {len(valid_loader.dataset)}, {len(test_loader.dataset)}\")\n",
    "\n",
    "assert len(train_loader.dataset)==1526\n",
    "assert len(valid_loader.dataset)==326\n",
    "assert len(test_loader.dataset)==330\n",
    "\n",
    "print(f\"\\t\\tCN\\tMCI\\tAD\")\n",
    "for key in class_dist.keys():\n",
    "    print(f\"{key}\\t: \\t{class_dist[key]['CN']}\\t{class_dist[key]['MCI']}\\t{class_dist[key]['AD']}\")\n",
    "# Check a sample batch size\n",
    "idx =0\n",
    "for data in train_loader:\n",
    "    images, labels = data\n",
    "    print(f\"\\nShape of images and labels of a signle batch is {images.shape} and {labels.shape} respectively.\")\n",
    "    break\n",
    "\n",
    "\n",
    "# Get parameters for each layer of the model in a tabular format\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68863939-dd80-4102-9d79-834ad3744f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    The simple evaluator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, loss_fn, device):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, testloader):\n",
    "        original_labels, predicted_labels = [], []\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in testloader:\n",
    "                # Move the batch to the device\n",
    "                batch = [t.to(self.device) for t in batch]\n",
    "                images, labels = batch\n",
    "\n",
    "                # Get predictions\n",
    "                logits = self.model(images)[0]\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = self.loss_fn(logits, nn.functional.one_hot(labels, num_classes=3).type(torch.FloatTensor).cuda())\n",
    "                total_loss += loss.item() * len(images)\n",
    "\n",
    "                # Calculate the accuracy\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "\n",
    "                # Append to the lists\n",
    "                original_labels = original_labels + labels.tolist()\n",
    "                predicted_labels = predicted_labels + predictions.tolist()\n",
    "        \n",
    "        accuracy = correct / len(testloader.dataset)\n",
    "        avg_loss = total_loss / len(testloader.dataset)\n",
    "        return avg_loss, accuracy, original_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e66cd3-6cd8-4a66-9239-423e3ab189d6",
   "metadata": {},
   "source": [
    "## **Hybrid-CCT (Transformer Encoder Layer No - 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a20b78b1-0595-49a4-9529-cb08adf5a254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------------+\n",
      "|                       Modules                       | Parameters |\n",
      "+-----------------------------------------------------+------------+\n",
      "|                 embedding.cls_token                 |    216     |\n",
      "|            embedding.position_embeddings            |   110808   |\n",
      "|    embedding.patch_embeddings.conv_1.conv.weight    |    864     |\n",
      "|     embedding.patch_embeddings.conv_1.conv.bias     |     32     |\n",
      "|     embedding.patch_embeddings.conv_1.bn.weight     |     32     |\n",
      "|      embedding.patch_embeddings.conv_1.bn.bias      |     32     |\n",
      "|    embedding.patch_embeddings.conv_2.conv.weight    |   55296    |\n",
      "|     embedding.patch_embeddings.conv_2.conv.bias     |     64     |\n",
      "|     embedding.patch_embeddings.conv_2.bn.weight     |     64     |\n",
      "|      embedding.patch_embeddings.conv_2.bn.bias      |     64     |\n",
      "|    embedding.patch_embeddings.conv_3.conv.weight    |   221184   |\n",
      "|     embedding.patch_embeddings.conv_3.conv.bias     |    128     |\n",
      "|     embedding.patch_embeddings.conv_3.bn.weight     |    128     |\n",
      "|      embedding.patch_embeddings.conv_3.bn.bias      |    128     |\n",
      "|    embedding.patch_embeddings.conv_4.conv.weight    |   884736   |\n",
      "|     embedding.patch_embeddings.conv_4.conv.bias     |    256     |\n",
      "|     embedding.patch_embeddings.conv_4.bn.weight     |    256     |\n",
      "|      embedding.patch_embeddings.conv_4.bn.bias      |    256     |\n",
      "|    embedding.patch_embeddings.conv_5.conv.weight    |  3538944   |\n",
      "|     embedding.patch_embeddings.conv_5.conv.bias     |    512     |\n",
      "|     embedding.patch_embeddings.conv_5.bn.weight     |    512     |\n",
      "|      embedding.patch_embeddings.conv_5.bn.bias      |    512     |\n",
      "|   encoder.blocks.0.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.0.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.0.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.0.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.0.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.0.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.0.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.0.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.1.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.1.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.1.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.1.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.1.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.1.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.1.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.1.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.2.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.2.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.2.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.2.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.2.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.2.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.2.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.2.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_2.bias          |    216     |\n",
      "|                attention_pool.weight                |    216     |\n",
      "|                 attention_pool.bias                 |     1      |\n",
      "|                  classifier.weight                  |    1296    |\n",
      "|                   classifier.bias                   |     3      |\n",
      "+-----------------------------------------------------+------------+\n",
      "Total Trainable Params: 6223996\n",
      "Total parameters: 6223996, Trainable Parameters 6223996\n",
      "Total parameters: 6.223996M, Trainable Parameters 6.223996M\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Create the model, loss function and trainer\n",
    "config['num_hidden_layers'] = 3\n",
    "config['model_name'] = 'model_best_Hybrid.pt'\n",
    "config['weight_base_dir'] = '/home/admin1/Arindam/Alzheimer/ViT/experiments/3D HCCT'\n",
    "\n",
    "model = ViTForClassfication(config)\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(os.path.join(config['weight_base_dir'], config['model_name'])))\n",
    "\n",
    "count_parameters(model)\n",
    "# Get number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}, Trainable Parameters {total_trainable_params}\\nTotal parameters: {total_params/1000000}M, Trainable Parameters {total_trainable_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b54f823-ff44-46d0-8e2b-116d86d83325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Hybrid CCT, Model name: model_best_Hybrid.pt\n",
      "Test Loss: 0.17257203954974085, Test Accuracy: 0.9575757575757575\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "tester = Evaluator(model, loss_fn, device)\n",
    "test_loss, test_accuracy, original_labels, predicted_labels = tester.evaluate(test_loader)\n",
    "print(f\"Test Name: Hybrid CCT, Model name: {config['model_name']}\\nTest Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f8b0a85-84f2-4eaa-b3cf-aa9935bc5e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9735    0.9735    0.9735       113\n",
      "           1     0.9589    0.9459    0.9524       148\n",
      "           2     0.9296    0.9565    0.9429        69\n",
      "\n",
      "    accuracy                         0.9576       330\n",
      "   macro avg     0.9540    0.9586    0.9562       330\n",
      "weighted avg     0.9578    0.9576    0.9576       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original_labels, predicted_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d4f2b9c-532a-471a-b4c1-e76eefd17ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdc743d3e30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44187d22-6c25-45a1-9679-e07b6abfe752",
   "metadata": {},
   "source": [
    "## **Hybrid-CCT Fine-Tune (Transformer Encoder Layer No - 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41e74637-b30d-4a28-95ca-663fbeef4e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------------+\n",
      "|                       Modules                       | Parameters |\n",
      "+-----------------------------------------------------+------------+\n",
      "|                 embedding.cls_token                 |    216     |\n",
      "|            embedding.position_embeddings            |   110808   |\n",
      "|    embedding.patch_embeddings.conv_1.conv.weight    |    864     |\n",
      "|     embedding.patch_embeddings.conv_1.conv.bias     |     32     |\n",
      "|     embedding.patch_embeddings.conv_1.bn.weight     |     32     |\n",
      "|      embedding.patch_embeddings.conv_1.bn.bias      |     32     |\n",
      "|    embedding.patch_embeddings.conv_2.conv.weight    |   55296    |\n",
      "|     embedding.patch_embeddings.conv_2.conv.bias     |     64     |\n",
      "|     embedding.patch_embeddings.conv_2.bn.weight     |     64     |\n",
      "|      embedding.patch_embeddings.conv_2.bn.bias      |     64     |\n",
      "|    embedding.patch_embeddings.conv_3.conv.weight    |   221184   |\n",
      "|     embedding.patch_embeddings.conv_3.conv.bias     |    128     |\n",
      "|     embedding.patch_embeddings.conv_3.bn.weight     |    128     |\n",
      "|      embedding.patch_embeddings.conv_3.bn.bias      |    128     |\n",
      "|    embedding.patch_embeddings.conv_4.conv.weight    |   884736   |\n",
      "|     embedding.patch_embeddings.conv_4.conv.bias     |    256     |\n",
      "|     embedding.patch_embeddings.conv_4.bn.weight     |    256     |\n",
      "|      embedding.patch_embeddings.conv_4.bn.bias      |    256     |\n",
      "|    embedding.patch_embeddings.conv_5.conv.weight    |  3538944   |\n",
      "|     embedding.patch_embeddings.conv_5.conv.bias     |    512     |\n",
      "|     embedding.patch_embeddings.conv_5.bn.weight     |    512     |\n",
      "|      embedding.patch_embeddings.conv_5.bn.bias      |    512     |\n",
      "|   encoder.blocks.0.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.0.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.0.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.0.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.0.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.0.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.0.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.0.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.1.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.1.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.1.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.1.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.1.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.1.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.1.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.1.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.2.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.2.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.2.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.2.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.2.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.2.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.2.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.2.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_2.bias          |    216     |\n",
      "|                attention_pool.weight                |    216     |\n",
      "|                 attention_pool.bias                 |     1      |\n",
      "|                  classifier.weight                  |    1296    |\n",
      "|                   classifier.bias                   |     3      |\n",
      "+-----------------------------------------------------+------------+\n",
      "Total Trainable Params: 6223996\n",
      "Total parameters: 6223996, Trainable Parameters 6223996\n",
      "Total parameters: 6.223996M, Trainable Parameters 6.223996M\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Create the model, loss function and trainer\n",
    "config['num_hidden_layers'] = 3\n",
    "config['model_name'] = 'model_best_finetuned.pt'\n",
    "config['weight_base_dir'] = '/home/admin1/Arindam/Alzheimer/ViT/experiments/Hybrid-Finetune'\n",
    "\n",
    "model = ViTForClassfication(config)\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(os.path.join(config['weight_base_dir'], config['model_name'])))\n",
    "\n",
    "count_parameters(model)\n",
    "# Get number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}, Trainable Parameters {total_trainable_params}\\nTotal parameters: {total_params/1000000}M, Trainable Parameters {total_trainable_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea2332e3-c2f4-4df4-92ea-0224e82fa808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Hybrid CCT Fine-Tune, Model name: model_best_finetuned.pt \n",
      "Test Loss: 0.13705849823705224, Test Accuracy: 0.9606060606060606\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "tester = Evaluator(model, loss_fn, device)\n",
    "test_loss, test_accuracy, original_labels, predicted_labels = tester.evaluate(test_loader)\n",
    "print(f\"Test Name: Hybrid CCT Fine-Tune, Model name: {config['model_name']} \\nTest Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32c6579e-cc0c-4d8b-a672-57186d66a106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9649    0.9735    0.9692       113\n",
      "           1     0.9592    0.9527    0.9559       148\n",
      "           2     0.9565    0.9565    0.9565        69\n",
      "\n",
      "    accuracy                         0.9606       330\n",
      "   macro avg     0.9602    0.9609    0.9605       330\n",
      "weighted avg     0.9606    0.9606    0.9606       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original_labels, predicted_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77360c64-4342-4056-b4fd-38dbf5a1bd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdc743d3e30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b226d-9e63-43ec-9de0-1eba50d87816",
   "metadata": {},
   "source": [
    "## **Hybrid-CCT (Transformer Encoder Layer No - 4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f68b2bca-a707-4a9a-8673-602a2e147081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------------+\n",
      "|                       Modules                       | Parameters |\n",
      "+-----------------------------------------------------+------------+\n",
      "|                 embedding.cls_token                 |    216     |\n",
      "|            embedding.position_embeddings            |   110808   |\n",
      "|    embedding.patch_embeddings.conv_1.conv.weight    |    864     |\n",
      "|     embedding.patch_embeddings.conv_1.conv.bias     |     32     |\n",
      "|     embedding.patch_embeddings.conv_1.bn.weight     |     32     |\n",
      "|      embedding.patch_embeddings.conv_1.bn.bias      |     32     |\n",
      "|    embedding.patch_embeddings.conv_2.conv.weight    |   55296    |\n",
      "|     embedding.patch_embeddings.conv_2.conv.bias     |     64     |\n",
      "|     embedding.patch_embeddings.conv_2.bn.weight     |     64     |\n",
      "|      embedding.patch_embeddings.conv_2.bn.bias      |     64     |\n",
      "|    embedding.patch_embeddings.conv_3.conv.weight    |   221184   |\n",
      "|     embedding.patch_embeddings.conv_3.conv.bias     |    128     |\n",
      "|     embedding.patch_embeddings.conv_3.bn.weight     |    128     |\n",
      "|      embedding.patch_embeddings.conv_3.bn.bias      |    128     |\n",
      "|    embedding.patch_embeddings.conv_4.conv.weight    |   884736   |\n",
      "|     embedding.patch_embeddings.conv_4.conv.bias     |    256     |\n",
      "|     embedding.patch_embeddings.conv_4.bn.weight     |    256     |\n",
      "|      embedding.patch_embeddings.conv_4.bn.bias      |    256     |\n",
      "|    embedding.patch_embeddings.conv_5.conv.weight    |  3538944   |\n",
      "|     embedding.patch_embeddings.conv_5.conv.bias     |    512     |\n",
      "|     embedding.patch_embeddings.conv_5.bn.weight     |    512     |\n",
      "|      embedding.patch_embeddings.conv_5.bn.bias      |    512     |\n",
      "|   encoder.blocks.0.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.0.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.0.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.0.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.0.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.0.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.0.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.0.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.1.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.1.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.1.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.1.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.1.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.1.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.1.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.1.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.2.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.2.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.2.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.2.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.2.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.2.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.2.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.2.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.3.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.3.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.3.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.3.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.3.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.3.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.3.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.3.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.3.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.3.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.3.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.3.layernorm_2.bias          |    216     |\n",
      "|                attention_pool.weight                |    216     |\n",
      "|                 attention_pool.bias                 |     1      |\n",
      "|                  classifier.weight                  |    1296    |\n",
      "|                   classifier.bias                   |     3      |\n",
      "+-----------------------------------------------------+------------+\n",
      "Total Trainable Params: 6693148\n",
      "Total parameters: 6693148, Trainable Parameters 6693148\n",
      "Total parameters: 6.693148M, Trainable Parameters 6.693148M\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Create the model, loss function and trainer\n",
    "config['num_hidden_layers'] = 4\n",
    "config['model_name'] = 'model_best_Hybrid-Ablation-TEL4.pt'\n",
    "config['weight_base_dir'] = '/home/admin1/Arindam/Alzheimer/ViT/Ablation Studies/experiments/Hybrid-Ablation-TEL4'\n",
    "\n",
    "model = ViTForClassfication(config)\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(os.path.join(config['weight_base_dir'], config['model_name'])))\n",
    "\n",
    "count_parameters(model)\n",
    "# Get number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}, Trainable Parameters {total_trainable_params}\\nTotal parameters: {total_params/1000000}M, Trainable Parameters {total_trainable_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fc284a6-aaed-49cb-adbc-bc494bbf7f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Hybrid CCT TEL4, Model name: model_best_Hybrid-Ablation-TEL4.pt \n",
      "Test Loss: 0.38854642112025173, Test Accuracy: 0.9242424242424242\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "tester = Evaluator(model, loss_fn, device)\n",
    "test_loss, test_accuracy, original_labels, predicted_labels = tester.evaluate(test_loader)\n",
    "print(f\"Test Name: Hybrid CCT TEL4, Model name: {config['model_name']} \\nTest Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d2955ab-8150-4586-95df-cc340da621d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9407    0.9823    0.9610       113\n",
      "           1     0.9362    0.8919    0.9135       148\n",
      "           2     0.8732    0.8986    0.8857        69\n",
      "\n",
      "    accuracy                         0.9242       330\n",
      "   macro avg     0.9167    0.9242    0.9201       330\n",
      "weighted avg     0.9246    0.9242    0.9240       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original_labels, predicted_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fdc8ea1-1fe2-4bea-a9af-d551aed9a76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdc743d3e30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f1ef1-0fe3-4e5c-9433-9c728faf968e",
   "metadata": {},
   "source": [
    "## **Hybrid-CCT Fine-Tune (Transformer Encoder Layer No - 4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a83467d-7514-4202-87a5-3ed2d3c66172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------------+\n",
      "|                       Modules                       | Parameters |\n",
      "+-----------------------------------------------------+------------+\n",
      "|                 embedding.cls_token                 |    216     |\n",
      "|            embedding.position_embeddings            |   110808   |\n",
      "|    embedding.patch_embeddings.conv_1.conv.weight    |    864     |\n",
      "|     embedding.patch_embeddings.conv_1.conv.bias     |     32     |\n",
      "|     embedding.patch_embeddings.conv_1.bn.weight     |     32     |\n",
      "|      embedding.patch_embeddings.conv_1.bn.bias      |     32     |\n",
      "|    embedding.patch_embeddings.conv_2.conv.weight    |   55296    |\n",
      "|     embedding.patch_embeddings.conv_2.conv.bias     |     64     |\n",
      "|     embedding.patch_embeddings.conv_2.bn.weight     |     64     |\n",
      "|      embedding.patch_embeddings.conv_2.bn.bias      |     64     |\n",
      "|    embedding.patch_embeddings.conv_3.conv.weight    |   221184   |\n",
      "|     embedding.patch_embeddings.conv_3.conv.bias     |    128     |\n",
      "|     embedding.patch_embeddings.conv_3.bn.weight     |    128     |\n",
      "|      embedding.patch_embeddings.conv_3.bn.bias      |    128     |\n",
      "|    embedding.patch_embeddings.conv_4.conv.weight    |   884736   |\n",
      "|     embedding.patch_embeddings.conv_4.conv.bias     |    256     |\n",
      "|     embedding.patch_embeddings.conv_4.bn.weight     |    256     |\n",
      "|      embedding.patch_embeddings.conv_4.bn.bias      |    256     |\n",
      "|    embedding.patch_embeddings.conv_5.conv.weight    |  3538944   |\n",
      "|     embedding.patch_embeddings.conv_5.conv.bias     |    512     |\n",
      "|     embedding.patch_embeddings.conv_5.bn.weight     |    512     |\n",
      "|      embedding.patch_embeddings.conv_5.bn.bias      |    512     |\n",
      "|   encoder.blocks.0.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.0.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.0.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.0.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.0.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.0.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.0.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.0.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.1.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.1.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.1.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.1.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.1.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.1.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.1.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.1.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.2.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.2.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.2.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.2.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.2.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.2.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.2.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.2.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.3.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.3.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.3.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.3.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.3.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.3.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.3.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.3.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.3.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.3.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.3.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.3.layernorm_2.bias          |    216     |\n",
      "|                attention_pool.weight                |    216     |\n",
      "|                 attention_pool.bias                 |     1      |\n",
      "|                  classifier.weight                  |    1296    |\n",
      "|                   classifier.bias                   |     3      |\n",
      "+-----------------------------------------------------+------------+\n",
      "Total Trainable Params: 6693148\n",
      "Total parameters: 6693148, Trainable Parameters 6693148\n",
      "Total parameters: 6.693148M, Trainable Parameters 6.693148M\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Create the model, loss function and trainer\n",
    "config['num_hidden_layers'] = 4\n",
    "config['model_name'] = 'model_best__finetuned_Hybrid-Ablation-TEL4.pt'\n",
    "config['weight_base_dir'] = '/home/admin1/Arindam/Alzheimer/ViT/Ablation Studies/experiments/Hybrid-Ablation-TEL4'\n",
    "\n",
    "model = ViTForClassfication(config)\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(os.path.join(config['weight_base_dir'], config['model_name'])))\n",
    "\n",
    "count_parameters(model)\n",
    "# Get number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}, Trainable Parameters {total_trainable_params}\\nTotal parameters: {total_params/1000000}M, Trainable Parameters {total_trainable_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d35d9184-5de8-44f3-bb53-a21a03a92176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Hybrid CCT TEL4 Fine-Tune, Model name: model_best__finetuned_Hybrid-Ablation-TEL4.pt \n",
      "Test Loss: 0.38628612571456467, Test Accuracy: 0.9484848484848485\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "tester = Evaluator(model, loss_fn, device)\n",
    "test_loss, test_accuracy, original_labels, predicted_labels = tester.evaluate(test_loader)\n",
    "print(f\"Test Name: Hybrid CCT TEL4 Fine-Tune, Model name: {config['model_name']} \\nTest Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6935336c-3db7-4d1a-b735-677cd5f71597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9244    0.9735    0.9483       113\n",
      "           1     0.9645    0.9189    0.9412       148\n",
      "           2     0.9571    0.9710    0.9640        69\n",
      "\n",
      "    accuracy                         0.9485       330\n",
      "   macro avg     0.9487    0.9545    0.9512       330\n",
      "weighted avg     0.9492    0.9485    0.9484       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original_labels, predicted_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76511258-f8bb-4031-a1b5-d77ad4d437f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdc743d3e30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937084d-f46c-4e77-b3c4-516d07da25b8",
   "metadata": {},
   "source": [
    "## **Hybrid-CCT (Transformer Encoder Layer No - 5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0402426f-9bf9-4182-8c8c-96f4f801969a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------------+\n",
      "|                       Modules                       | Parameters |\n",
      "+-----------------------------------------------------+------------+\n",
      "|                 embedding.cls_token                 |    216     |\n",
      "|            embedding.position_embeddings            |   110808   |\n",
      "|    embedding.patch_embeddings.conv_1.conv.weight    |    864     |\n",
      "|     embedding.patch_embeddings.conv_1.conv.bias     |     32     |\n",
      "|     embedding.patch_embeddings.conv_1.bn.weight     |     32     |\n",
      "|      embedding.patch_embeddings.conv_1.bn.bias      |     32     |\n",
      "|    embedding.patch_embeddings.conv_2.conv.weight    |   55296    |\n",
      "|     embedding.patch_embeddings.conv_2.conv.bias     |     64     |\n",
      "|     embedding.patch_embeddings.conv_2.bn.weight     |     64     |\n",
      "|      embedding.patch_embeddings.conv_2.bn.bias      |     64     |\n",
      "|    embedding.patch_embeddings.conv_3.conv.weight    |   221184   |\n",
      "|     embedding.patch_embeddings.conv_3.conv.bias     |    128     |\n",
      "|     embedding.patch_embeddings.conv_3.bn.weight     |    128     |\n",
      "|      embedding.patch_embeddings.conv_3.bn.bias      |    128     |\n",
      "|    embedding.patch_embeddings.conv_4.conv.weight    |   884736   |\n",
      "|     embedding.patch_embeddings.conv_4.conv.bias     |    256     |\n",
      "|     embedding.patch_embeddings.conv_4.bn.weight     |    256     |\n",
      "|      embedding.patch_embeddings.conv_4.bn.bias      |    256     |\n",
      "|    embedding.patch_embeddings.conv_5.conv.weight    |  3538944   |\n",
      "|     embedding.patch_embeddings.conv_5.conv.bias     |    512     |\n",
      "|     embedding.patch_embeddings.conv_5.bn.weight     |    512     |\n",
      "|      embedding.patch_embeddings.conv_5.bn.bias      |    512     |\n",
      "|   encoder.blocks.0.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.0.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.0.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.0.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.0.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.0.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.0.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.0.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.1.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.1.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.1.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.1.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.1.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.1.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.1.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.1.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.2.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.2.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.2.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.2.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.2.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.2.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.2.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.2.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.3.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.3.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.3.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.3.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.3.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.3.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.3.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.3.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.3.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.3.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.3.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.3.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.4.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.4.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.4.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.4.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.4.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.4.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.4.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.4.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.4.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.4.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.4.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.4.layernorm_2.bias          |    216     |\n",
      "|                attention_pool.weight                |    216     |\n",
      "|                 attention_pool.bias                 |     1      |\n",
      "|                  classifier.weight                  |    1296    |\n",
      "|                   classifier.bias                   |     3      |\n",
      "+-----------------------------------------------------+------------+\n",
      "Total Trainable Params: 7162300\n",
      "Total parameters: 7162300, Trainable Parameters 7162300\n",
      "Total parameters: 7.1623M, Trainable Parameters 7.1623M\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Create the model, loss function and trainer\n",
    "config['num_hidden_layers'] = 5\n",
    "config['model_name'] = 'model_best_Hybrid-Ablation-TEL5.pt'\n",
    "config['weight_base_dir'] = '/home/admin1/Arindam/Alzheimer/ViT/Ablation Studies/experiments/Hybrid-Ablation-TEL5'\n",
    "\n",
    "model = ViTForClassfication(config)\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(os.path.join(config['weight_base_dir'], config['model_name'])))\n",
    "\n",
    "count_parameters(model)\n",
    "# Get number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}, Trainable Parameters {total_trainable_params}\\nTotal parameters: {total_params/1000000}M, Trainable Parameters {total_trainable_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4382bc9-d96e-4c95-936d-b3249f5cd9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Hybrid CCT TEL5, Model name: model_best_Hybrid-Ablation-TEL5.pt \n",
      "Test Loss: 0.2757029222984685, Test Accuracy: 0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "tester = Evaluator(model, loss_fn, device)\n",
    "test_loss, test_accuracy, original_labels, predicted_labels = tester.evaluate(test_loader)\n",
    "print(f\"Test Name: Hybrid CCT TEL5, Model name: {config['model_name']} \\nTest Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a07e6e0-061f-4b72-a6ba-b29b69bdb0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9483    0.9735    0.9607       113\n",
      "           1     0.9524    0.9459    0.9492       148\n",
      "           2     0.9701    0.9420    0.9559        69\n",
      "\n",
      "    accuracy                         0.9545       330\n",
      "   macro avg     0.9569    0.9538    0.9552       330\n",
      "weighted avg     0.9547    0.9545    0.9545       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original_labels, predicted_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "724fb1be-f015-4828-bba4-191ba8bbb4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdc743d3e30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac49c26-5ca8-42f0-b0b8-0ec5d191663d",
   "metadata": {},
   "source": [
    "## **Hybrid-CCT Fine-Tune (Transformer Encoder Layer No - 5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f904ad8-bf55-4be2-9f08-33f5aa602a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------------+\n",
      "|                       Modules                       | Parameters |\n",
      "+-----------------------------------------------------+------------+\n",
      "|                 embedding.cls_token                 |    216     |\n",
      "|            embedding.position_embeddings            |   110808   |\n",
      "|    embedding.patch_embeddings.conv_1.conv.weight    |    864     |\n",
      "|     embedding.patch_embeddings.conv_1.conv.bias     |     32     |\n",
      "|     embedding.patch_embeddings.conv_1.bn.weight     |     32     |\n",
      "|      embedding.patch_embeddings.conv_1.bn.bias      |     32     |\n",
      "|    embedding.patch_embeddings.conv_2.conv.weight    |   55296    |\n",
      "|     embedding.patch_embeddings.conv_2.conv.bias     |     64     |\n",
      "|     embedding.patch_embeddings.conv_2.bn.weight     |     64     |\n",
      "|      embedding.patch_embeddings.conv_2.bn.bias      |     64     |\n",
      "|    embedding.patch_embeddings.conv_3.conv.weight    |   221184   |\n",
      "|     embedding.patch_embeddings.conv_3.conv.bias     |    128     |\n",
      "|     embedding.patch_embeddings.conv_3.bn.weight     |    128     |\n",
      "|      embedding.patch_embeddings.conv_3.bn.bias      |    128     |\n",
      "|    embedding.patch_embeddings.conv_4.conv.weight    |   884736   |\n",
      "|     embedding.patch_embeddings.conv_4.conv.bias     |    256     |\n",
      "|     embedding.patch_embeddings.conv_4.bn.weight     |    256     |\n",
      "|      embedding.patch_embeddings.conv_4.bn.bias      |    256     |\n",
      "|    embedding.patch_embeddings.conv_5.conv.weight    |  3538944   |\n",
      "|     embedding.patch_embeddings.conv_5.conv.bias     |    512     |\n",
      "|     embedding.patch_embeddings.conv_5.bn.weight     |    512     |\n",
      "|      embedding.patch_embeddings.conv_5.bn.bias      |    512     |\n",
      "|   encoder.blocks.0.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.0.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.0.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.0.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.0.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.0.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.0.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.0.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.1.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.1.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.1.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.1.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.1.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.1.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.1.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.1.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.2.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.2.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.2.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.2.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.2.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.2.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.2.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.2.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.3.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.3.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.3.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.3.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.3.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.3.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.3.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.3.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.3.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.3.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.3.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.3.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.4.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.4.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.4.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.4.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.4.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.4.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.4.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.4.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.4.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.4.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.4.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.4.layernorm_2.bias          |    216     |\n",
      "|                attention_pool.weight                |    216     |\n",
      "|                 attention_pool.bias                 |     1      |\n",
      "|                  classifier.weight                  |    1296    |\n",
      "|                   classifier.bias                   |     3      |\n",
      "+-----------------------------------------------------+------------+\n",
      "Total Trainable Params: 7162300\n",
      "Total parameters: 7162300, Trainable Parameters 7162300\n",
      "Total parameters: 7.1623M, Trainable Parameters 7.1623M\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Create the model, loss function and trainer\n",
    "config['num_hidden_layers'] = 5\n",
    "config['model_name'] = 'model_best__finetuned_Hybrid-Ablation-TEL5.pt'\n",
    "config['weight_base_dir'] = '/home/admin1/Arindam/Alzheimer/ViT/Ablation Studies/experiments/Hybrid-Ablation-TEL5'\n",
    "\n",
    "model = ViTForClassfication(config)\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(os.path.join(config['weight_base_dir'], config['model_name'])))\n",
    "\n",
    "count_parameters(model)\n",
    "# Get number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}, Trainable Parameters {total_trainable_params}\\nTotal parameters: {total_params/1000000}M, Trainable Parameters {total_trainable_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2c7c7f1-e2aa-43ee-9eaf-80c1bb096c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Hybrid CCT TEL5 Fine-Tune, Model name: model_best__finetuned_Hybrid-Ablation-TEL5.pt \n",
      "Test Loss: 0.29130117399202937, Test Accuracy: 0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "tester = Evaluator(model, loss_fn, device)\n",
    "test_loss, test_accuracy, original_labels, predicted_labels = tester.evaluate(test_loader)\n",
    "print(f\"Test Name: Hybrid CCT TEL5 Fine-Tune, Model name: {config['model_name']} \\nTest Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38b09ac8-125b-4a3b-8b0d-f7b8fb6064c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9483    0.9735    0.9607       113\n",
      "           1     0.9786    0.9257    0.9514       148\n",
      "           2     0.9189    0.9855    0.9510        69\n",
      "\n",
      "    accuracy                         0.9545       330\n",
      "   macro avg     0.9486    0.9615    0.9544       330\n",
      "weighted avg     0.9557    0.9545    0.9545       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original_labels, predicted_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c505328-1699-4f4e-8c3f-ca1c2c5945dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdc743d3e30>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d0c7ed-c4a7-4bbc-8b27-6f0a5a283a03",
   "metadata": {},
   "source": [
    "## **Hybrid-CCT (Transformer Encoder Layer No - 6)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec72dddb-ec80-49cf-b011-f975b4866813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------------+\n",
      "|                       Modules                       | Parameters |\n",
      "+-----------------------------------------------------+------------+\n",
      "|                 embedding.cls_token                 |    216     |\n",
      "|            embedding.position_embeddings            |   110808   |\n",
      "|    embedding.patch_embeddings.conv_1.conv.weight    |    864     |\n",
      "|     embedding.patch_embeddings.conv_1.conv.bias     |     32     |\n",
      "|     embedding.patch_embeddings.conv_1.bn.weight     |     32     |\n",
      "|      embedding.patch_embeddings.conv_1.bn.bias      |     32     |\n",
      "|    embedding.patch_embeddings.conv_2.conv.weight    |   55296    |\n",
      "|     embedding.patch_embeddings.conv_2.conv.bias     |     64     |\n",
      "|     embedding.patch_embeddings.conv_2.bn.weight     |     64     |\n",
      "|      embedding.patch_embeddings.conv_2.bn.bias      |     64     |\n",
      "|    embedding.patch_embeddings.conv_3.conv.weight    |   221184   |\n",
      "|     embedding.patch_embeddings.conv_3.conv.bias     |    128     |\n",
      "|     embedding.patch_embeddings.conv_3.bn.weight     |    128     |\n",
      "|      embedding.patch_embeddings.conv_3.bn.bias      |    128     |\n",
      "|    embedding.patch_embeddings.conv_4.conv.weight    |   884736   |\n",
      "|     embedding.patch_embeddings.conv_4.conv.bias     |    256     |\n",
      "|     embedding.patch_embeddings.conv_4.bn.weight     |    256     |\n",
      "|      embedding.patch_embeddings.conv_4.bn.bias      |    256     |\n",
      "|    embedding.patch_embeddings.conv_5.conv.weight    |  3538944   |\n",
      "|     embedding.patch_embeddings.conv_5.conv.bias     |    512     |\n",
      "|     embedding.patch_embeddings.conv_5.bn.weight     |    512     |\n",
      "|      embedding.patch_embeddings.conv_5.bn.bias      |    512     |\n",
      "|   encoder.blocks.0.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.0.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.0.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.0.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.0.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.0.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.0.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.0.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.1.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.1.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.1.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.1.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.1.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.1.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.1.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.1.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.2.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.2.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.2.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.2.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.2.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.2.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.2.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.2.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.3.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.3.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.3.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.3.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.3.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.3.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.3.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.3.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.3.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.3.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.3.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.3.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.4.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.4.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.4.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.4.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.4.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.4.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.4.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.4.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.4.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.4.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.4.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.4.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.5.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.5.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.5.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.5.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.5.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.5.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.5.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.5.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.5.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.5.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.5.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.5.layernorm_2.bias          |    216     |\n",
      "|                attention_pool.weight                |    216     |\n",
      "|                 attention_pool.bias                 |     1      |\n",
      "|                  classifier.weight                  |    1296    |\n",
      "|                   classifier.bias                   |     3      |\n",
      "+-----------------------------------------------------+------------+\n",
      "Total Trainable Params: 7631452\n",
      "Total parameters: 7631452, Trainable Parameters 7631452\n",
      "Total parameters: 7.631452M, Trainable Parameters 7.631452M\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Create the model, loss function and trainer\n",
    "config['num_hidden_layers'] = 6\n",
    "config['model_name'] = 'model_best_Hybrid-Ablation-TEL6.pt'\n",
    "config['weight_base_dir'] = '/home/admin1/Arindam/Alzheimer/ViT/Ablation Studies/experiments/Hybrid-Ablation-TEL6'\n",
    "\n",
    "model = ViTForClassfication(config)\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(os.path.join(config['weight_base_dir'], config['model_name'])))\n",
    "\n",
    "count_parameters(model)\n",
    "# Get number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}, Trainable Parameters {total_trainable_params}\\nTotal parameters: {total_params/1000000}M, Trainable Parameters {total_trainable_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d950a95-336c-4a27-8d78-165e605a9136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Hybrid CCT TEL6, Model name: model_best_Hybrid-Ablation-TEL6.pt \n",
      "Test Loss: 0.40422898917441763, Test Accuracy: 0.9424242424242424\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "tester = Evaluator(model, loss_fn, device)\n",
    "test_loss, test_accuracy, original_labels, predicted_labels = tester.evaluate(test_loader)\n",
    "print(f\"Test Name: Hybrid CCT TEL6, Model name: {config['model_name']} \\nTest Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f55ffc00-c1aa-4f46-9a1b-23dc430f51b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9250    0.9823    0.9528       113\n",
      "           1     0.9643    0.9122    0.9375       148\n",
      "           2     0.9286    0.9420    0.9353        69\n",
      "\n",
      "    accuracy                         0.9424       330\n",
      "   macro avg     0.9393    0.9455    0.9418       330\n",
      "weighted avg     0.9434    0.9424    0.9423       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original_labels, predicted_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d11aef7-9864-4ec3-b7c8-458b20f6a1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdc743d3e30>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91eb022-41fb-4da7-b698-3e7f22b4e306",
   "metadata": {},
   "source": [
    "## **Hybrid-CCT Fine-Tune (Transformer Encoder Layer No - 6)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c0dcf8a-7ce5-41db-8a92-ac10695f3cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------------+\n",
      "|                       Modules                       | Parameters |\n",
      "+-----------------------------------------------------+------------+\n",
      "|                 embedding.cls_token                 |    216     |\n",
      "|            embedding.position_embeddings            |   110808   |\n",
      "|    embedding.patch_embeddings.conv_1.conv.weight    |    864     |\n",
      "|     embedding.patch_embeddings.conv_1.conv.bias     |     32     |\n",
      "|     embedding.patch_embeddings.conv_1.bn.weight     |     32     |\n",
      "|      embedding.patch_embeddings.conv_1.bn.bias      |     32     |\n",
      "|    embedding.patch_embeddings.conv_2.conv.weight    |   55296    |\n",
      "|     embedding.patch_embeddings.conv_2.conv.bias     |     64     |\n",
      "|     embedding.patch_embeddings.conv_2.bn.weight     |     64     |\n",
      "|      embedding.patch_embeddings.conv_2.bn.bias      |     64     |\n",
      "|    embedding.patch_embeddings.conv_3.conv.weight    |   221184   |\n",
      "|     embedding.patch_embeddings.conv_3.conv.bias     |    128     |\n",
      "|     embedding.patch_embeddings.conv_3.bn.weight     |    128     |\n",
      "|      embedding.patch_embeddings.conv_3.bn.bias      |    128     |\n",
      "|    embedding.patch_embeddings.conv_4.conv.weight    |   884736   |\n",
      "|     embedding.patch_embeddings.conv_4.conv.bias     |    256     |\n",
      "|     embedding.patch_embeddings.conv_4.bn.weight     |    256     |\n",
      "|      embedding.patch_embeddings.conv_4.bn.bias      |    256     |\n",
      "|    embedding.patch_embeddings.conv_5.conv.weight    |  3538944   |\n",
      "|     embedding.patch_embeddings.conv_5.conv.bias     |    512     |\n",
      "|     embedding.patch_embeddings.conv_5.bn.weight     |    512     |\n",
      "|      embedding.patch_embeddings.conv_5.bn.bias      |    512     |\n",
      "|   encoder.blocks.0.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.0.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.0.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.0.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.0.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.0.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.0.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.0.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.0.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.0.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.1.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.1.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.1.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.1.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.1.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.1.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.1.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.1.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.1.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.1.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.2.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.2.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.2.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.2.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.2.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.2.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.2.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.2.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.2.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.2.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.3.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.3.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.3.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.3.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.3.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.3.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.3.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.3.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.3.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.3.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.3.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.3.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.4.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.4.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.4.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.4.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.4.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.4.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.4.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.4.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.4.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.4.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.4.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.4.layernorm_2.bias          |    216     |\n",
      "|   encoder.blocks.5.attention.qkv_projection.weight  |   139968   |\n",
      "|    encoder.blocks.5.attention.qkv_projection.bias   |    648     |\n",
      "| encoder.blocks.5.attention.output_projection.weight |   46656    |\n",
      "|  encoder.blocks.5.attention.output_projection.bias  |    216     |\n",
      "|         encoder.blocks.5.layernorm_1.weight         |    216     |\n",
      "|          encoder.blocks.5.layernorm_1.bias          |    216     |\n",
      "|         encoder.blocks.5.mlp.dense_1.weight         |   139968   |\n",
      "|          encoder.blocks.5.mlp.dense_1.bias          |    648     |\n",
      "|         encoder.blocks.5.mlp.dense_2.weight         |   139968   |\n",
      "|          encoder.blocks.5.mlp.dense_2.bias          |    216     |\n",
      "|         encoder.blocks.5.layernorm_2.weight         |    216     |\n",
      "|          encoder.blocks.5.layernorm_2.bias          |    216     |\n",
      "|                attention_pool.weight                |    216     |\n",
      "|                 attention_pool.bias                 |     1      |\n",
      "|                  classifier.weight                  |    1296    |\n",
      "|                   classifier.bias                   |     3      |\n",
      "+-----------------------------------------------------+------------+\n",
      "Total Trainable Params: 7631452\n",
      "Total parameters: 7631452, Trainable Parameters 7631452\n",
      "Total parameters: 7.631452M, Trainable Parameters 7.631452M\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Create the model, loss function and trainer\n",
    "config['num_hidden_layers'] = 6\n",
    "config['model_name'] = 'model_best__finetuned_Hybrid-Ablation-TEL6.pt'\n",
    "config['weight_base_dir'] = '/home/admin1/Arindam/Alzheimer/ViT/Ablation Studies/experiments/Hybrid-Ablation-TEL6'\n",
    "\n",
    "model = ViTForClassfication(config)\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(os.path.join(config['weight_base_dir'], config['model_name'])))\n",
    "\n",
    "count_parameters(model)\n",
    "# Get number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}, Trainable Parameters {total_trainable_params}\\nTotal parameters: {total_params/1000000}M, Trainable Parameters {total_trainable_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "334658cf-185a-45fd-aa69-5265fccb17f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Name: Hybrid CCT Fine-Tune TEL6, Model name: model_best__finetuned_Hybrid-Ablation-TEL6.pt \n",
      "Test Loss: 0.2780846546467082, Test Accuracy: 0.9515151515151515\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "tester = Evaluator(model, loss_fn, device)\n",
    "test_loss, test_accuracy, original_labels, predicted_labels = tester.evaluate(test_loader)\n",
    "print(f\"Test Name: Hybrid CCT Fine-Tune TEL6, Model name: {config['model_name']} \\nTest Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3566cd3e-5c44-4658-a868-64f5917e1709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9250    0.9823    0.9528       113\n",
      "           1     0.9650    0.9324    0.9485       148\n",
      "           2     0.9701    0.9420    0.9559        69\n",
      "\n",
      "    accuracy                         0.9515       330\n",
      "   macro avg     0.9534    0.9523    0.9524       330\n",
      "weighted avg     0.9524    0.9515    0.9515       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original_labels, predicted_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "956df893-68e1-406f-a6b4-2af1fe48d752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdc743d3e30>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
